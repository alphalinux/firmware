;+
; ev6_osf_pal.mar
;-
	.title	"EV6_OSF_PAL"		; informational only
	.ident	"V1.50"			; informational only
;+
; Last Edit:	09-Apr-1999
;
; Edit History
;
; Who	Rev	When		What
; ---	---	----		----
; ES		04-Sep-96	Basic code. Passes Egore.
; ES		11-Sep-96	In trap__switch and trap__switch, form
;					PAL base more safely.
;				Put in pal__save_state and pal__restore_state
;				Put in pal__update_pcb_and_halt
; ES		13-Sep-96	Don't need console conditional. Default to it.
;					The important conditionl is srm_console.
;				Rearranged non-srm console powerup. Put powerup
;					totally in system_pal. Have powerup
;					do signature and swap checking.
;				Take redundant DTB flush out of switch.
;				Put in sys__enter_console, and sys__exit_console.
; ES		17-Sep-96	In save and restore, add vptb.
; ES		18-Sep-96	Added trap__cserve and sys__cserve, putting in
;					cserve__start and cserve__callback.
;				Added reference platform conditional.
;				In pal__restore_state, fix typo, so that
;					pal mode is or'ed in when turning
;					off shadow mode.
; ES		24-Sep-96	In pal__restore_state, when setting up
;					DTB_ASNx, don't trash r1.
;				Eliminate PT__M_CTL and PT__DC_CTL. Work
;					only from CNS__M_CTL and CNS__DC_CTL.
;					It's simpler and fixes save/restore bugs.
;				Add some PVC_VIOLATEs for hw_jmp_stalls.
; ES		25-Sep-96	Add mchk and crd.
;				In sys__switch_unknown, turn into HALT__JUMP0.
;				Add OSF_IPL_DEFS.
;				In trap__unalign_cm, had wrong reg in jmp_stall!
;				Add basic interrupt handling.
;				In cserve, add srm_console conditional.
; ES		27-Sep-96	In trap__interrupt, avoid cbr in 1st fetch block.
;				In trap__mt_fpcr, make 1st fetch block clean.
; ES		30-Sep-96	In trap__itb_miss, avoid trappable instructions
;					out of 1st fetc block.
; ES		24-Oct-96	In trap__arith, need to check for fpcr update
;					for integer overflow.
; ES		01-Nov-96	In switch, tbi functions, and imb, add
;					a write to ic_flush or ic_flush_asm.
;					Writes to itb_ia, itb_iap no longer
;					flush the icache.
;				In sys__crd, use current register to or
;					mchk code into p_misc. And use
;					MCES__PCE__S to test second error.
;				Rearrange order in ev6_osf_system_pal.mar.
; ES		06-Nov-96	In trap__unalign, fix bug where register
;					number was not being saved on stores.
; ES		12-Nov-96	In non-srm powerup, clear the fpcr to make the
;					simulator happy.
;				Add cflush call_pal.
; ES		13-Nov-96	In call_pal tbi, use r17 (not r16) for va.
; ES		27-Nov-96	Re-worked machine check handling in
;					ev6_osf_pal.mar, ev6_osf_system_pal.mar.
;				Added trap__pal_bugcheck code.
; ES		02-Dec-96	Added perfmon call_pal.
; ES		04-Dec-96	Don't trash r17 in call_pal__perfmon_wr_1.
; ES		06-Dec-96	Don't trash r17 in call_pal__perfmon_wr_1.
;					Really fix it this time.
; ES		09-Dec-96	Add ev6_p1, ev6_p2.
; ES		12-Dec-96	In ev6_osf_pal.mar and ev6_osf_system_pal.mar,
;					IMPORTANT CHANGE!!!
;					ev6__i_ctl<single_issue_l>
;						is really
;					ev6__i_ctl<single_issue_h>.
;				To avoid ISP/Behavorial mismatch on
;					hw_mfpr EV6__VA
;					when synching for asnx and isx, do a
;					hw_mfpr <EV6__PAL_BASE ! ^xF0>,
;					which scoreboards 4-7.
; ES		09-Jan-97	Re-worked reset. Moved more to the system
;					palcode. Need to do exact mapping
;					sequence with source map enable.
;					See SROM for retirator macro.
; ES		23-Jan-97	Still working on reset. Re-did switch entry
;					point a  little as well.
;					IMPORTANT: We assume that
;					INITIALIZE_RETIRATOR_AND_MAPPER and
;					MAP_SHADOW_REGISTERS
;					are both done in SROM; otherwise, we
;					we have to hack swppal support.
; ES		30-Jan-97	Set both spe modes so we can access i/o and
;					have normal unix superpage mode.
;				On pass1, set dc_ctl__dcdat_err_en to 0, since
;					there are ecc checking problems.
; ES		06-Feb-97	Fixed PVC violations found while checking vms.
;				(1) In powerup, separate mt_fpcr from mxpr.
;				(2) In powerup, pvc_violate 21 on asn clear.
;				(3) In sys__cbox, 1006. Also use hw_ret.
;				(4) In sys__mchk_scrub, pvc_violate 1006.
;				(5) In pal__save_state, pvc_violate 12.
;				(6) In pal__save_state, use hw_ret.
;				(7) In pal__restore_state, use hw_ret.
;				(8) In illegal call_pal, use hw_jmp.
;				(9) In swppal, use hw_jmp.
;				(10)In sys__int_post, use hw_ret_stall.
;				(11)In tbi, use hw_jmp.
;				(12)In wrent, use hw_jmp.
; ES		11-Feb-97	In wrperfmon, zap low nibbles of counts for p1.
; ES	1.1	24-Feb-97	(1) Start version numbers.
; ES		24-Feb-97	(2) In restore state, restore the vptb portion
;					of I_CTL and VA_CTL from CNS__VPTB.
;					Also restore PT__VA_CTL.
;				(3) Interrupt processing code jumped to
;					post_int without checking for swap
;					stack. Add that code to sys__int_post.
;					Also can eliminate that code from
;					machine check.
;				(3) In sys__enter_console, Go to 48-bit mode
;					in VA_CTL so that 1-1 console can
;					access I/O.
;				(4) Add dtbm_double_4.
;				(5) In pal__restore_state, write i_ctl
;					twice to cause a stall in case
;					we are toggling 48-bit mode bit.
; ES	1.2	10-Apr-97	Changes.
;				(1) Put IPL table at ^x0D00
;				(2) On p1, don't set DC_CTL<DCDAT_ERR_EN>.
;				(3) Do ldq on unlock in sys__reset and 
;					sys__exit_console. In sys__reset, put
;					an mb before ldq (pvc #28)
;				(4) Implement new palcode restriction, separating
;					retires of flush and hw_ret_stall.
;				(5) Bug in tbiap -- need ic_flush_asm,
;					not ic_flush.
;				(6) In switch, reset, exit console palcode,
;					redo some jmp_stall, scoreboard bit
;					synchronization.
;				(7) In hw_jmp_stall, use 1 instruction to
;					create offset instead of loading
;					offset and do a bis #1.
;				(8) In enter_console, write i_ctl twice
;					to wait until write completes
;					(pvc #31).
;				(9) In pass1, dc_ctl<dctag_par_en> must be 0.
;				(10)Change cbox read chain names to match spec.
;				(11)Add PAL_BASE, I_CTL, PROCESS_CONTEXT to
;					mchk logout frame.
;				(12)Add check_interrupt_pending conditional
;					for deasserting interrupts.
;				(13)In sys__cbox, add mb, eliminate mb's of
;					calling routines. Add clearing of
;					i_ctl<sbe> bits and ic_flush to
;					quiet the stream.
;				(14)In sys__reset, don't set spe<2>. Unix
;					just uses spe<1>, and works around
;					sign extension issues for i/o.
; ES	1.3	16-Apr-97	FP emulation support added.
;					Changes in fen, opcdec, arith,
;					wrfen, clrfen, swpctx, reset, fault
;					unalign, and tnv handling. Also added
;					CALL_PAL__003B to return to pal mode
;					after call to emulator.
;				In trap__switch, and sys_switch,
;					'bis' not 'and' asn and fpe!
;				Fix various pvc violations.
; ES	1.4	09-Jun-97	(1) Because of cbox triplicate tags, can
;					not do dc_ctl<flush> in reset. 
;					BIST cleans it anyway.
;				(2) In check_interrupt_pending code, hold
;					off writing IER until we have read
;					ISUM. Not good practice the other way.
;				(3) Ensure the ordering of ISUM and IER.
;				(4) Reorder mchk logout frame. Add VA_CTL.
; ES	1.5	26-Jun-97	(1) BUG FIX:
;					In sys__crd_second and
;					sys__crd_skip_frame, recover
;					exc_addr from PT__STACK_PC
;					because p23 has been trashed.
;				(2) Turn icache fill mchks into crd mchks.
;				(3) Turn recoverable dc_tag_perr into crd mchk.
;				(4) Eliminate VA and VA_CTL from mchk frame.
;				(5) Add mchk and mchk_crd revision.
; ES	1.6	01-Aug-97	(1) Add cserve code for DP264 debug monitor.
;				(2) Add clock interrupt clear for DP264.
;				(3) Add ev6_p2 conditional to C_ADDR_SHIFT
;					definition.
;				(4) On DFAULT for DC_TAG_PERR, need to turn
;					off DCTAG_PAR_EN to avoid a mchk on
;					the ECB.
;				(5) Add sample tsunami/DP264 clear interrupt
;					pending code.
;				(6) For p1, in restore_state, restore
;					CNS__FPE_STATE from CNS__PCTX to
;					keep it coherent with CNS__PCTX.
;				(7) In save_state, eliminate double mm_stat save.
;				(8) Go back to having PT__M_CTL as well as
;					CNS__M_CTL so we have a live and
;					saved version.
;				(9) Add va_48 conditional for xx_CTL__INIT
;					values.
;				(10)Added tb_mb_en conditional.
;				(11)Added mchk_en conditional.
;				(12)Dismiss missing/faulting WH64/ECB.
;				(13)On swppal with srm_console, get VA_48
;					status and set SPE accordingly.
;				(14)Handle I_CTL<SDE> with 5 fetch blocks
;					instead of jmp_stall to avoid
;					a conjectured race condition.
;				(15)In ev6_p1 initialization, corrected
;					zap of CNS__x31_EMUL.
;				(16)Eliminate unnecessary jmp_stalls in
;					switch pal code and power up.
;				(17)In emulator code, map 4MB region
;					depending on location of emulator.
;					Make stack space = ^x400.
;				(18)On IMB, do an MB.
;				(19)In CSERVE jtopal, add mb and 
;					or in 1 to r16 to form pal address.
;				(20)Handle FPE with 5 fetch blocks.
;				(21)In emulator, load PT_WHAMI off of
;					p_temp, NOT p4.
; ES	1.7	25-Aug-97	(1)Add dcache_set_en conditional. In p1,
;					default to 1. In p2, default to 3.
;					Use in dc_tag_perr processing and
;					reset. Also add an MB in dc_tag_perr.
; ES	1.8	12-Sep-97	(1)Changes to DTBM_SINGLE, DTBM_DOUBLE_3
;					and DTBM_DOUBLE_4. Get VA
;					before read of VPTE. Use p7<0>
;					as a double miss flag. On double
;					miss, put mm_stat from p5 into
;					<31:15> of p7, which contains
;					exc_sum and double miss flag.
;					Use p5 for exc_addr of double miss.
;					Restore mm_stat into p5 on
;					invalid_dpte when double miss.
;				(2)In ITB_MISS, don't use p5 until
;					after the VPTE fetch.
;				(3)Need CONT_HW_VECTOR in double_4 flow. 
; ES	1.9	18-Sep-97	(1)Emulator called in kseg. Need matching
;					emulator built for kseg.
;				(2)Xor in dtb miss before load virtual.
;					Ensures va is in hand.
; ES	1.10	24-Sep-97	(1)Add counter for fp emulates. Modifications
;					in swppal, fen, new call_pal ^xAD, to
;					get count.
;				(2)Pass serial line interrupts off to
;					platform specific call.
;				(3)Add a hw_ret after sys__int_err for pvc.
; ES	1.11	08-Oct-97	(1)Add clrmap symbol and clrmap call_pal.
;				   	Note: clrmap=1 is required to get
;					clrmap!
;				(2)In sys__enter_console, update PT__VA_CTL
;				   	to reflect change to VA_CTL, so that
;				   	PALcode running while console is
;					running doesn't have problems
;					flipping modes.
;				(3)In trap__switch_base, have conditional
;					code which turns on multi-issue.
;					Requested by DP264 debug team.
; ES	1.12	24-Oct-97	(1)In trap__reset, use r26 when branching to
;					sys__reset, as dp264 srom passes
;					up parameter in r1.
;				(2)Eliminate use of SROM conditional in
;					WAKEUP. Not testing that way
;					anymore.
;				(3)Add SROM parameter passing for DP264 debug
;					monitor	in the system code.
;				(4)Change beh_model default to 0.
;				(5)Add dbm_serial_io conditional for debug
;					monitor	serial line i/o.
; ES	1.13	31-Oct-97	(1)Fix typo in system code. Change _INIT to
;					__INIT in EV6__DC_CTL__INIT.
;				(2)On dc_tag_perr, only halt on dc_tag_perr
;					while in pal_mode. Add a new halt
;					code HALT__DC_TAG_PERR_FROM_PAL.
;					On mchk in progress, just dismiss
;					after fixing.
;				(3)Save mm_stat in short frame. Before
;					merge to sys__crd_merge, each
;					flow must write MCHK_CRD__MM_STAT.
;					The dc_tag_perr error writes a
;					useful mm_stat, showing the error.
;				(4)In system pal, in sys__crd, correct typo
;					in building p_misc: use p6, not p4.
;				(5)In swppal, use a quadword for PAL__ENTER_OSF.
; ES	1.14	04-Nov-97	(1)In exit_console, on load/unlock, set
;					protection to all RWE in case we
;					are exiting into user mode (we
;					have already changed mode at
;					this point).
;				(2)Add CSERVE__START to non-srm console cserve.
;				(3)Add pvc labels for bsr emul_restore
;					so that pvc doesn't terminate
;					permutation on subroutine return.
; ES	1.15	17-Dec-97	(1)Change DC_STAT field names.
; ES	1.16	18-Dec-97	(1)Don't zap low nibble of counters on writing
;					performance counters.
;				(2)In machine flow, move read and store of isum
;					earlier so we only read isum for system
;					machine interrupt once.
;				(3)Put MB in double flow in ev6_p1 conditional.
;				(4)Put CLRMAP in ev6_p1 conditional, and make
;					default CLRMAP = 1.
;				(5)Add some PVC_VIOLATE <35>'s. (HW_INT_CLR)
;					Re-arrange sys__crd_skip_frame a bit.
; ES	1.17	12-Jan-98	(1)In arithmetic exception, ignore SWC bit
;					when checking whether to take an
;					exception.
; ES	1.18	21-Jan-98	(1)Another PVC_VIOLATE <35>, this one in switch
;					palcode.
;				(2)Change <ev6_p1 ! ev6_p2> conditional to
;					just ev6_p1 in cbox read chain code.
; ES	1.19	04-Feb-98	(1)Read ISUM twice in interrupt to minimize the
;					possibility of a read/write conflict
;					causing a read of 0.
;				(2)In perfmon, zap counter on disable for
;					both pass1 AND pass2.
;				(3)Add check for double bit error before
;					istream error in mchk.
;				(4)Add check for istream mchk in crd flow.
;				(5)In reset, read the cbox error read chain
;					to clear it out.
;				(6)In reset, comment out clearing fpcr. It's
;					unpredictable anyway, and it's possibly
;					set up by the srom.
;				(7)In sys__mchk and sys__mchk_clear_crd,
;					change r5 to p5 (just a name change).
; ES	1.20	09-Feb-98	(1)In perfmon, fix typo in conditional
;					that turns on zapping counter
;					for both pass1 and pass2.
; ES	1.21	13-Mar-98	(A)Large rewrite of error handling.
;				(1)sys__int_err -- does more logging and then
;					branches to sys__mchk_header.
;				(2)sys__crd
;					(a)Now scrubs under certain conditions.
;						Does it before merge point from
;						630's.
;					(b)Now logs i_stat, dc_stat before merge
;						from 630's. Only clear crd-type
;						errors in dc_stat in case there
;						is a delayed mchk in the wings.
;					(c)Crd from speculative mchk now handled
;						directly in this code.
;					(d)New merge point is sys__crd_header.
;					(e)Dpc and pce flows also scrub
;						as necessary.
;				(3)sys_crd_scrub -- new routine to scrub. Also
;					includes a store to make block dirty.
;				(4)sys__mchk_dc_tag_perr
;					(a)Does more logging before merging at
;						sys__crd_header.
;					(b)Has its own dpc and pce flows.
;				(5)sys__mchk
;					(a)Now logs i_stat, dc_stat before merge
;						from 660's.
;					(b)New merge point is sys__mchk_header.
;					(c)Istream mchk to crd now does more
;						logging before branching to
;						sys__crd_header.
;				(6)sys__mchk_scrub -- includes a store to make
;					block dirty.
;				(7)trap__pal_bugcheck -- does more logging
;					before branching to sys__mchk_header.
;				(8)perfmon -- minor change to an ASSUME.
;
;				(B)Turn hw_jmp's, jmp's into hw_ret, ret's to
;				avoid issuing instructions in PALmode on random
;				cache line predictions.
; ES	1.22	31-Mar-98	In sys__crd_scrub, need to save off p7
;					before calling sys__cbox, and then
;					restore on return.	
; ES	1.23	29-Apr-98	(1)In trap__save_state and trap__restore_state,
;					don't save and restore pctr_ctl
;					(which causes the counter
;					to increment since we get a
;					post-incremented copy from the chip
;					even if counting is disabled).
;				(2)Add comments and definitions relating to
;					ev6_p3 changes of I_STAT.
; ES	1.24	15-May-98	(1)In double flows, force a stall until
;					the pte writes retire to avoid
;					issue of tag write in single flow
;					before retire of pte write in double
;					flow.
; ES	1.25	18-May-98	(1)In sys__reset, just check for 'DEC', not
;					'DECB'.
;				(2)Add reference platform support for use
;					of write_many_chain.
; ES	1.26	29-Jun-98	(1)Add sleep support (First cut).
;				(2)On save_state, just zero exc_sum value.
;					Not useful and can be misleading.
;				(2)In arith exception, zap the upper portion
;					of exc_sum so we don't step on
;					fpcr.
; ES	1.27	09-Jul-98	(0)Stick reference_platform conditionals
;					around wtint and wakeup.
;				(1)In sys__mchk_scrub, map
;					address and then scrub using
;					ldq_l, stq_c. Also move mb.
;				(2)In sys__mchk_clear_crd, also clear
;					dc_stat, and force ipr writes
;					to finish.
;				(3)In sys__crd, look for a double
;					bit error that may have
;					occurred just after the single
;					bit error.
;				(4)In sys__crd, don't scrub for
;					dstream_dc_err.
;				(5)In sys__crd_merge, also
;					clear dc_stat and force
;					ipr writes to finish.
;				(6)In sys__crd_skip_frame, don't
;					scrub for dstream_dc_err.
;				(7)In sys__crd_skip_frame_merge, also
;					clear dc_stat and force
;					ipr writes to finish.
;				(8)In sys__crd_scrub, map
;					address and then scrub using
;					ldq_l, stq_c. Also move mb.
;				(9)Put in pte eco.
;				(10)Add call_pal for debug monitor. Add
;					force_ecc, and set clrmap default
;					to 0.
; ES	1.28	21-Jul-98	(1)Clean up comments on force_ecc.
;				(2)Fix scoreboard bit on sleep stuff.
;				(3)Add check_mem call_pal.
;				(4)Work around for rdblkmodify nxm bug.
; ES	1.29	27-Jul-98	Do some more tweaking on nxm bug.
; ES	1.30	27-Jul-98	More tweaking
; ES	1.31	28-Jul-98	More tweaking
; ES	1.32	30-Jul-98	Optimize kseg miss to do GH=3 and ASM=1.
;				Fix comments on dc_tag_perr.
;				Fix bug in i/o space checking in kseg hack.
;					Check bit 40, which is sign
;					extended into 43 on 43 bit kseg.
; ES	1.33	03-Aug-98	Undo istream kseg hack.
; ES	1.34	05-Aug-98	In RTI/RETSYS remove unnecessary stl to clear
;				locks.
; ES	1.35	06-Aug-98	(1)Put kseg hack in conditionals.
;				(2)Take out check_mem call_pal.
;				(3)In enter console, add mm_stat stall at the
;					end to interlock everything.
; ES	1.36	25-Aug-98	Avoid speculative tag overwrite by holding it
;					up until previous PTE write retires.
;					Keeping the tag and PTE write in a
;					fetch block holds up speculative
;					set in the mapper, but add insurance
;					with scoreboard bits. The ALIGN
;					macro is marked with 1.36.
;				Also take cycle counts out of miss flows.
; ES	1.37	27-Aug-98	Urti -- take pc from stack_pc on illop because
;					a miss to the stack steps on p23.
;					Also take out stq to clear lock because
;					the loads will take care of it.
; ES	1.38	08-Sep-98	(1)Wrperfmon -- add subfunctions fo
;					profileme.
;					Eliminate return status in r0<0>.
;					Add some waits for retires (though it
;					probably doesn't really matter).
;					Change how nibbles are zapped on
;					disable.
;				(2)Explicitly define i_stat tpe and dpe 2.3
;					bits since they are no longer in
;					ev6_defs.
; ES	1.39	16-Sep-98	On a sequence of virtual operations, allow for
;				misses on each reference even if they are to
;				the same page. The virtual references can start
;				after the issue of an unrelated MTPR PTE/TAG
;				set, and have its tb entry kicked out when
;				the MTPR PTE/TAG retires. Marked with 1.39.
;				Affects in ev6_osf_pal:	fen, unalign,
;				  invalid_dpte, dfault, opcdec, iacv,
;				  invalid_ipte, arith, bpt, call_sys, urti, rti
;				Affects in ev6_osf_system_pal: int_post
; ES	1.40	22-Sep-98	On an mp system, the other cpu can mark PTEs
;				with fault bits to indicate they are LRU
;				candidates in the VM handler. So we have
;				to ensure that we see the same PTE throughout
;				the entire urti operation by doing a
;				scoreboard stall at the beginning. This
;				shouldn't be a problem with the kernel stack
;				operations.
; ES	1.41	21-Oct-98	(1)In trap__mchk, add some code that helps
;				eliminate double mchk's.
;				(2)In trap__mchk, fix flow in internally
;				detected bugchecks. Had a couple bugs
;				from rework of mchk a while back.
;				(3)On istream mchk, check on cmov at pc-4 and
;				punt if so. Bug in EV6 -- the cmov may or
;				may not have been executed.
;				(4)STx_C erroneous succeed problem -- A
;				ldx_l/stx_c sequence with an exception
;				in the middle that doesn't do a virtual
;				operation (which breaks the lock) and that
;				can have unknown bad path code that pulls back
;				in the locked block after it has been
;				pulled away from another processor. Fixed
;				as I100 in ev67, where a hw_ret mispredict
;				will cause the lock to be cleared. Handled here
;				with FORCE_PATH conditional in the
;				following locations:
;					dtb single miss
;					itb single miss
;					dtb double miss
;					interrupt passive release
;					crd passive relesae
;				Fixed with a hw_jmp followed by a
;					infinite loop branch in the
;					same fetch block. None of the
;					various type of predictors will
;					predict other than back to the jmp,
;					allowing no bad path code.
;				(4)In sys__reset and sys__enter_console, get
;					ipl31 from ipl table.
;				(5)In sys__enter_console, add some NOPs.
;				(6)In sys__reset, add write to hw_int_clr
;					for perf counter restriction.
; ES	1.42	04-Nov-98	(1)In ev6_vms_system_pal, change
;					assume_fetch_block <...> to
;					align_fetch_block <...>.
;				(3)In force_path hack, use pvc_violate <1008>
;					on all but 1 of each branch to
;					sys__cbox and sys__crd_scrub. The
;					pvc label skips the branch on goes
;					on to the next instruction. This will
;					speed up pvc considerably.
; ES	1.43	09-Nov-98	Make the new pvc happy with pvc_violate <1008>
;					sections by enforcing ipr operations
;					in different fetch blocks.
; ES	1.44	24-Nov-98	New force_path2 hack. Leave force_path=0.
;				ITB_MISS -- do a new hack
;					to force mispredict before a load
;					can fire off
;				DTB_MISS -- do a ldbu hack
;				DOUBLE -- do the new hack
;				1to1 mapping cases -- ignore
;				interrupt dismiss -- do the new hack
;
;				Also, minor fix in trap__ldvpte_dfault
; ES	1.45	16-Dec-98	Only do ldbu on force_path2 hack if
;				gh !=0. If gh !=0, we DON'T HIT OFF
;				the holding latch EVEN THOUGH WE HAVE
;				BEEN ALLOWED TO ISSUE -- thus the ldbu
;				takes a corrupting tb miss.
; ES	1.46	11-Jan-99	Development version.
; ES	1.47	12-Jan-99	(1)On call_pals, ev6 erroneously pushes
;				the prediction stack twice.
;				Add add_extra_ret conditional that
;				adds extra ret to call_pal macro, but
;				leave it turned off. If the stack is not
;				deep, the mispredict that it causes
;				is more harmful. We leave it around for
;				documentation of the problem.
;				(2)Fix comments on 2.3 perfmon.
;				Retired branch mispredicts are not
;				counted.
; ES	1.48	08-Mar-99	New PALcode restriction. On writing any
;				ebox ipr (va_ctl,cc,cc_ctl), it is necessary
;				to force its retire before any close-in-
;				program-order mispredict from a branch,
;				hw_ret_stall, or dtb miss.
;				The younger mispredict can actually stop the
;				ipr write even though the instruction is older
;				and retires.
; ES	1.49	12-Mar-99	Add double branch to ldbu hack in order
;				to avoid speculative issue of ldbu, which
;				causes the exact problem we are trying
;				to fix. As long are we are there,
;				conditionalize the d1to1 check to
;				compact the code.
; ES	1.50	09-Apr-99	(1)Ldbu hack enhanced once last time.
;				(2)Eliminate hw_jmp in sys__cbox.
;-
	vmaj = 1
	vmin = 50
	vms_pal = 1
	osf_pal = 2
	pal_type = osf_pal
	osfpal_version_l = <<pal_type@16> ! <vmaj@8> ! <vmin@0>>
;
; EV6 hardware definitions
;
	PAL_SHADOW_DEFS
	MCES_DEFS
	MM_DEFS
	MM_STAT_DEFS
	HALT_DEFS
	MCHK_DEFS
	CBOX_IPR_DEFS
;
; OSF definitions
;
	OSFPAL_FUNC_DEFS
	OSF_PTE_DEFS
	OSF_P_MISC_DEFS
	OSF_PS_DEFS
	OSF_SCB_DEFS
	OSF_PCB_DEFS
	OSF_FRM_DEFS
	OSF_A0_DEFS
	OSF_MMCSR_DEFS
	OSF_IPL_DEFS

; Configuration options
;
.iif ndf ev6_p1, ev6_p1 = 0
.iif ndf ev6_p2, ev6_p2 = 1
;
; Pass 3 chips will work with ev6_p2. The ev6_p3 definition just changes
; some I_STAT definitions, but it will not affect functionality.
;
.iif ndf ev6_p3, ev6_p3 = 0

ASSUME <ev6_p1+ev6_p2+ev6_p3> eq 1

.iif ndf osfpal, osfpal = 1
.iif ndf beh_model, beh_model = 0
.iif ndf focus, focus = 0
.iif ndf srom, srom = 1
.iif ndf srm_console, srm_console = 1
.iif ndf reference_platform, reference_platform = 0
.iif ndf pte_eco, pte_eco = 1
.iif ndf check_interrupt_pending, check_interrupt_pending = 0
.iif ndf dbm_serial_io, dbm_serial_io = 0

.iif ndf va_48, va_48 = 0
.iif ndf tb_mb_en, tb_mb_en = 0
.iif ndf mchk_en, mchk_en = 1
.iif ndf fp_count, fp_count = 0
.iif ndf clrmap, clrmap = 0
.iif ndf force_ecc, force_ecc = 0
.iif ndf force_multi_issue, force_multi_issue = 0
.iif ndf kseg_hack, kseg_hack = 0
.iif ndf spinlock_hack, spinlock_hack = 0		; 1.41
;
; Turn off force_path. Use force_path2 conditionally.		; 1.44
;
force_path = 0							; 1.44
.iif ndf force_path2, force_path2 = 0				; 1.44


.if ne ev6_p1
	.iif ndf dcache_set_en, dcache_set_en = 1
.iff
	.iif ndf dcache_set_en, dcache_set_en = 3
.endc

.if ndf	max_cpuid
	max_cpuid = 2
.endc
.if ndf	osf_svmin		; platform specific palcode version number
	osf_svmin = 0
.endc

osfpal_version_h = <<max_cpuid@16> ! <osf_svmin@0>>

;
; EV6 shadow register usage 
;
;	p4	= p4	reserved for itb/dtb miss
;	p5	= p5	reserved for itb/dtb miss
;	p6	= p6	reserved for itb/dtb miss
;	p7	= p7	reserved for itb/dtb miss
;
;	p20	= r20	reserved for call_pal and non-miss handling routines
;	p21	= r21	address of pal temps in memory (p_temp)
;	p22	= r22	p_misc (phys_mode in <63> and ps in <15:0>)
;	p23	= r23	call_pal linkage register
;

;
; Notes on speculative execution, non-renamed hardware registers, and
; branch prediction.
;
; EXC_ADDR, IVA_FORM, EXC_SUM, VA_FORM, VA, MM_STAT are sourced by
; non-renamed hardware registers that need to be available for subsequent traps.
; Hardware protects the values from overwrite for the first fetch block
; (4 instructions) of a pal flow. During this fetch block, the PALcode should
; save whatever registers it will need.
;
; In addition, VA, VA_FORM, DC_CTL and MM_STAT can have issue order problems
; with subsequent loads that issue out of order in respect to the mxfr from/to
; these registers. A MB before the load can be used to enforce the issue order.
; 
; Note that branch prediction is turned off in PALmode. The fall-through
; path should be the common path. Also note that in the exception flows,
; it takes a fetch block to turn off branch prediction, so therefore
; there should be no conditional branches in the first fetch block.
;
; We can also use this fact to help us avoid the overwrite problems
; For example, a dismiss ofa dtb miss is down a branch path, and thus is not
; speculatively issued. If the taken branch depends on these registers, we
; have enforced issue order.
;


;+
; SWITCH - offset 0
;
; Entry:
;	This entry is conditionalized for swap from vms palcode.

;	This entry could also be reached erroneously on a jump through 0.
;
; Entry on SWPPAL from VMS PALcode:
;	r17(a1)		new PC
;	r18(a2)		new PCBB
;	r19(a3)		new VPTB
;	p_misc		switch bit should be set
;	p_temp		valid
;
; Exit conditions on SWPPAL from VMS PALcode:
;	ASN		from PCB
;	FEN		from PCB
;	IPL		7
;	MCES		0 (should it be 8??)
;	PCBB		passed to SWPPAL
;	PC		passed to SWPPAL
;	PS		IPL=7, CM=K
;	PTBR		from PCB
;	Unique		from PCB
;	WHAMI		unchanged
;	Sysvalue	unchanged
;	KSP		from PCB
;	r0		0
;	other IPRs	UNPREDICTABLE
;	other rx,fx	UNPREDICTABLE except r0 and r30
;-
.if ne srm_console

	EV6__SWITCH_ENTRY = ^x0

TRAP__START:					; used by vector macros
trap__pal_base:

	START_HW_VECTOR <SWITCH>

	srl	p_misc, #OSF_P_MISC__SWITCH__S, r1	; check the switch bit
	br	r31, trap__check_switch

	.long	osfpal_version_l			; location 0x8
	.long	osfpal_version_h			; location 0xC

trap__check_switch:
	blbc	r1, sys__switch_unknown			; jumped through 0
	br	r31, trap__switch_base			; been switched

	.align	quad
trap__lock_cell:
	.quad	0

	CONT_HW_VECTOR				; continue in free space

;
; Init and w1c values
;
	EV6__I_STAT__TPE__S = ^x1d
	EV6__I_STAT__DPE__S = ^x1e

	EV6__I_STAT__W1C = -
		<<1@EV6__I_STAT__TPE__S> ! -
		<1@EV6__I_STAT__DPE__S>>

	EV6__DC_STAT__W1C = -
		<<1@EV6__DC_STAT__TPERR_P0__S> ! -
		<1@EV6__DC_STAT__TPERR_P1__S> ! -
		<1@EV6__DC_STAT__ECC_ERR_ST__S> ! -
		<1@EV6__DC_STAT__ECC_ERR_LD__S> ! -
		<1@EV6__DC_STAT__SEO__S>>

	ASSUME EV6__HW_INT_CLR__MCHK_D__S lt EV6__HW_INT_CLR__PC__S 
	ASSUME EV6__HW_INT_CLR__MCHK_D__S lt EV6__HW_INT_CLR__CR__S 
	ASSUME EV6__HW_INT_CLR__MCHK_D__S lt EV6__HW_INT_CLR__SL__S 

	EV6__HW_INT_CLR__INIT = -
	    <<1@<EV6__HW_INT_CLR__MCHK_D__S - EV6__HW_INT_CLR__MCHK_D__S>> ! -
	     <3@<EV6__HW_INT_CLR__PC__S - EV6__HW_INT_CLR__MCHK_D__S>> ! -
	     <1@<EV6__HW_INT_CLR__CR__S - EV6__HW_INT_CLR__MCHK_D__S>> ! -
	     <1@<EV6__HW_INT_CLR__SL__S - EV6__HW_INT_CLR__MCHK_D__S>>>

	EV6__IER__INIT = 0

;
; Do the intialization. We rely on fetch blocks to separate scoreboard bits!!!
;
	ALIGN_FETCH_BLOCK

trap__switch_base:
	br	r1, trap__switch_get_pal_base
trap__switch_get_pal_base:
	GET_32ADDR r1, <trap__pal_base - trap__switch_get_pal_base>, r1
	hw_mtpr	r1, EV6__PAL_BASE			; (4,0L) pal base

	hw_mtpr	r31, EV6__ITB_IA			; (4,0L) flush ITB
	hw_mtpr	r31, EV6__DTB_IA			; (7,0L) flush DTB
	GET_32CONS 	r1, EV6__I_STAT__W1C, r31	; get i_stat clr value

	GET_16CONS	R3, EV6__DC_STAT__W1C, R31	; get dc_stat clr value
	hw_mtpr	r1, EV6__I_STAT				; (4,0L) w1c I_STAT
	hw_mtpr r3, EV6__DC_STAT			; (6,0L) w1c DC_STAT
	rc	r31					; clear intr_flag

	GET_32CONS	r1, EV6__IER__INIT, r31 
	hw_mtpr	r1, EV6__IER_CM				; (4,0L) clear ier_cm
	hw_mtpr	r31, EV6__DTB_ALT_MODE			; (6,0L) clear alt_mode

	hw_stq/p r31, PT__DTB_ALT_MODE(p_temp)		; clear alt_mode temp
	zapnot	p_misc, #4, p_misc			; zap all but mces
	bis	p_misc, #7, p_misc			; cm=0,ipl=7
	hw_mtpr	r31, EV6__SIRR				; (4,0L) Clear int req
	
	GET_16CONS	r1, EV6__HW_INT_CLR__INIT, r31
	sll	r1, #EV6__HW_INT_CLR__MCHK_D__S, r1	; move into position
	PVC_VIOLATE <35>				;
	hw_mtpr	r1, EV6__HW_INT_CLR			; (4,0L) Clear int req
	NOP						; pad fetch block
;
; Current state:
;	r17 (a1)	new PC
;	r18 (a2)	new PCBB
;	r19 (a3)	new VPTB
;
	hw_ldq/p p4, OSF_PCB__FEN(r18)		; get new fen/pme
	hw_ldl/p p5, OSF_PCB__CPC(r18)		; get charged cycle counter
	hw_ldl/p p6, OSF_PCB__ASN(r18)		; get new asn
	hw_ldq/p p7, OSF_PCB__PTBR(r18)		; get new ptbr

	sll	p7, #page_offset_size_bits, p7	; convert pfn to pa
	hw_stq/p p7, PT__PTBR(p_temp)		; store PTBR
	hw_stq/p r18, PT__PCBB(p_temp)		; store PCBB
	bic	r17, #3, p23			; clean PC to p23
;
; Initialize VA_CTL with new VPTB value and saved control value.
; Store VPTB in PT__VPTB in pal temps area.
; Initialize M_CTL based on VA_48, and save it away.
; Set up initialization of I_CTL, with VPTB, old control value, and
;		SPE based on VA_48.
;
	hw_ldq/p r1, PT__VA_CTL(p_temp)		; get control part
	hw_stq/p r19, PT__VPTB(p_temp)		; store vptb base part
	bis	r1, r19, r1			; or in vptb part
	hw_mtpr	r1, EV6__VA_CTL			; (5,1L)

	hw_mfpr	r1, EV6__I_CTL			; (4,0L) read I_CTL
	lda	p4, 2(r31)			; get SPE<1> bit
	srl	r1, #EV6__I_CTL__VA_48__S, r3	; get VA_48 bit
	and	r3, #1, r3			; r3=1 -> VA_48

	sll	p4, r3, p4			; shift to SPE<2> if VA_48
	sll	p4, #EV6__M_CTL__SPE__S, r3	; shift into M_CTL position
.if ne kseg_hack				; kseg hack
	hw_mtpr	r31, EV6__M_CTL			; kseg hack
	hw_stq/p r31, PT__M_CTL(p_temp)		; kseg hack
.iff
	hw_mtpr	r3, EV6__M_CTL			; (6,0L) write M_CTL
	hw_stq/p r3, PT__M_CTL(p_temp)		; save away
.endc

	sll	p4, #EV6__I_CTL__SPE__S, r3	; shift into I_CTL position
	bis	r1, r3, r1			; or into I_CTL
	sll	r1, #<64 - EV6__I_CTL__VPTB__S>, r1	; clean VPTB
	srl	r1, #<64 - EV6__I_CTL__VPTB__S>, r1	; clean VPTB

	bis	r1, r19, r1			; or in new VPTB

.if ne force_multi_issue
	lda	p7, 1(r31)				; get a 1
	sll	p7, #EV6__I_CTL__SINGLE_ISSUE__S, p7	; into position
	bic	r1, p7, r1				; clear single issue
.endc

;
; Initialize I_CTL. Write it twice (pvc #31).
;

	hw_mtpr	r1, EV6__I_CTL				; (4,0L) write I_CTL
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	hw_mtpr	r1, EV6__I_CTL				; (4,0L) write I_CTL
;
; Do USP and KSP
;
	hw_ldq/p r30, OSF_PCB__USP(r18)		; get new USP
	hw_stq/p r30, PT__USP(p_temp)		; save it away
	hw_ldq/p r30, OSF_PCB__KSP(r18)		; get KSP
;
; Now do DTB_ASNx.
;
; There must be a scoreboard bit -> register dependency chain to prevent
; hw_mtpr DTB_ASx from issuing while ANY of scoreboard bits <7:4> are set.
;
; Current state:
;	p6	ASN
;
ASSUME OSF_PCB__ASN__M eq ^xFF

	and	p6, #OSF_PCB__ASN__M, p6	; clean ASN quadword
	sll	p6, #EV6__DTB_ASN0__ASN__S, p20	; ASN into mbox spot

	hw_mfpr	p7, <EV6__PAL_BASE ! ^xF0>	; (4-7, 0L)
	xor	p7, p7, p7			; zap p7
	bis	p7, p20, p20			; force register dependency
	bis	r31, r31, r31
	hw_mtpr	p20, EV6__DTB_ASN0		; (4,0L)
	hw_mtpr	p20, EV6__DTB_ASN1		; (7,1L)
;
; Create Ibox Process Context IPR.
; Fill in ASN, FPE, clearing ASTEN, ASTRR, PPCE.
;
; Current state:
;	p4	fen/pme quadword
;	p5	cpc
;	p6	asn
;	p23	new pc
;
	sll	p6, #EV6__PROCESS_CONTEXT__ASN__S, p6	; shift asn into place
	and	p4, #1, p4				; get just the fen bit
	sll	p4, #EV6__PROCESS_CONTEXT__FPE__S, p7	; shift fpe into place
	bis	p7, p6, p7				; or together
	hw_mtpr	p7, EV6__PROCESS_CONTEXT		; (4,0L) write them

.if ne ev6_p1
	hw_ldq/p p7, PT__IMPURE(p_temp)			; get impure pointer
	hw_stq/p p4, CNS__FPE_STATE(p7)			; write FPE_STATE
    .if ne fp_count
	hw_stq/p r31, PT__RSV_FOR_PAL(p_temp)		; clear counter
    .endc
.endc

;
; Do the cycle counter
; Not sure we need this, but ensure 4-7 clear before any dtb writes.
; Current state:
;	p5		CPC from PCB

	rpcc	r1				; get cycle counter
	subl	p5, r1, r1			; gen new offset
	insll	r1, #4, r1			; shift left 32
	hw_mtpr r1, <EV6__CC ! ^xF0>		; (4-7,1L) write it
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31
;
; Now we need to clear the lock flag, which must be done with a LDQ
; So we need to set up the DTB, do the write, and re-clear the DTB.
;
; Write the dtb, do the reference, and then clear the dtb.
;
	hw_mfpr	r1, EV6__PAL_BASE		; (4,0L) get pal base back
	sll	r1, #<32-13>, r2		; shift into position
	lda	r3, ^x1101(r31)			; set KWE, KRE
	or	r2, r3, r2			; produce pte

	ALIGN_FETCH_BLOCK <^x47FF041F>		; Edit 1.36

	PVC_VIOLATE <2>				; ignore scoreboard violation
	hw_mtpr	r1, EV6__DTB_TAG0		; (2&6,0L) write tag0
	hw_mtpr r1, EV6__DTB_TAG1		; (1&5,1L) write tag1
	hw_mtpr	r2, <EV6__DTB_PTE0 ! ^x44>	; (0,4,2,6) (0L) write pte0
	hw_mtpr	r2, <EV6__DTB_PTE1 ! ^x22>	; (3,7,1,5) (1L) write pte1
;
; Do the unlock.
;
	ALIGN_FETCH_BLOCK <^x47FF041F>
	ldq	r31, <trap__lock_cell - trap__pal_base>(r1)
;
; Now clear the dtb again. The stall at the end will do the
; necessary synchronization.
;
	hw_mtpr	r31, EV6__DTB_IA		; (7,0L)
;
; Mark success, flush icache, and hw_ret_stall to pc.
;
	or	r31, r31, r0			; success
	hw_mtpr	r31, EV6__IC_FLUSH		; (4,0L) flush icache
	bne	r31, .				; pvc #24
	hw_ret_stall (p23)			; switch complete

	END_HW_VECTOR

.endc ; if ne srm_console


;+
; POWERUP - offset 0
;
; Entry:
;	This entry is conditionalized for jump from SROM code
;	on non-SRM console systems.
;
;	Jumped to by the SROM code on reset. Vector 780 will be used for
;	wakeup. It is assumed that the retirator, the integer/floating
;	map, and the shadow registers have been initialized.
;
; Reference platform standard:
;	r0 - r5		chip specific srom parameters
;	r15 - r21 	standard srom parameters (even though
;			it steps on shadow set)
;-
.if eq srm_console

	EV6__POWERUP_ENTRY = ^x0
TRAP__START:					; used by vector macros
trap__pal_base:
	START_HW_VECTOR <POWERUP>

	br	r26, sys__reset			; off to sys__reset

	.long	0
	.long	osfpal_version_l			; location 0x8
	.long	osfpal_version_h			; location 0xC

	.align	quad
trap__lock_cell:
	.quad	0

	END_HW_VECTOR

.endc ; if eq srm_console


;+
; DTBM_DOUBLE_3 - offset 100
;
; Entry:
;	Vectored into via hardware trap on TB miss on level 3
;	page table entry.
;
; Function:
;	Translate level 3 PTE va.
;	If valid, load TB and return to redo the ld_vpte, which will now hit.
;	If not valid, then take TNV/ACV exception.
;
; Current state:
;	p4	va of level 3 page table entry
;	p5	mm_stat (from dtbm_single)
;	p6	original va
;	p7	exc_sum (from dtbm_single)
;	p23	pc of instruction causing the TB miss
;
;	VA	original va for dstream TB miss
;
; Register Use:
;	p5	exc_addr, pc of ld_vpte in TB miss flows
;	p7	<31:16>=mm_stat<15:0>,<15:1>=exc_sum<15:1>,<0>=1
;
;	r25	saved, used as scratch, restored on hw_ret
;	r26	saved, used as scratch, restored on hw_ret
;
; Exit state:
;	On exit to trap__double3_pte_inv
;	p5	scratch
;	p6	original va
;	p7	<31:16>=mm_stat<15:0>,<15:1>=exc_sum<15:1>,<0>=1
;	p23	pc of instruction causing TB miss
;	r25	pte
;-
	START_HW_VECTOR <DTBM_DOUBLE_3>

; Entered on a miss on va of level 3 page table entry
; 	level 1 = vptb
; 	level 2 = level 1 of original va
; 	level 3 = level 2 of original va
; byte_within_page = level 3 of original va as offset into level 3 page table
;
; Optimize by starting with the fetch of level 2 pte
;
	zapnot	p7, #3, p7			; clean exc_sum to 16 bits
	sll	p5, #16, p5			; shift mm_stat into place
	bis	p7, p5, p7			; combine 
	hw_mfpr	p5, EV6__EXC_ADDR		; (0L) save exc_addr

	bis	p7, #1, p7			; double miss flag

	hw_stq/p r25, PT__R25(p_temp)		; get a scratch register
	hw_stq/p r26, PT__R26(p_temp)		; get a scratch register
	hw_ldq/p r25, PT__PTBR(p_temp)		; get phys page table addr

	blt	p_misc, trap__double3_1to1	; 1-to-1 => branch

	sll	p4, #<64-<<2*level_bits>+13>>, r26
	srl	r26, #<61-level_bits>, r26	; get level 2 into offset
	addq	r25, r26, r25			; pa for level 2 pte
	hw_ldq/p r25, 0(r25)			; get level 2 pte
	sll	p4, #<64-<<1*level_bits>+13>>, r26
	srl	r26, #<61-level_bits>, r26	; get level 3 into offset
	blbc	r25, trap__double3_pte_inv	; branch => invalid pte
	srl	r25, #32, r25			; extract pfn from pte
	sll	r25, #13, r25			; get into position
	addq	r25, r26, r25			; pa for level 3 pte
	hw_ldq/p	r25, 0(r25)		; get level 3 pte
	blbc	r25, trap__double3_pte_inv	; branch => invalid pte
	srl	r25, #7, r26			; get mb bit

	ALIGN_FETCH_BLOCK <^x47FF041F>		; Edit 1.36

	PVC_VIOLATE <2>				; ignore scoreboard violation
	hw_mtpr	p4, EV6__DTB_TAG0		; (2&6,0L) write tag0
	hw_mtpr p4, EV6__DTB_TAG1		; (1&5,1L) write tag1
	hw_mtpr	r25, <EV6__DTB_PTE0 ! ^x44>	; (0,4,2,6) (0L) write pte0
	hw_mtpr	r25, <EV6__DTB_PTE1 ! ^x22>	; (3,7,1,5) (1L) write pte1

	hw_ldq/p r25, PT__R25(p_temp)		; restore register
;
; We need to wait until the pte writes retire. We need to avoid the
; case where the tag writes in the single flow issue before the
; pte writes in this flow issue and retire. With changes in 1.36, we
; can probably remove this stall, but at this point in the project,
; let's just leave it in.
;
	hw_mtpr	r31, <EV6__MM_STAT ! ^x80>	; wait for pte write

	CONT_HW_VECTOR <DTBM_DOUBLE_3>

ASSUME <tb_mb_en + pte_eco> ne 2

.if eq force_path2				; 1.44 force_path2 = 0

  .if ne pte_eco
	blbc	r26, trap__dtbm_double3_mb	; branch for mb
	hw_ldq/p r26, PT__R26(p_temp)		; restore register
	hw_ret	(p5)				; (0L) return

trap__dtbm_double3_mb:
	mb
	hw_ldq/p r26, PT__R26(p_temp)		; restore register
	hw_ret	(p5)				; (0L) return
  .iff
	hw_ldq/p r26, PT__R26(p_temp)		; restore register
	hw_ret	(p5)				; (0L) return
  .endc						; pte_eco

.iff						; force_path2 = 1
;
; We need to avoid the situation where a ldx_l has acquired a lock,
; another processor has taken it, and a bad path before a stx_c has
; a load which pulls the data back in and makes it look like the
; lock has succeeded. Hold up loads by writing to MM_STAT with
; scoreboard bit 2 (and 6 -- 6 is required or we hang).
;
	hw_ldq/p r26, PT__R26(p_temp)		; restore register
	mb					; allow hw_ret to fire

	ALIGN_FETCH_BLOCK <^x47FF041F>
	mulq	p6, #1, p6			; hold up loads
	mulq	p6, #1, p6			; hols up loads
	hw_mtpr p6, <EV6__MM_STAT ! ^x44>	; hold up loads
	hw_ret	(p5)				; return
.endc						; 1.44 force_path2


;+
; trap__double3_pte_inv
;
; Entry:
; 	Double TB miss flow found an invalid level 2/3 page table entry,
;	which is equivalent to invalid level 1/2 PTE of original va.
;
; Function:
;	Prepare to take a TNV or ACV exception. Access violation takes
;	priority over translation not valid. Return to single miss with
;	fake PTE, and the invalid single miss flow will report the error.
;
; Current state:
;	p4	available
;	p5	pc of ld_vpte instruction in TB miss flow
;	p6	original va
;	p7	<31:16>=mm_stat<15:0>,<15:1>=exc_sum<15:1>,<0>=1
;	p23	pc of instruction causing TB miss
;
;	r25	pte
;	r26	scratch
;
;	PT__R25	saved r25
;	PT__R26	saved r26
;
; Register use:
;	p20	available if original exception not from pal
;
; Exit state:
;	On exit back to single miss flow
;	p4	PTE
;	p5	scratch
;	p7	<31:16>=mm_stat<15:0>,<15:1>=exc_sum<15:1>,<0>=1
;	p23	pc of instruction taking TB miss
;-
trap__double3_pte_inv:
	srl	r25, #OSF_PTE__KRE__S, r25	; get kre bit to <0>
	addq	p5, #4, p5			; inc pc for return to single
	lda	p4, OSF_PTE__PROT__M(r31)	; UWE,KWE,URE,KRE set
	cmovlbc	r25, r31, p4			; if <kre>=0 => ACV

	hw_ldq/p r25, PT__R25(p_temp)		; restore r25
	hw_ldq/p r26, PT__R26(p_temp)		; restore r26

	hw_ret	(p5)				; (0L) return
;+
; Do a 1-to-1 mapping
; Current state:
;	p5	exc_addr of ld_vpte
;	r25	saved, needs to be restored
;	r26	saved, needs to be restored
;-
trap__double3_1to1:
	hw_ldq/p r25, PT__R25(p_temp)		; restore r25
	hw_ldq/p r26, PT__R26(p_temp)		; restore r26
	addq	p5, #4, p5			; return past the ld_vpte

    .if eq force_path				; 1.41
	hw_ret	(p5)				; do the ret
    .iff
	ALIGN_FETCH_BLOCK <^x47FF041F>		; align
	PVC_VIOLATE <1007>
	PVC_VIOLATE <1020>			; stop permutation
	hw_jmp	(p5)				; return with jmp
	br	r31, .-4			; stop predictor
    .endc					; 1.41

	END_HW_VECTOR

;+
; DTBM_DOUBLE_4 - offset 180
;
; Entry:
;	Vectored into vis hardware trap on TB miss on level 3
;	page table entry. Use 4-level flow.
;
; Function:
;	Translate level 3 PTE va.
;	If valid, load TB and return to redo the ld_vpte, which will now hit.
;	If not valid, then take TNV/ACV exception.
;
; Current state:
;	p4	va of level 3 page table entry
;	p5	mm_stat (from dtbm_single)
;	p7	exc_sum (from dtbm_single)
;	p23	pc of instruction causing the TB miss
;
;	VA	original VA for dstream TB miss
;
; Register Use:
;	p5	exc_addr, pc of ld_vpte in TB miss flows
;	p6	original va
;	p7	<31:16>=mm_stat<15:0>,<15:1>=exc_sum<15:1>,<0>=1
;
;	r25	saved, used as scratch, restored on hw_ret
;	r26	saved, used as scratch, restored on hw_ret
;
; Exit state:
;	On exit to trap__double3_pte_inv
;	p5	scratch
;	p6	original va
;	p7	<31:16>=mm_stat<15:0>,<15:1>=exc_sum<15:1>,<0>=1
;	p23	pc of instruction causing TB miss
;	r25	pte
;-

	START_HW_VECTOR <DTBM_DOUBLE_4>

;
; Entered on a miss on va of level 3 page table entry.
;	level 0 = VPTB
;	level 1 = level 0 of original va
;	level 2 = level 1 of original va
;	level 3 = level 2 of original va
; byte_within_page = level 3 or original va as offset in level 3 page table
;
; Optimize by starting with the fetch of level 1 pte
;
ASSUME OSF_P_MISC__PHYS__S eq 63
	zapnot	p7, #3, p7			; clean exc_sum to 16 bits
	sll	p5, #16, p5			; shift mm_stat into place
	bis	p7, p5, p7			; combine 
	hw_mfpr	p5, EV6__EXC_ADDR		; (0L) save exc_addr

	bis	p7, #1, p7			; double miss flag

	hw_stq/p r25, PT__R25(p_temp)		; get a scratch register
	hw_stq/p r26, PT__R26(p_temp)		; get a scratch register
	hw_ldq/p r25, PT__PTBR(p_temp)		; get phys page table addr

	blt	p_misc, trap__double3_1to1	; 1-to-1 => branch

	sll	p4, #<64-<<3*level_bits>+13>>, r26
	srl	r26, #<61-level_bits>, r26	; get level 1 into offset
	addq	r25, r26, r25			; pa for level 1 pte
	hw_ldq/p r25, 0(r25)			; get level 1 pte

	sll	p4, #<64-<<2*level_bits>+13>>, r26
	srl	r26, #<61-level_bits>, r26	; get level 2 into offset
	blbc	r25, trap__double3_pte_inv	; branch => invalid pte
	srl	r25, #32, r25			; extract pfn from pte
	sll	r25, #13, r25			; get into position
	addq	r25, r26, r25			; pa for level 2 pte
	hw_ldq/p r25, 0(r25)			; get level 2 pte

	sll	p4, #<64-<<1*level_bits>+13>>, r26
	srl	r26, #<61-level_bits>, r26	; get level 3 into offset
	blbc	r25, trap__double3_pte_inv	; branch => invalid pte
	srl	r25, #32, r25			; extract pfn from pte
	sll	r25, #13, r25			; get into position
	addq	r25, r26, r25			; pa for level 3 pte
	hw_ldq/p r25, 0(r25)			; get level 3 pte

	blbc	r25, trap__double3_pte_inv	; branch => invalid pte
	srl	r25, #7, r26			; get mb bit

	CONT_HW_VECTOR <DTBM_DOUBLE_4>

	ALIGN_FETCH_BLOCK <^x47FF041F>		; Edit 1.36

	PVC_VIOLATE <2>				; ignore scoreboard violation
	hw_mtpr	p4, EV6__DTB_TAG0		; (2&6,0L) write tag0
	hw_mtpr p4, EV6__DTB_TAG1		; (1&5,1L) write tag1
	hw_mtpr	r25, <EV6__DTB_PTE0 ! ^x44>	; (0,4,2,6) (0L) write pte0
	hw_mtpr	r25, <EV6__DTB_PTE1 ! ^x22>	; (3,7,1,5) (1L) write pte1

	ASSUME_FETCH_BLOCK

	hw_ldq/p r25, PT__R25(p_temp)		; restore register
;
; We need to wait until the pte writes retire. We need to avoid the
; case where the tag writes in the single flow issue before the
; pte writes in this flow issue and retire. With changes in 1.36, we
; can probably remove this stall, but at this point in the project,
; let's just leave it in.
;
	hw_mtpr	r31, <EV6__MM_STAT ! ^x80>	; wait for pte write

ASSUME <tb_mb_en + pte_eco> ne 2

.if eq force_path2				; 1.44 force_path = 0

  .if ne pte_eco
	blbc	r26, trap__dtbm_double4_mb	; branch for mb
	hw_ldq/p r26, PT__R26(p_temp)		; restore register
	hw_ret	(p5)				; (0L) return

trap__dtbm_double4_mb:
	mb
	hw_ldq/p r26, PT__R26(p_temp)		; restore register
	hw_ret	(p5)				; (0L) return
  .iff
	hw_ldq/p r26, PT__R26(p_temp)		; restore register
	hw_ret	(p5)				; (0L) return
  .endc						; pte eco

.iff						; force_path2 = 1
;
; We need to avoid the situation where a ldx_l has acquired a lock,
; another processor has taken it, and a bad path before a stx_c has
; a load which pulls the data back in and makes it look like the
; lock has succeeded. Hold up loads by writing to MM_STAT with
; scoreboard bit 2 (and 6 -- 6 is required or we hang).
;
	hw_ldq/p r26, PT__R26(p_temp)		; restore register
	mb					; allow hw_ret to fire

	ALIGN_FETCH_BLOCK <^x47FF041F>
	mulq	p6, #1, p6			; hold up loads
	mulq	p6, #1, p6			; hols up loads
	hw_mtpr p6, <EV6__MM_STAT ! ^x44>	; hold up loads
	hw_ret	(p5)				; return
.endc						; 1.44 force_path2

	END_HW_VECTOR

;+
; FEN - offset 200
;
; Entry:
;	Vectored into via hardware trap on floating disable fault.
;
; Function:
;	Build stack frame.
;		r16 (a0)	FEN code
;		r17 (a1)	unpredictable
;		r18 (a2)	unpredictable
;	Vector via entIF.
;
; Note:
;	Unlike previous processors, this entry point is *only* for
;	floating point disabled fault, so we don't need to check fen
;	to see if we need to generate an OPCDEC instead.
;-

	START_HW_VECTOR <FEN>

ASSUME OSF_P_MISC__PS__S eq 0

.if ne ev6_p1
	hw_mfpr	p23, EV6__EXC_ADDR			; (0L) save exc_addr
	NOP						; no conditional br
	NOP						;	in 1st block
	NOP

	hw_ldq/p p4, PT__IMPURE(p_temp)			; get base of impure area
	hw_ldq/p p5, CNS__FPE_STATE(p4)			; get real fpe state
	blbc	p5, trap__emul_fen			; if clear, take fen trap

trap__emul_merge:				; merge from opcdec

	hw_stq/p p23, CNS__FP_PC(p4)		; save user pc

    .if ne fp_count
	hw_ldq/p p6, PT__RSV_FOR_PAL(p_temp)	; get fp counter
	addq	p6, #1, p6			; add 1
	hw_stq/p p6, PT__RSV_FOR_PAL(p_temp)	; write it back
	br	r31, trap__fen_cont		; continue
    .endc

	CONT_HW_VECTOR <FEN>
;
; Fen with FPE_STATE set
;	or
; Opcdec with unknown FPE_STATE
;
;	Save GPRs.
;	Get the instruction and parse for load/store.
;	Execute locally for ld/st.
;	Go to emulator for operates and branches.
;	Restore GPRs.
;	HW_RET to correct PC. 
;
ASSUME EV6__I_CTL__SDE__S eq 6

	ALIGN_FETCH_BLOCK <^x47FF041F>		; align with nops

	hw_stq/p r0, CNS__R0_EMUL(p4)		; save r0
	hw_stq/p r1, CNS__R1_EMUL(p4)		; save r1
	hw_stq/p r2, CNS__R2_EMUL(p4)		; save r2
	hw_mfpr	r2, EV6__I_CTL			; (4,0L) get i_ctl

	bis	p4, r31, r1			; impure base into r1
	bic	r2, #<2@EV6__I_CTL__SDE__S>, r2	; zap sde
	hw_stq/p r3, CNS__R3_EMUL(p4)		; save r3
	hw_stq/p r8, CNS__R8_EMUL(r1)		; save gpr

	hw_mtpr	r2, EV6__I_CTL			; (4,0L) write i_ctl
	hw_stq/p r9, CNS__R9_EMUL(r1)		; save gpr
	hw_stq/p r10, CNS__R10_EMUL(r1)		; save gpr
	hw_stq/p r11, CNS__R11_EMUL(r1)		; save gpr

	hw_mtpr	r2, EV6__I_CTL			; (4,0L) stall outside IQ
	hw_stq/p r12, CNS__R12_EMUL(r1)		; save gpr
	hw_stq/p r13, CNS__R13_EMUL(r1)		; save gpr
	hw_stq/p r14, CNS__R14_EMUL(r1)		; save gpr

	hw_stq/p r15, CNS__R15_EMUL(r1)		; buffer block 1 -- save gpr
	hw_stq/p r16, CNS__R16_EMUL(r1)		; save gpr
	hw_stq/p r17, CNS__R17_EMUL(r1)		; save gpr
	hw_stq/p r18, CNS__R18_EMUL(r1)		; save gpr

	hw_stq/p r19, CNS__R19_EMUL(r1)		; buffer block 2 -- save gpr
	hw_stq/p r24, CNS__R24_EMUL(r1)		; save gpr
	hw_stq/p r25, CNS__R25_EMUL(r1)		; save gpr
	hw_stq/p r26, CNS__R26_EMUL(r1)		; save gpr

	hw_stq/p r27, CNS__R27_EMUL(r1)		; buffer block 3 -- save gpr
	hw_stq/p r28, CNS__R28_EMUL(r1)		; save gpr
	hw_stq/p r29, CNS__R29_EMUL(r1)		; save gpr
	hw_stq/p r30, CNS__R30_EMUL(r1)		; save gpr

	hw_stq/p r4, CNS__R4_EMUL(r1)		; save gpr
	hw_stq/p r5, CNS__R5_EMUL(r1)		; save gpr
	hw_stq/p r6, CNS__R6_EMUL(r1)		; save gpr
	hw_stq/p r7, CNS__R7_EMUL(r1)		; save gpr

	hw_stq/p r20, CNS__R20_EMUL(r1)		; save gpr
	hw_stq/p r21, CNS__R21_EMUL(r1)		; save gpr
	hw_stq/p r22, CNS__R22_EMUL(r1)		; save gpr
	hw_stq/p r23, CNS__R23_EMUL(r1)		; save gpr

	hw_stq/p r31, CNS__R31_EMUL(r1)		; save gpr
;
; Now turn shadow mode back on
;
	bis	r2, #<2@EV6__I_CTL__SDE__S>, r2	; enable sde
	hw_mtpr	r2, EV6__I_CTL			; (4,0L) write i_ctl
	bis	r31, r31, r31			; finish fetch block

	ALIGN_FETCH_BLOCK <^x47FF041F>		; align with nops

	hw_mtpr	r2, EV6__I_CTL			; (4,0L) stall outside IQ
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 1
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 2
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 3
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31
;
; Now start parsing the instruction.
;	r1	pointer to impure area
;	p23	pc of instruction
; After getting the instruction, we can no longer depend on p23, as
; we could have taken a miss. So copy to r2 for the fetch.
;
	bic	p23, #3, r2			; clean instruction to r2
trap__emul_get_instr:
	ldl	p5, 0(r2)			; fatal if faults, okay to tnv
	zap	p5, #^xF0, p5			; clean to 32 bits
	srl	p5, #26, p6			; get opcode
	and	p6, #^x30, p7			; just get <5:4> of opcode
	cmpeq	p7, #^x20, p7			; check against ^x20
	beq	p7, trap__emul_emul		; branch => pass to emulator
;
; Load/Store
;	r1	pointer to impure area
;	p5	instruction
;	p6	opcode
;
	sll	p5, #43, r2			; Rb.ab
	srl	r2, #59, r2			; Rb.ab
	s8addq	r2, r1, r2			; impure area + offset for RA
	hw_ldq/p r3, CNS__R0_EMUL(r2)		; get contents of register

	sll	p5, #48, r8			; get bit 15 to 63
	sra	r8, #48, r8			; now have sign ext displacement
	addq	r8, r3, r8			; (reg)+disp = mem addr

	sll	p5, #38, r9			; Fa
	srl	r9, #59, r9			; Fa
	bis	r9, r31, r10			; Fa for F31 check
	s8addq	r9, r1, r9			; impure area + offset for FA
;
; We now have the operand info.
;	r1	pointer to impure area
;	r8	memory address
;	r9	impure area + offset to FA
;	r10	FA for F31 check
;
;	p5	instr
;	p6	opcode
;
; We need to save the opcode into r11 in case of alignment trap.
;
	bis	p6, r31, r11			; save for alignment trap
	and	p6, #^x04, r2			; check for load or store
	bne	r2, trap__emul_parse		; branch for stores
	cmpeq	r10, #^x1F, r3			; check for load to f31
	bne	r3, trap__emul_ldst_done	; done if load to f31

trap__emul_parse:
	cmpeq	p6, #^x20, r2
	bne	r2, trap__emul_ldf		; branch for ldf
	cmpeq	p6, #^x21, r3
	bne	r3, trap__emul_ldg		; branch for ldg
	cmpeq	p6, #^x22, r2
	bne	r2, trap__emul_lds		; branch for lds
	cmpeq	p6, #^x23, r3
	bne	r3, trap__emul_ldt		; branch for ldt

	cmpeq	p6, #^x24, r2
	bne	r2, trap__emul_stf		; branch for stf
	cmpeq	p6, #^x25, r3
	bne	r3, trap__emul_stg		; branch for stg
	cmpeq	p6, #^x26, r2
	bne	r2, trap__emul_sts		; branch for sts
	cmpeq	p6, #^x27, r3
	beq	r3, trap__pal_exc_bugcheck	; bugcheck if not stt
	br	r31, trap__emul_stt		; fall through for stt
;
; Do load or store.
;	r8	memory address
;	r9	impure area + fpr offset
;

;
; LDF
;
trap__emul_ldf:
	ldl	r0, 0(r8)		; load from memory (may fault)
	extwl   r0, #2, r1		; <31:16> -> r1
	inswl	r0, #2, r0		; <15:0> << 16 -> r0
        addq	r0, r1, r0
	sll	r0, #41, r1		; shift out all but fraction bits
	srl	r1, #12, r1		; shift fraction bits inplace
        addq	r31, #^x070, r16
        srl	r0, #30, r19		; shift out all but sign and bias
        sll	r19, #62, r19		; move into correct place
        srl	r0, #23, r0
        addq	r1, r19, r1		; insert sign and bias
        and	r0, #255, r0		; clear all but exp
	cmoveq	r0, #0, r16		; clear map
	cmpult	r0, #^x80, r19		; test for small exp
	cmpeq	r0, #^xff, r17		; check for Nan and Inf
	bic	r19, r17, r19		; r19 and ~r17
	cmoveq	r19, #0, r16		; clear map
        and	r0, #^x7f, r0		; clear all but exp
        s8addq	r16, r0, r16
	sll	r16, #52, r16
        addq	r16, r1, r0		; merge it
	hw_stq/p r0, CNS__F0_EMUL(r9)	; write to our register file
	br	r31, trap__emul_ldst_done
;
; LDG
;
trap__emul_ldg:
	ldq	r0, 0(r8)		; load from memory (may fault)
	sll	r0, #48, r1		; <15:0> -> <63:48>
	srl	r0, #48, r16		; <63:48> -> <15:0>
	extwl	r0, #2,	r17		; <31:16> -> r17
	addq	r1, r16, r1		; <63:48> + <15:0>
	inswl	r17, #4, r16		; <31:16> -> <47:32>
	extwl	r0, #4,	r17		; <47:32> -> r17
	addq	r1, r16, r1		; <63:48> + <47:32> + <15:0> 
	inswl	r17, #2, r0		; <47:32> -> <31:16>
	addq	r1, r0, r0		; <63:48> + <47:32> + <31:16> + <15:0>
	hw_stq/p r0, CNS__F0_EMUL(r9)	; write to our register file
	br	r31, trap__emul_ldst_done
;
; LDS
;
trap__emul_lds:
	ldl	r0, 0(r8)		; load from user memory
	sll	r0, #41, r1		; shift out all but fraction bits
	srl	r1, #12, r1		; shift fraction bits inplace
	addq	r31, #^x70, r16	
	srl	r0, #30, r19		; shift out all but sign and bias
	sll	r19, #62, r19		; move into correct place
	srl	r0, #23, r0
	addq	r1, r19, r1		; insert sign and bias
	and	r0, #255, r0		; clear all but exp
	cmoveq	r0, #0, r16		; clear map
	cmpult	r0, #^x80, r19		; test for small exp
	cmpeq	r0, #^xff, r17		; check for Nan and Inf
	bis	r17, r19, r19		; or result
	cmoveq	r19, #0, r16		; clear map
	and	r0, #^x7f, r0		; clear all but exp
	s8addq	r16, r0, r16
	sll	r16, #52, r16
	addq	r16, r1, r0		; merge it
	hw_stq/p r0, CNS__F0_EMUL(r9)	; write to our register file
	br	r31, trap__emul_ldst_done
;
; LDT
;
trap__emul_ldt:
	ldq	r0, 0(r8)		; load from memory (may fault)
	hw_stq/p r0, CNS__F0_EMUL(r9)	; write to our register file
	br	r31, trap__emul_ldst_done

;
; STF
;
trap__emul_stf:
	hw_ldq/p r0, CNS__F0_EMUL(r9)	; read from our register file
	sll	r0, #5, r1		; shift out all but fraction bits
	srl	r1, #34, r1		; shift fraction bits inplace
	srl	r0, #62, r0		; shift out all but fraction bits
	sll	r0, #30, r0
	addq	r0, r1, r0
	extwl	r0, #2, r1		; <31:16> -> r1
	inswl	r0, #2, r0		; <15:0> << 16 -> r0
	addq	r0, r1, r0
trap__emul_stf_s:
	stl	r0, 0(r8)		; store it in to memory (may fault)
	br	r31, trap__emul_ldst_done
;
; STG
;
trap__emul_stg:
	hw_ldq/p r0, CNS__F0_EMUL(r9)	; read from our register file
	sll	r0, #48, r1		; <15:0> -> <63:48>
	srl	r0, #48, r16		; <63:48> -> <15:0>
	extwl	r0, #2,	r17		; <31:16> -> r17
	addq	r1, r16, r1		; <63:48> + <15:0>
	inswl	r17, #4, r16		; <31:16> -> <47:32>
	extwl	r0, #4,	r17		; <47:32> -> r17
	addq	r1, r16, r1		; <63:48> + <47:32> + <15:0> 
	inswl	r17, #2, r0		; <47:32> -> <31:16>
	addq	r1, r0, r0		; <63:48> + <47:32> + <31:16> + <15:0>
trap__emul_stg_s:
	stq	r0, 0(r8)		; store it in memory (may fault)
	br	r31, trap__emul_ldst_done
;
; STS
;
trap__emul_sts:
	hw_ldq/p r0, CNS__F0_EMUL(r9)	; read from our register file
	sll	r0, #5, r1		; shift out all but fraction bits
	srl	r1, #34, r1		; shift fraction bits in place
	srl	r0, #62, r0		; shift out all but fraction bits
	sll	r0, #30, r0
	addq	r0, r1, r0
trap__emul_sts_s:
	stl	r0, 0(r8)		; store in user memory (may fault)
	br	r31, trap__emul_ldst_done
;
; STT
;
trap__emul_stt:
	hw_ldq/p r0, CNS__F0_EMUL(r9)	; read from our register file
trap__emul_stt_s:
	stq	r0, 0(r8)		; store in user memory (may fault)
	br	r31, trap__emul_ldst_done
;
; Load/Store Done.
;
trap__emul_ldst_done:
	hw_ldq/p p4, PT__IMPURE(p_temp)		; get base of impure area
	hw_ldq/p p6, CNS__FP_PC(p4)		; get pc back
	addq	p6, #4, p6			; point to next instruction
	hw_stq/p p6, CNS__FP_PC(p4)		; write back to save location
	br	r31, trap__emul_done


;+
; Done emulation. Restore and go back to user.
; Current state:
;	p4	base of impure area
;	p20	unaligned flag
;
;	Get out of shadow mode.
;	Restore overshadowed registers.
;	Get back into shadow mode.
;	Restore the rest of the registers.
;	Return to user.
;-
trap__emul_done:
	bis	p4, r31, r1			; impure base into r1
	PVC_JSR	emul_restore, bsr=1
	bsr	r3, trap__emul_restore		; restore GPRs except r0-r3
;
; Now restore the rest of the GPRs.;
;
	hw_ldq/p r0, CNS__R0_EMUL(p4)		; restore r0
	hw_ldq/p r1, CNS__R1_EMUL(p4)		; restore r1
	hw_ldq/p r2, CNS__R2_EMUL(p4)		; restore r2
	hw_ldq/p r3, CNS__R3_EMUL(p4)		; restore r3
;
; Return to user.
;
	hw_ldq/p p23, CNS__FP_PC(p4)		; get nextpc
	hw_ret (p23)				; return to user

;+
; Subroutine to restore GPRs.
;	p4	pointer to impure area
;	r1	pointer to impure area
;	r3	return address
;
; Do not touch shadow registers.
;-
	ALIGN_FETCH_BLOCK

trap__emul_restore:
	hw_mfpr	r2, EV6__I_CTL			; (4,0L) get i_ctl
	bic	r2, #<2@EV6__I_CTL__SDE__S>, r2	; zap sde
	hw_ldq/p r8, CNS__R8_EMUL(r1)		; restore gpr
	hw_ldq/p r9, CNS__R9_EMUL(r1)		; restore gpr

	hw_mtpr	r2, EV6__I_CTL			; (4,0L) write i_ctl
	hw_ldq/p r10, CNS__R10_EMUL(r1)		; restore gpr
	hw_ldq/p r11, CNS__R11_EMUL(r1)		; restore gpr
	hw_ldq/p r12, CNS__R12_EMUL(r1)		; restore gpr

	hw_mtpr	r2, EV6__I_CTL			; stall outside IQ
	hw_ldq/p r13, CNS__R13_EMUL(r1)		; restore gpr
	hw_ldq/p r14, CNS__R14_EMUL(r1)		; restore gpr
	hw_ldq/p r15, CNS__R15_EMUL(r1)		; restore gpr

	hw_ldq/p r16, CNS__R16_EMUL(r1)		; buffer block 1 -- restore gpr
	hw_ldq/p r17, CNS__R17_EMUL(r1)		; restore gpr
	hw_ldq/p r18, CNS__R18_EMUL(r1)		; restore gpr
	hw_ldq/p r19, CNS__R19_EMUL(r1)		; restore gpr

	hw_ldq/p r24, CNS__R24_EMUL(r1)		; buffer block 2 -- restore gpr
	hw_ldq/p r25, CNS__R25_EMUL(r1)		; restore gpr
	hw_ldq/p r26, CNS__R26_EMUL(r1)		; restore gpr
	hw_ldq/p r27, CNS__R27_EMUL(r1)		; restore gpr

	hw_ldq/p r28, CNS__R28_EMUL(r1)		; buffer block 3 -- restore gpr
	hw_ldq/p r29, CNS__R29_EMUL(r1)		; restore gpr
	hw_ldq/p r30, CNS__R30_EMUL(r1)		; restore gpr
	bis	r31, r31, r31

	hw_ldq/p r4, CNS__R4_EMUL(r1)		; restore gpr
	hw_ldq/p r5, CNS__R5_EMUL(r1)		; restore gpr
	hw_ldq/p r6, CNS__R6_EMUL(r1)		; restore gpr
	hw_ldq/p r7, CNS__R7_EMUL(r1)		; restore gpr

	hw_ldq/p r20, CNS__R20_EMUL(r1)		; restore gpr
	hw_ldq/p r21, CNS__R21_EMUL(r1)		; restore gpr
	hw_ldq/p r22, CNS__R22_EMUL(r1)		; restore gpr
	hw_ldq/p r23, CNS__R23_EMUL(r1)		; restore gpr
;
; Now turn shadow mode back on.
;
	bis	r2, #<2@EV6__I_CTL__SDE__S>, r2	; enable sde
	hw_mtpr	r2, EV6__I_CTL			; (4,0L) write i_ctl
	bis	r31, r31, r31
	bis	r31, r31, r31

	hw_mtpr	r2, EV6__I_CTL			; (4,0L) stall outside IQ
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 1
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 2
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 3
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r3, #1, r3			; return in pal mode
	PVC_JSR emul_restore, bsr=1, dest=1
	hw_ret_stall (r3)			; stall for pvc

;+
; Load/Store operand dfault.
;
; We need to restore GPRs. Then we need to set up for an exception.
;
; Current register state:
;	p5		mm_stat
;	p6		pc of palcode instruction
;	p7		available
;
;	r25		will be restored as part of gpr restore
;	r26		will be restored as part of gpr restore
;
;	CNS__FP_PC	user pc
;
;	VA		faulting VA
;-
trap__emul_ldst_dfault:
	hw_ldq/p p4, PT__IMPURE(p_temp)		; impure base into p4
	bis	p4, r31, r1			; also put it into r1
	PVC_JSR	emul_restore, bsr=1
	bsr	r3, trap__emul_restore		; restore all but r0-r3
;
; Now restore the rest of the GPRs.
;
	hw_ldq/p r0, CNS__R0_EMUL(p4)		; restore r0
	hw_ldq/p r1, CNS__R1_EMUL(p4)		; restore r1
	hw_ldq/p r2, CNS__R2_EMUL(p4)		; restore r2
	hw_ldq/p r3, CNS__R3_EMUL(p4)		; restore r3
;
; Now restore restore pc to p6, and merge with dfault code
;
	hw_ldq/p p6, CNS__FP_PC(p4)		; get back user pc
	br	r31, trap__dfault_no_dismiss	; merge to post

;+
; Load/Store operand tnv.
;
; We need to restore GPRs. Then we need to set up for an exception.
;
; Current register state:
;	p4		PTE
;	p5		mm_stat
;	p6		va
;	p7		exc_sum
;
;	r25		will be restored as part of gpr restore
;	r26		will be restored as part of gpr restore
;
;	CNS__FP_PC	user pc
;-
trap__emul_ldst_tnv:
	hw_ldq/p p23, PT__IMPURE(p_temp)	; impure base into p23
	bis	p23, r31, r1			; also put it into r1
	PVC_JSR	emul_restore, bsr=1
	bsr	r3, trap__emul_restore		; restore all but r0-r3
;
; Now restore the rest of the GPRs.
;
	hw_ldq/p r0, CNS__R0_EMUL(p23)		; restore r0
	hw_ldq/p r1, CNS__R1_EMUL(p23)		; restore r1
	hw_ldq/p r2, CNS__R2_EMUL(p23)		; restore r2
	hw_ldq/p r3, CNS__R3_EMUL(p23)		; restore r3
;
; Now restore restore pc to p6, and merge with dfault code
;
	hw_ldq/p p23, CNS__FP_PC(p23)			; get back user pc
	br	r31, trap__invalid_dpte_no_dismiss	; merge to post

;+
; Load/Store operand unalign
;
; We need to restore GPRs. Then we need to set up for an exception.
;
; Current register state:
;	p5		va
;	p6		mm_stat
;	p7		exc_sum
;	p23		exc_addr in PALcode
;
;	r10		fa -- move to p7
;	r11		opcode -- move to proper spot in p6
;-
trap__emul_ldst_unalign:
	bis	r10, r31, p7				; register number
	sll	r11, #EV6__MM_STAT__OPCODE__S, p6	; opcode

	hw_ldq/p p4, PT__IMPURE(p_temp)		; impure base into p4
	bis	p4, r31, r1			; also put it into r1
	PVC_JSR	emul_restore, bsr=1
	bsr	r3, trap__emul_restore		; restore all but r0-r3
;
; Now restore the rest of the GPRs.
;
	hw_ldq/p r0, CNS__R0_EMUL(p4)		; restore r0
	hw_ldq/p r1, CNS__R1_EMUL(p4)		; restore r1
	hw_ldq/p r2, CNS__R2_EMUL(p4)		; restore r2
	hw_ldq/p r3, CNS__R3_EMUL(p4)		; restore r3
;
; Now take an unalign exception.
; Current state:
;	p4		base of impure area
;	p5		va
;	p6		"mm_stat" with opcode from r11
;	p7		register number from from r10
;
	hw_ldq/p p23, CNS__FP_PC(p4)		; get pc back
	addq	p23, #4, p23			; bump to nextpc

	br	r31, trap__unalign_nodismiss	; merge with unalign code

;+
; Operates and branches are passed to emulator.
; Current state:
;	GPRs have been saved.
;	CNS__FP_PC	pc
;	p5		instruction
;
; Map a 4MB chunck of virtual to physical.
; NOTE: The impure area has to be in the same naturally aligned 4mb chunk!!!
;-
.if ndf PAL__EMUL_BASE
	PAL__EMUL_BASE = ^xE0000
.endc

	ALIGN_FETCH_BLOCK
trap__emul_emul:
	subq	r31, #1, r8			; get a -1
	srl	r8, #42, r8			; shift off low bits of kseg addr
	sll	r8, #42, r8			; shift back into position

	br	r1, trap__emul_entry
	.long	PAL__EMUL_BASE			; transfer address
trap__emul_entry:
	hw_ldl/p r27, 0(r1)			; get transfer address
	bis	r27, r8, r27			; transfer address as kseg

	ALIGN_FETCH_BLOCK <^x47FF041F>
	hw_mfpr	r16, EV6__IER_CM		; (4,0L) get old IER_CM
	hw_ldq/p p4, PT__IMPURE(p_temp)		; get impure base
	hw_stq/p r16, CNS__IER_CM(p4)		; save old IER_CM
	bis	p4, r8, p4			; impure area as kseg

	ASSUME_FETCH_BLOCK
	hw_mtpr r31, EV6__IER_CM		; (4,0L) ints off, cm=kernel
	bis	p5, r31, r16			; instruction
	bis	p4, r31, r17			; pointer to impure area
	bis	r31, r31, r31

	ASSUME_FETCH_BLOCK
	lda	p6,^x400(r31)			; get a ^x400
	hw_ldq/p p4, PT__WHAMI(p_temp)		; get whami
	mulq	p4, p6, p6			; whami * ^x400
	subq	r27, p6, r30			; set top of stack to be kseg
						;   xfer addr - (whami * ^x400)

kseg_offset = <trap__emul_kseg_done - trap__emul_kseg>

	ASSUME_FETCH_BLOCK
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31
	br	r1, trap__emul_kseg

	ASSUME_FETCH_BLOCK
trap__emul_kseg: 
	addq	r1, #<kseg_offset>, r1
	bis	r1, r8, r1			; ret as native mode kernel kseg
	bsr	r31, .				; push prediction stack
	PVC_JSR emul_kseg
	hw_ret_stall (r1)			; pop prediction stack
	PVC_JSR emul_kseg, dest=1

trap__emul_kseg_done:

;
; Now we are in native kernel kseg
;
	ASSUME_FETCH_BLOCK
	PVC_VIOLATE <29>
	PVC_VIOLATE <1007>
	jsr	r26, (r27)			; jsr to emulator
;
; Return from emulation.
; We are still in native kernel kseg mode.
;	r0		return status
;	CNS__FP_PC	updated pc
;	CNS__FPCR	updated
;	CNS__EXC_SUM	exception information
;
trap__emul_return:				; return in non-pal mode
	lda	r16, ^xBAC(r31)			; flag to call_pal
	.long	^x3B				; pop back into pal mode
;
; Now restore IER_CM
;
	hw_ldq/p p4, PT__IMPURE(p_temp)		; get base of impure area
	hw_ldq/p r16, CNS__IER_CM(p4)		; get old IER_CM back
	bis	r31, r31, p20			; zap unalign flag
	hw_mtpr	r16, EV6__IER_CM		; (4,0L) restore IER_CM
;
; Check r0 for status.
;	r0	 0	success
;		-1	arith trap
;		-2	opcdec
;		-4	fen (FEN from OPCDEC with FPE_STATE clear)
;
	bne	r0, trap__emul_arith		; branch for arith or opcdec
	br	r31, trap__emul_done		; restore registers and return

;+
; Arithmetic exception or opcdec or fen during emulation.
;	Restore state. Merge with appropriate flow.
;	Emulator left FP_PC as current pc.
;-
trap__emul_arith:
	bis	r0, r31, p5			; save status
	bis	p4, r31, r1			; impure base into r1
	PVC_JSR	emul_restore, bsr=1
	bsr	r3, trap__emul_restore		; restore all but r0-r3
;
; Now restore the rest of the GPRs.
;
	hw_ldq/p r0, CNS__R0_EMUL(p4)		; restore r0
	hw_ldq/p r1, CNS__R1_EMUL(p4)		; restore r1
	hw_ldq/p r2, CNS__R2_EMUL(p4)		; restore r2
	hw_ldq/p r3, CNS__R3_EMUL(p4)		; restore r3
;
; Current state:
;	p4	base of impure area
;	p5	status
;		-1	arith trap
;		-2	opcdec
;		-4	fen (FPE_STATE clear)
;
	hw_ldq/p p23, CNS__FP_PC(p4)		; get PC back
	blbc	p5, trap__emul_opcdec_or_fen	; branch for opcdec or fen

	hw_ldq/p p7, CNS__EXC_SUM(p4)		; get emulated exc_sum
	br	r31, trap__arith_merge		; merge with arithmetic
;
; 
; Opcdec
;	or
; Fen (FEN from OPCDEC with FPE_STATE clear)
;	p5	-2	opcdec
;		-4	fen (FEN from OPCDEC with FPE_STATE clear)
;
;	p23		user pc
;
trap__emul_opcdec_or_fen:
	srl	p5, #1, p5			; shift to determine which
	blbc	p5, trap__emul_fen		; branch for fen
	br	r31, trap__opcdec_merge		; take opcdec

.iff
	hw_mfpr	p23, EV6__EXC_ADDR			; (0L) save exc_addr
	NOP						; no conditional br
	NOP						;	in 1st block
	NOP
.endc

trap__emul_fen:						; take real fen

	blbs	p23, trap__pal_exc_bugcheck		; mchk if fen within pal

	and	p_misc, #<1@OSF_P_MISC__CM__S>, p4	; current mode
	and	p_misc, #OSF_P_MISC__PS__M, p20		; save original ps
	beq	p4, trap__fen_stack			; skip switch if kernel
;
; Switch to kernel mode. 
;
fen_cm_offset = <trap__fen_stack - trap__fen_cm>

	hw_stq/p r30, PT__USP(p_temp)			; save user SP
	hw_ldq/p r30, PT__KSP(p_temp)			; get kernel SP
	bic	p_misc, #OSF_P_MISC__PS__M, p_misc	; cm=0, ipl=0

	br	p6, trap__fen_cm			; change mode to kernel
trap__fen_cm:
	addq	p6, #<fen_cm_offset+1>, p6		; jump past in palmode
	hw_mtpr	r31, EV6__PS				; (4,0L) switch to kern
	bsr	r31, .					; push prediction stack
	PVC_JSR	fen_cm					; synch up
	hw_ret_stall (p6)				; pop prediction stack
	PVC_JSR	fen_cm, dest=1
;
; Take the trap. Be careful of tb miss!
; Current state:
;	p_misc__ps	if we were in kernel, original ps
;			if we were in user, new ps = 0 (cm=0, ipl=0)
;	p20		original ps
;	p23		pc
;
trap__fen_stack:
	hw_stq/p p23, PT__STACK_PC(p_temp)		; store away pc

	lda	r30, <0-OSF_FRM__SIZE>(r30)		; allocate stack space

	stq	p20, OSF_FRM__PS(r30)			; save original ps
	stq	r18, OSF_FRM__A2(r30)			; save a2

	hw_ldq/p p20, PT__STACK_PC(p_temp)		; 1.39 get pc back

	stq	r29, OSF_FRM__GP(r30)			; save gp
	stq	r16, OSF_FRM__A0(r30)			; save a0
	stq	r17, OSF_FRM__A1(r30)			; save a1
	stq	p20, OSF_FRM__PC(r30)			; 1.39 save pc

	hw_ldq/p p23, PT__ENT_IF(p_temp)		; get entry point
	hw_ldq/p r29, PT__KGP(p_temp)			; get kgp
	bis	r31, #OSF_A0__FEN, r16			; set a0 

	hw_ret	(p23)					; to os

	END_HW_VECTOR


;+
; UNALIGN - offset 280
;
; Entry:
;	Vectored in via hardware trap on unaligned Dstream reference.
;
; Function:
;	Take a trap unless to x31.
;	Build stack frame.
;		r16 (a0)	VA
;		r17 (a1)	Opcode
;		r18 (a2)	Src/Dst register
;	Vector via entUna.
;-

	START_HW_VECTOR <UNALIGN>

ASSUME EV6__MM_STAT__WR__S eq 0

	hw_mfpr	p23, EV6__EXC_ADDR		; (0L) get exc_addr
	hw_mfpr p7, EV6__EXC_SUM		; (0L) get register field
	hw_mfpr p6, EV6__MM_STAT		; (0L) opcode, r/w bit
	hw_mfpr	p5, EV6__VA			; (4-7,1L) get va

.if eq ev6_p1
	blbs	p23, trap__pal_exc_bugcheck	; mchk if within pal
.iff
	blbs	p23, trap__unalign_check_fp	; check for fp ld/st problem
.endc

	addq	p23, #4, p23			; bump to nextpc

	srl	p7, #EV6__EXC_SUM__REG__S, p7	; shift register number
	and	p7, #EV6__EXC_SUM__REG__M, p7	; clean register number
	blbs	p6, trap__unalign_nodismiss	; store => take a trap
;
; Check load to x31, which we dismiss.
;
	cmpeq	p7, #^x1F, p4			; check for x31
	beq	p4, trap__unalign_nodismiss	; not x31 => nodismiss
;
; Dismiss the load x31
;
	mb					; protect mm_stat and va
	hw_ret	(p23)				; return

	CONT_HW_VECTOR
;
; Take a trap.
;
trap__unalign_nodismiss:
	mb						; protect mm_stat and va

	and	p_misc, #<1@OSF_P_MISC__CM__S>, p4	; current mode
	and	p_misc, #OSF_P_MISC__PS__M, p20		; save original ps
	beq	p4, trap__unalign_stack			; skip switch if kernel
;
; Switch to kernel mode. 
;
unalign_cm_offset = <trap__unalign_stack - trap__unalign_cm>

	hw_stq/p r30, PT__USP(p_temp)			; save user SP
	hw_ldq/p r30, PT__KSP(p_temp)			; get kernel SP
	bic	p_misc, #OSF_P_MISC__PS__M, p_misc	; cm=0, ipl=0

	br	p4, trap__unalign_cm			; change mode to kernel
trap__unalign_cm:
	addq	p4, #<unalign_cm_offset+1>, p4		; jump past in palmode
	hw_mtpr	r31, EV6__PS				; (4,0L) switch to kern
	bsr	r31, .					; push prediction stack
	PVC_JSR	unalign_cm				; synch up
	hw_ret_stall (p4)				; pop prediction stack
	PVC_JSR	unalign_cm, dest=1
;
; Take the trap. Be careful of tb miss!
; Current state:
;	p5		va
;	p6		mm_stat
;	p7		register number
;	p20		original ps
;	p23		nextpc
;	p_misc__ps	if we were in kernel, original ps
;			if we were in user, new ps = 0 (cm=0, ipl=0)
;
trap__unalign_stack:
	hw_stq/p p23, PT__STACK_PC(p_temp)		; store away nextpc
	srl	p6, #EV6__MM_STAT__OPCODE__S, p6	; get opcode
	and	p6, #EV6__MM_STAT__OPCODE__M, p6	; clean it
	hw_stq/p p5, PT__NEW_A0(p_temp)			; save away va
	hw_stq/p p6, PT__NEW_A1(p_temp)			; save away opcode
	hw_stq/p p7, PT__NEW_A2(p_temp)			; save away reg number

	lda	r30, <0-OSF_FRM__SIZE>(r30)		; allocate stack space

	stq	p20, OSF_FRM__PS(r30)			; save original ps
	stq	r18, OSF_FRM__A2(r30)			; save a2

	hw_ldq/p p20, PT__STACK_PC(p_temp)		; 1.39 get nextpc back

	stq	r29, OSF_FRM__GP(r30)			; save gp
	stq	r16, OSF_FRM__A0(r30)			; save a0
	stq	r17, OSF_FRM__A1(r30)			; save a1
	stq	p20, OSF_FRM__PC(r30)			; 1.39 save nextpc

	hw_ldq/p p23, PT__ENT_UNA(p_temp)		; get entry point
	hw_ldq/p r29, PT__KGP(p_temp)			; get kgp
	hw_ldq/p r16, PT__NEW_A0(p_temp)		; a0 <- va
	hw_ldq/p r17, PT__NEW_A1(p_temp)		; a1 <- opcode
	hw_ldq/p r18, PT__NEW_A2(p_temp)		; a2 <- register number

	hw_ret	(p23)					; to os

;
; We turn ld/st unaligns into alignment traps.
; Current state:
;	p5	va
;	p6	mm_stat (opcode, r/w bit)
;	p7	exc_sum (register field)
;	p23	exc_addr
;
.if ne ev6_p1
trap__unalign_check_fp:
	bic	p23, #3, p4		; clean PAL pc
	hw_mfpr	p20, EV6__PAL_BASE	; (4,0L) get pal base
	subq	p20, p4, p4		; pal base - cleaned pc

	lda	p20, <trap__emul_ldf - trap__pal_base>(p4)
	beq	p20, trap__emul_ldst_unalign
	
	lda	p20, <trap__emul_ldg - trap__pal_base>(p4)
	beq	p20, trap__emul_ldst_unalign
	
	lda	p20, <trap__emul_lds - trap__pal_base>(p4)
	beq	p20, trap__emul_ldst_unalign
	
	lda	p20, <trap__emul_ldt - trap__pal_base>(p4)
	beq	p20, trap__emul_ldst_unalign
	
	lda	p20, <trap__emul_stf_s - trap__pal_base>(p4)
	beq	p20, trap__emul_ldst_unalign
	
	lda	p20, <trap__emul_stg_s - trap__pal_base>(p4)
	beq	p20, trap__emul_ldst_unalign
	
	lda	p20, <trap__emul_sts_s - trap__pal_base>(p4)
	beq	p20, trap__emul_ldst_unalign
	
	lda	p20, <trap__emul_stt_s - trap__pal_base>(p4)
	beq	p20, trap__emul_ldst_unalign

	br	r31, trap__pal_exc_bugcheck	; mchk otherwise

.endc

	END_HW_VECTOR

;+
; DTBM_SINGLE - offset 300
;
; Entry:
;	Vectored into via hardware trap on Dstream single
;	translation buffer miss.
;
; Function:
;	Do a virtual fetch of the PTE, and fill the DTB if the PTE is valid.
;	Can trap into double.
;
; Note:
;	We want to keep the main flow in a single cache line
;	(4 fetch blocks) to be as efficient as possible.
;
;	The update of the TB occurs on the when the write to PTE1 retires.
;
; NOTE:
;	1-to-1 mapping scheme implemented with bit in p_misc shadow register.
;
; Register use:
;	p4	shadow register reserved for itb/dtb miss
;	p5	shadow register reserved for itb/dtb miss
;	p6	shadow register reserved for itb/dtb miss
;	p7	shadow register reserved for itb/dtb miss
;	p_misc	physical mode bit in <63>
;	p23	exc_addr, pc of instruction causing DTB miss
;
; Exit state:
;	On exit to trap__invalid_dpte
;	p4	PTE
;	p5	no double miss: mm_stat 
;		double miss: scratch
;	p6	va
;	p7	no double miss: exc_sum 
;		double miss: <31:16>=mm_stat<15:0>,<15:1>=exc_sum<15:1>,<0>=1
;	p23	pc of instruction causing DTB miss
;
;
; Note on scoreboarding and register dependencies: Having (4-7) on the
; VA/VA_FORM register reads ensures that they don't issue until all of the
; previous dtb miss's tag/pte writes have retired. The register dependency
; between the VA/VA_FORM read and the tag/pte writes ensures that none
; of the tag/pte issue until all of the previous tag/pte writes from
; a previous miss have retired.
;
; How to squeeze 5 pegs into 4 holes:
; 	Before fetching the vpte, we clear the low bit of p7.
;	If we take a double miss, we squeeze mm_stat and exc_sum into p7,
;		setting the low bit to indicate we took a double miss.
;	The invalid_pte code checks p7 and restores p5 if indicated.
;-
	START_HW_VECTOR <DTBM_SINGLE>

ASSUME OSF_P_MISC__PHYS__S eq 63

	hw_mfpr	p23, EV6__EXC_ADDR		; (0L) get exception address
	hw_mfpr	p4, EV6__VA_FORM		; (4-7,1L) get vpte address
	hw_mfpr	p5, EV6__MM_STAT		; (0L) get miss info
	hw_mfpr p7, EV6__EXC_SUM		; (0L) get exc_sum for ra

	hw_mfpr p6, EV6__VA			; (4-7,1L) get original va
	bic	p7, #1, p7			; clear double miss flag

	xor	p4, p6, p4			; interlock p4 and p6
	xor	p4, p6, p4			; restore p4

.if ne kseg_hack					; kseg hack
	hw_stq/p p7, PT__RSV_FOR_PAL(p_temp)		; save off p7
	and	p_misc, #<1@OSF_P_MISC__CM__S>, p7	; current mode
	beq	p7, trap__hack_kseg			; do hack for kseg
	hw_ldq/p p7, PT__RSV_FOR_PAL(p_temp)		; restore p7
	br	r31, trap__dtbm_single_vpte		; as we were

	CONT_HW_VECTOR					; 1.41
.endc

trap__dtbm_single_vpte:
	hw_ldq/v p4, (p4)			; (1L) get vpte

.if eq srm_console				; 1.49
	blt	p_misc, trap__d1to1		; (xU) <63>=1 => 1-to-1
.endc						; 1.49 srm_console
	blbc	p4, trap__invalid_dpte		; (xU) invalid => branch
;
; We need to avoid the situation where a ldx_l has acquired a lock,
; another processor has taken it, and a bad path before a stx_c has
; a load which pulls the data back in and makes it look like the
; lock has succeeded. So we either need to do a virtual load
; or hold up bad path loads by doing a sequence of mulq and
; a write to MM_STAT with bit 2 and 6 (6 is required or we hang).
;
; 1.50
; For the pte_eco case, do a ldbu down every path except the
; i/o case, where we couldn't have missed on a stx_c.
; 1.50
;
.if eq force_path2				; 1.45
	and	p4, #^x80, p7			; isolate mb bit
	xor	p7, #^x80, p7			; flip mb bit
.iff						; 1.45
	srl	p4, #62, p5			; get i/o space bit
	and	p5, #1, p5			; isolate i/o space bit
.endc						; 1.45

	ALIGN_FETCH_BLOCK <^x47FF041F>		; Edit 1.36

	PVC_VIOLATE <2>				; ignore scoreboard violation
	hw_mtpr	p6, EV6__DTB_TAG0		; (2&6,0L) write tag0
	hw_mtpr p6, EV6__DTB_TAG1		; (1&5,1L) write tag1
	hw_mtpr	p4, <EV6__DTB_PTE0 ! ^x44>	; (0,4,2,6) (0L) write pte0
	hw_mtpr	p4, <EV6__DTB_PTE1 ! ^x22>	; (3,7,1,5) (1L) write pte1

.if ne force_path2				; 1.45
	and	p4, #^xe0, p7			; isolate mb, GH bits
	xor	p7, #^x80, p7			; flip mb bit
	bis	p7, p5, p7			; merge bits
.endc

ASSUME <tb_mb_en + pte_eco> ne 2

.if ne pte_eco
	bne	p7, trap__dtbm_single_gh_mb_io	; 1.45 branch for mb

  .if ne force_path2				; 1.45
trap__dtbm_single_hack1:
	ldbu	r31, (p6)			; do a virtual operation
  .endc						; 1.45

	hw_ret	(p23)				; return

  .if ne force_path2				; 1.41
	CONT_HW_VECTOR
  .endc						; 1.41

trap__dtbm_single_gh_mb_io:
  .if ne force_path2
	and	p7, #^x81, p5			; check io,mb bits
	bne	p5, trap__dtbm_single_mb_io	; continue for GH case
	hw_mtpr p6, <EV6__VA ! ^x22>		; hold up loads
	ALIGN_FETCH_BLOCK <^x47FF041F>		; Edit 1.36
trap__dtbm_single_hack2:
	ldbu	r31, (p6)
	hw_ret	(p23)				; return

trap__dtbm_single_mb_io:
	blbs	p7, trap__dtbm_single_io	; continue for mb case
	mb
	hw_mtpr p6, <EV6__VA ! ^x22>		; hold up loads
	ALIGN_FETCH_BLOCK <^x47FF041F>		; Edit 1.36
trap__dtbm_single_hack3:
	ldbu	r31, (p6)
	hw_ret	(p23)				; return

trap__dtbm_single_io:
  .endc						; force_path2
	mb
	hw_ret	(p23)				; return

.iff						; 1.45 no need to be clever

  .if ne force_path2				; 1.45
	mb
	ALIGN_FETCH_BLOCK <^x47FF041F>
	mulq	p6, #1, p6			; hold up loads
	mulq	p6, #1, p6			; hols up loads
	hw_mtpr p6, <EV6__VA ! ^x22>		; hold up loads
  .endc						; 1.45

	hw_ret	(p23)				; return
.endc

.if eq force_path2				; 1.41
	CONT_HW_VECTOR
.endc						; 1.41


.if ne kseg_hack				; kseg hack
;
; tsunami hack
; current state:
;	p23	return address
;	p6	va
; Avoid issuing DTB write until the PALcode path is verified.
; Add a scoreboard interlock to avoid speculative reads.
; We won't return until we have committed the DTB write.
;

trap__hack_kseg:				; do hack for kseg
	srl	p6, #41, p7			; get <47:41>
	and	p7, #^x7F, p7			; clean <47:41>
	cmpeq	p7, #^x7E, p7			; kseg?
	beq	p7, trap__hack_kseg_not		; branch if not
	br	r31, trap__hack_kseg_bits	; test bits

trap__hack_kseg_not:				; not kseg
	srl	p6, #46, p7			; check for <47:46>
	and	p7, #^x3, p7			; clean <47:46>
	cmpeq	p7, #^x2, p7			; 48 bit kseg?
	beq	p7, trap__hack_kseg_not2	; branch if not
	br	r31, trap__hack_kseg_bits2	; test bits

trap__hack_kseg_not2:
	hw_ldq/p p7, PT__RSV_FOR_PAL(p_temp)	; restore p7
	br	r31, trap__dtbm_single_vpte	; as we were

trap__hack_kseg_bits:				; kseg
	srl	p6, #40, p7			; check <40> (IO)
	blbs	p7, trap__hack_kseg_bits_io	; branch for io
	srl	p6, #32, p7			; check <34:32>
	and	p7, #^x7, p7			; any bits 1?
	bne	p7, trap__hack_kseg_hack	; branch if so
	beq	p7, trap__hack_kseg_bits_io	; branch if not
	PVC_VIOLATE <1006>
	br	r31, .-4			; infinite loop
trap__hack_kseg_bits_io:			; merge for io addr
	bis	p6, r31, p7			; 1to1 translation into p7
	br	r31, trap__hack_kseg_1to1	; now map 1to1

trap__hack_kseg_hack:				; force nxm
	bis	r31, #1, p7			; get a 1
	sll	p7, #35, p7			; move into <35>
	bis	p6, p7, p7			; or into translation

trap__hack_kseg_1to1:
	sll	p7, #23, p7			; <40> to position 63
	sra	p7, #23, p7			; force sign extension
	sll	p7, #20, p7			; <43> to position 63
	srl	p7, #20, p7			; translation

	lda	p5, ^x1171(r31)				; KxE,GH=3,ASM=1,V=1
	srl	p7, #<13+9>, p4				; byte offset+9
	sll	p4, #<EV6__DTB_PTE0__PFN__S+9>, p4	; pfn into position
	bis	p4, p5, p4				; produce the pte

	ALIGN_FETCH_BLOCK <^x47FF041F>		; Edit 1.36

	PVC_VIOLATE <2>				; ignore scoreboard violation
	hw_mtpr	p6, EV6__DTB_TAG0		; (2&6,0L) write tag0
	hw_mtpr p6, EV6__DTB_TAG1		; (1&5,1L) write tag1
	hw_mtpr	p4, <EV6__DTB_PTE0 ! ^x44>	; (0,4,2,6) (0L) write pte0
	hw_mtpr	p4, <EV6__DTB_PTE1 ! ^x22>	; (3,7,1,5) (1L) write pte1

	hw_mtpr	r31, <EV6__MM_STAT ! ^xF0>	; (4-7,0L) stall outside IQ
	hw_ret	(p23)				; (0L) return

trap__hack_kseg_bits2:				; kseg
	srl	p6, #43, p7			; check <43> (IO)
	blbs	p7, trap__hack_kseg_bits2_io	; branch if io
	srl	p6, #32, p7			; check <34:32>
	and	p7, #^x7, p7			; any bits 1?
	bne	p7, trap__hack_kseg_hack2	; branch if so
	beq	p7, trap__hack_kseg_bits2_io	; branch if not
	PVC_VIOLATE <1006>
	br	r31, .-4			; infinite loop
trap__hack_kseg_bits2_io:			; merge for io
	bis	p6, r31, p7			; 1to1 translation into p7
	br	r31, trap__hack_kseg_1to1_2	; now map 1to1

trap__hack_kseg_hack2:				; force nxm
	bis	r31, #1, p7			; get a 1
	sll	p7, #35, p7			; move into <35>
	bis	p6, p7, p7			; or into translation

trap__hack_kseg_1to1_2:
	sll	p7, #20, p7			; <43> to position 63
	srl	p7, #20, p7			; translation

	lda	p5, ^x1171(r31)				; KxE,GH=3,ASM=1,V=1
	srl	p7, #<13+9>, p4				; byte offset+9
	sll	p4, #<EV6__DTB_PTE0__PFN__S+9>, p4	; pfn into position
	bis	p4, p5, p4				; produce the pte

	ALIGN_FETCH_BLOCK <^x47FF041F>		; Edit 1.36

	PVC_VIOLATE <2>				; ignore scoreboard violation
	hw_mtpr	p6, EV6__DTB_TAG0		; (2&6,0L) write tag0
	hw_mtpr p6, EV6__DTB_TAG1		; (1&5,1L) write tag1
	hw_mtpr	p4, <EV6__DTB_PTE0 ! ^x44>	; (0,4,2,6) (0L) write pte0
	hw_mtpr	p4, <EV6__DTB_PTE1 ! ^x22>	; (3,7,1,5) (1L) write pte1

	hw_mtpr	r31, <EV6__MM_STAT ! ^xF0>	; (4-7,0L) stall outside IQ
	hw_ret	(p23)				; (0L) return
.endc

;+
; trap__invalid_dpte
;
; Entry:
;	Branched to on invalid Dstream PTE fetched by DTB miss routine.
;
; Function:
;	Prepare to take a Translation Not Valid or Access Violation
;	exception. Take a different flow for pal mode.
;
; Current state:
;	p4	PTE
;	p5	no double miss: mm_stat 
;		double miss: scratch
;	p6	va
;	p7	no double miss: exc_sum 
;		double miss: <31:16>=mm_stat<15:0>,<15:1>=exc_sum<15:1>,<0>=1
;	p23	pc of instruction taking DTB miss
;
; Register use:
;	p20	available once we know we didn't trap from pal mode
;
;-
trap__invalid_dpte:
	blbs	p7, trap__invalid_dpte_get	; did we double miss?
	br	r31, trap__invalid_dpte_have	; we didn't

trap__invalid_dpte_get:				; we double missed
	srl	p7, #16, p5			; recover mm_stat

trap__invalid_dpte_have:
	blbs	p23, trap__tnv_in_pal			; TNV in pal mode

	blbc	p5, trap__invalid_dpte_check		; load => check x31

	srl	p5, #EV6__MM_STAT__OPCODE__S, p7	; get opcode
	and	p7, #EV6__MM_STAT__OPCODE__M, p7	; clean it
	cmpeq	p7, #^x18, p7				; ECB/WH64?
	bne	p7, trap__invalid_dpte_dismiss		; dismiss if so

	br	r31, trap__invalid_dpte_no_dismiss	; store
;
; Check for load x31
;
trap__invalid_dpte_check:
	srl	p7, #EV6__EXC_SUM__REG__S, p7	; get ra field
	and	p7, #EV6__EXC_SUM__REG__M, p7	; check for load x31
	cmpeq	p7, #^x1F, p7			; compare against x31
	bne	p7, trap__invalid_dpte_dismiss	; branch => dismiss
;
; Take a trap.
;
trap__invalid_dpte_no_dismiss:
	mb						; protect mm_stat and va

	and	p_misc, #<1@OSF_P_MISC__CM__S>, p7	; current mode
	and	p_misc, #OSF_P_MISC__PS__M, p20		; save original ps
	beq	p7, trap__invalid_dpte_stack		; skip switch if kernel
;
; Switch to kernel mode.
; Current state:
;	p4		PTE
;	p20		original ps
;
dpte_cm_offset = <trap__invalid_dpte_stack - trap__invalid_dpte_cm>

	hw_stq/p r30, PT__USP(p_temp)				; save user SP
	hw_ldq/p r30, PT__KSP(p_temp)				; get kernel SP
	srl	p4, #<OSF_PTE__URE__S - OSF_PTE__KRE__S>, p4	; shift u to k
	bic	p_misc, #OSF_P_MISC__PS__M, p_misc		; cm=0, ipl=0

	br	p7, trap__invalid_dpte_cm		; change mode to kernel
trap__invalid_dpte_cm:
	addq	p7, #<dpte_cm_offset+1>, p7		; jump past in palmode
	hw_mtpr	r31, EV6__PS				; (4,0L) switch to kern
	bsr	r31, .					; push prediction stack
	PVC_JSR	dpte_cm					; synch up
	hw_ret_stall (p7)				; pop prediction stack
	PVC_JSR	dpte_cm, dest=1
;
; Take the trap. Be careful of tb miss!
; Current state:
;	p4		PTE. If user, now shifted right to kernel position
;	p5		mm_stat
;	p6		va
;	p20		original ps
;	p23		pc of instruction taking dtb miss
;	p_misc__ps	if we were in kernel, original ps
;			if we were in user, new ps = 0 (cm=0, ipl=0)
;

trap__invalid_dpte_stack:
	hw_stq/p p23, PT__STACK_PC(p_temp)		; store away pc

	hw_stq/p p6, PT__NEW_A0(p_temp)			; save away va
	and	p5, #EV6__MM_STAT__WR__M, p5		; get r/w flag
	hw_stq/p p5, PT__NEW_A2(p_temp)			; save r/w flag

	srl	p4, #OSF_PTE__KRE__S, p4		; get <re> to <0>
	bis	r31, #OSF_MMCSR__ACV, p23		; assume acv
	srl	p4, #<OSF_PTE__KWE__S-OSF_PTE__KRE__S>, p7; get <we> to <0>
	cmovlbs	p5, p7, p4				; if write, check <we>
	cmovlbs p4, #OSF_MMCSR__TNV, p23		; tnv if access okay
	hw_stq/p p23, PT__NEW_A1(p_temp)		; save mmcsr

	lda	r30, <0-OSF_FRM__SIZE>(r30)		; allocate stack space

	stq	p20, OSF_FRM__PS(r30)			; save original ps
	stq	r18, OSF_FRM__A2(r30)			; save a2

	hw_ldq/p p20, PT__STACK_PC(p_temp)		; 1.39 get nextpc back

	stq	r29, OSF_FRM__GP(r30)			; save gp
	stq	r16, OSF_FRM__A0(r30)			; save a0
	stq	r17, OSF_FRM__A1(r30)			; save a1
	stq	p20, OSF_FRM__PC(r30)			; 1.39 save nextpc

	hw_ldq/p p23, PT__ENT_MM(p_temp)		; get entry point
	hw_ldq/p r29, PT__KGP(p_temp)			; get kgp
	hw_ldq/p r16, PT__NEW_A0(p_temp)		; a0 <- va
	hw_ldq/p r17, PT__NEW_A1(p_temp)		; a1 <- MMCSR
	hw_ldq/p r18, PT__NEW_A2(p_temp)		; a2 <- cause

	hw_ret	(p23)					; to os

;+
; Dismiss the load to x31. We don't need a MB to hold up loads that
; may affect MM_STAT. To get here, we have taken a conditional branch
; that depended on the mfpr MM_STAT being issued.
;-
trap__invalid_dpte_dismiss:
	addq	p23, #4, p23			; dismiss, so increment pc
	hw_ret	(p23)				; return

;+
; DTB miss in PALmode
;
; There are two sources of tnv_in_pal --
;	(1) urti user stack access - turn into mmfault
;	(2) stack builders, rti, retsys -- take a ksp not valid halt
;
; In pass1, there can also be a tnv_in_pal for floating ld/st instructions.
;
; Current state:
;	p4		PTE
;	p5		mm_stat
;	p6		va
;	p7		exc_sum
;	p23		pc of instruction taking DTB miss
;
;	PT__STACK_PC	in case of urti, pc+4 of call_pal invocation
;			in case of rti & retsys, leave pc+4 for now
;			in case of stack builders, whatever SavedPC was
;			in case of istream mchk check for cmov, exc_addr 
;- 
trap__tnv_in_pal:
	hw_stq/p r25, PT__R25(p_temp)	; get some scratch space
	hw_stq/p r26, PT__R26(p_temp)	; get some scratch space

	bic	p23, #3, r26		; clean PAL pc
	hw_mfpr	r25, EV6__PAL_BASE	; (4,0L) get pal base
	subq	r25, r26, r26		; pal base - cleaned pc

	lda	p7, <call_pal__urti_ldq - trap__pal_base>(r26)
	zapnot	p7, #^x3, p7		; 1.41 clean to be safe
	beq	p7, trap__tnv_in_pal_urti

					; 1.37 take out urti_stq

					; 1.41 test for cmov check
	lda	p7, <sys__mchk_istream_check_cmov - trap__pal_base>(r26)
	zapnot	p7, #^x3, p7
	beq	p7, trap__tnv_in_pal_istream_mchk

.if ne ev6_p1
	lda	p7, <trap__emul_ldf - trap__pal_base>(r26)
	beq	p7, trap__emul_ldst_tnv
	
	lda	p7, <trap__emul_ldg - trap__pal_base>(r26)
	beq	p7, trap__emul_ldst_tnv
	
	lda	p7, <trap__emul_lds - trap__pal_base>(r26)
	beq	p7, trap__emul_ldst_tnv
	
	lda	p7, <trap__emul_ldt - trap__pal_base>(r26)
	beq	p7, trap__emul_ldst_tnv
	
	lda	p7, <trap__emul_stf_s - trap__pal_base>(r26)
	beq	p7, trap__emul_ldst_tnv
	
	lda	p7, <trap__emul_stg_s - trap__pal_base>(r26)
	beq	p7, trap__emul_ldst_tnv
	
	lda	p7, <trap__emul_sts_s - trap__pal_base>(r26)
	beq	p7, trap__emul_ldst_tnv
	
	lda	p7, <trap__emul_stt_s - trap__pal_base>(r26)
	beq	p7, trap__emul_ldst_tnv
.endc

	br	r31, trap__ksp_invalid
;+
; trap__tnv_in_pal_urti
;
; Current state:
;	p4		PTE
;	p5		mm_stat
;	p6		va
;	p7		exc_sum
;	r25		needs to be restored
;	r26		needs to be restored
;
;	PT__STACK_PC	pc+4 of urti
;-
trap__tnv_in_pal_urti:
	hw_ldq/p r25, PT__R25(p_temp)			; restore r25
	hw_ldq/p r26, PT__R26(p_temp)			; restore r26

	hw_ldq/p p23, PT__STACK_PC(p_temp)		; get urti pc+4
	subq	p23, #4, p23				; back up the pc

	br	r31, trap__invalid_dpte_no_dismiss	; merge to post
;+
; trap__tnv_in_pal_istream_mchk
;
; Current state:
;	p4		PTE
;	p5		mm_stat
;	p6		va
;	p7		exc_sum
;	r25		needs to be restored
;	r26		needs to be restored
;
;	PT__STACK_PC	exc_addr
;	PT__R1		saved mchk frame addr
;-
trap__tnv_in_pal_istream_mchk:				; 1.41
	hw_ldq/p r25, PT__R25(p_temp)			; restore r25
	hw_ldq/p r26, PT__R26(p_temp)			; restore r26

	br	r31, sys__mchk_istream_cmov_fault	; error on cmov
;+
; KSP invalid
;
; Entry:
;	Branched to on KSP not valid.
;
; Function:
;	Restore r25 and r26.
;	Recover original pc.
;	Set PT__HALT_CODE to HALT__KSP_INVAL.
;	Branch to update PCB and enter console.
;
; Current state:
;	EV6__PS		kernel
;
;	PT__USP		updated on switch from user to kernel
;	PT__STACK_PC	original SavedPC or pc+4
;
;	p20		original ps
;	p_misc_ps	if we were in kernel, original ps
;			if we were in user, ps = 0 (cm=0, ipl=0)
;
;	r25		needs to be restored
;	r26		needs to be restored
;	r30		pointing to base of stack frame		
;-
trap__ksp_invalid:
	hw_ldq/p r25, PT__R25(p_temp)		; restore r25
	hw_ldq/p r26, PT__R26(p_temp)		; restore r26

	hw_ldq/p p23, PT__STACK_PC(p_temp)	; get back original pc

	lda	p20, HALT__KSP_INVAL(r31)	; ksp invalid
	hw_stq/p p20, PT__HALT_CODE(p_temp)	; store code (??)

	br	r31, trap__update_pcb_and_halt
;+
; Do a 1-1 mapping
; Current state:
;	p6	va
;	p23	exc_addr

; We don't need a MB to hold up loads that may affect VA, etc.
; The writes to DTB_TAGx and DTB_PTE0 hold up loads.
;-
trap__d1to1:
	lda	p5, ^x3301(r31)			; all r/w enable
	srl	p6, #13, p4			; shift out the byte offset
	sll	p4, #EV6__DTB_PTE0__PFN__S, p4	; get pfn into position
	bis	p4, p5, p4			; produce the pte

	ALIGN_FETCH_BLOCK <^x47FF041F>		; Edit 1.36

	PVC_VIOLATE <2>				; ignore scoreboard violation
	hw_mtpr	p6, EV6__DTB_TAG0		; (2&6,0L) write tag0
	hw_mtpr p6, EV6__DTB_TAG1		; (1&5,1L) write tag1
	hw_mtpr	p4, <EV6__DTB_PTE0 ! ^x44>	; (0,4,2,6) (0L) write pte0
	hw_mtpr	p4, <EV6__DTB_PTE1 ! ^x22>	; (3,7,1,5) (1L) write pte1

	hw_mtpr	r31, <EV6__MM_STAT ! ^xF0>	; (4-7,0L) stall outside IQ

    .if eq force_path				; 1.41
	hw_ret	(p23)				; do the ret
    .iff
	ALIGN_FETCH_BLOCK <^x47FF041F>		; align
	PVC_VIOLATE <1007>
	PVC_VIOLATE <1020>			; stop permutation
	hw_jmp	(p23)				; return with jmp
	br	r31, .-4			; stop predictor
    .endc					; 1.41                                                
	END_HW_VECTOR

;+
; DFAULT - offset 380
;
; Entry:
;	Vectored into via hardware trap on a Dstream fault or virtual
;	address sign check error.
;
; Function:
;	Prepare to take an Access Violation, FOR, or FOW.
;
; Note: VA and MM_STAT are not written on ld_vpte fault. Also, on
; dfault not in pal, we can only get one of FOR or FOW (only
; hw_ld/w can get both), so the order of checking does not matter.
;
; Register use:
;	p4	scratch
;	p5	mm_stat. For ld_vpte fault, should be unchanged from dtb miss
;	p6	exc_addr
;	p7	exc_sum
;	p20	available once we determine we weren't in pal mode
;
; Exit state:
;	On exit to trap__dc_tag_perr
;	p5	mm_stat
;	p6	pc of faulting instruction
;	p7	exc_sum
;
;	On exit to dfault_in_pal
;	p5	mm_stat
;	p6	pc of faulting instruction
;	p7	exc_sum
;-
	START_HW_VECTOR <DFAULT>

	hw_mfpr	p6, EV6__EXC_ADDR		; (0L) save exception address
	hw_mfpr p7, EV6__EXC_SUM		; (0L) get exc_sum
	hw_mfpr	p5, EV6__MM_STAT		; (0L) get mm_stat

	srl	p5, #EV6__MM_STAT__DC_TAG_PERR__S, p4
	blbs	p4, trap__dc_tag_perr		; dc_tag_perr => another flow
	blbs	p6, trap__dfault_in_pal		; pal mode => another flow

	CONT_HW_VECTOR <DFAULT>

	blbc	p5, trap__dfault_check		; load => check x31

	srl	p5, #EV6__MM_STAT__OPCODE__S, p7	; get opcode
	and	p7, #EV6__MM_STAT__OPCODE__M, p7	; clean it
	cmpeq	p7, #^x18, p7				; ECB/WH64?
	bne	p7, trap__dfault_dismiss		; dismiss if so

	br	r31, trap__dfault_no_dismiss	; store => no dismiss

trap__dfault_check:
	srl	p7, #EV6__EXC_SUM__REG__S, p7	; get ra
	and	p7, #EV6__EXC_SUM__REG__M, p7	; clean ra
	cmpeq	p7, #^x1F, p7			; compare against x31
	bne	p7, trap__dfault_dismiss	; branch => x31
;
; Take the trap.
; Current state.
;	p5	mm_stat
;	p6	exc_addr
;
trap__dfault_no_dismiss:

	hw_mfpr	p23, EV6__VA				; (4-7,1L) get va

	mb						; protect mm_stat and va

	and	p_misc, #<1@OSF_P_MISC__CM__S>, p4	; current mode
	and	p_misc, #OSF_P_MISC__PS__M, p20		; save original ps
	beq	p4, trap__dfault_stack			; skip switch if kernel
;
; Switch to kernel mode.
; Current state:
;	p5		mm_stat
;	p6		exc_addr
;	p20		original ps
;	p23		va
;
dfault_cm_offset = <trap__dfault_stack - trap__dfault_cm>

	hw_stq/p r30, PT__USP(p_temp)				; save user SP
	hw_ldq/p r30, PT__KSP(p_temp)				; get kernel SP
	bic	p_misc, #OSF_P_MISC__PS__M, p_misc		; cm=0, ipl=0

	br	p4, trap__dfault_cm			; change mode to kernel
trap__dfault_cm:
	addq	p4, #<dfault_cm_offset+1>, p4		; jump past in palmode
	hw_mtpr	r31, EV6__PS				; (4,0L) switch to kern
	bsr	r31, .					; push prediction stack
	PVC_JSR	dfault_cm				; synch up
	hw_ret_stall (p4)				; pop prediction stack
	PVC_JSR	dfault_cm, dest=1
;
; Take the trap. Be careful of tb miss!
; Current state:
;	p5		mm_stat
;	p6		exc_addr
;	p20		original ps
;	p23		va
;	p_misc__ps	if we were in kernel, original ps
;			if we were in user, new ps = 0 (cm=0, ipl=0)
;
trap__dfault_stack:
	hw_stq/p p6, PT__STACK_PC(p_temp)		; store away pc
	hw_stq/p p23, PT__NEW_A0(p_temp)		; store away va
	srl	p5, #1, p7				; shift mm_stat bits
	and	p7, #^x7, p7				; clean to fow/for/acv
	cmovlbs	p7, #1, p7				; take acv over fox
	hw_stq/p p7, PT__NEW_A1(p_temp)			; MMCSR
	and	p5, #1, p4				; get r/w bit
	hw_stq/p p4, PT__NEW_A2(p_temp)			; store away r/w bit

	lda	r30, <0-OSF_FRM__SIZE>(r30)		; allocate stack space

	stq	p20, OSF_FRM__PS(r30)			; save original ps
	stq	r18, OSF_FRM__A2(r30)			; save a2

	hw_ldq/p p20, PT__STACK_PC(p_temp)		; 1.39 get nextpc back

	stq	r29, OSF_FRM__GP(r30)			; save gp
	stq	r16, OSF_FRM__A0(r30)			; save a0
	stq	r17, OSF_FRM__A1(r30)			; save a1
	stq	p20, OSF_FRM__PC(r30)			; 1.39 save pc

	hw_ldq/p p23, PT__ENT_MM(p_temp)		; get entry point
	hw_ldq/p r29, PT__KGP(p_temp)			; get kgp
	hw_ldq/p r16, PT__NEW_A0(p_temp)		; a0 <- va
	hw_ldq/p r17, PT__NEW_A1(p_temp)		; a1 <- opcode
	hw_ldq/p r18, PT__NEW_A2(p_temp)		; a2 <- register number

	hw_ret	(p23)					; to os

;+
; Dismiss load x31
;
; We don't need a MB to hold up loads that may affect MM_STAT.
; To get here, we have taken conditional branches that on the
; mfpr MM_STAT being issued.

;-
trap__dfault_dismiss:
	addq	p6, #4, p6			; dismiss
	hw_ret	(p6)
;+
; trap__dfault_in_pal
;
; Entry:
; 	Dstream fault trap has been taken, exc_addr points to pal code.
;
; Current state:
;	p5 		mm_stat
;	p6		pc of instruction causing Dstream fault. Can be from
;				call_pal PALcode
;				kernel stack processing (KSP not valid case)
;	p7		available
;	p23		if vpte dfault => exc_addr of instruction causing miss
;
;	PT__STACK_PC	in case of urti, pc+4 of call_pal invocation
;			in case of rti & retsys, leave pc+4 for now
;			in case of stack builders, whatever SavedPC was
;
;	CNS__FP_PC	for pass1, user pc of floating point ld/st
;
;	VA		faulting VA
;

; Exit state:
;-
trap__dfault_in_pal:
	hw_stq/p r25, PT__R25(p_temp)	; get some scratch space
	hw_stq/p r26, PT__R26(p_temp)	; get some scratch space

	bic	p6, #3, r26		; clean PAL pc
	hw_mfpr	r25, EV6__PAL_BASE	; (4,0L) get pal base
	subq	r25, r26, r26		; pal base - cleaned pc

	.if ne pte_eco			; 1.49 conditionalize further
	.if ne force_path2		; 1.44 test for ldbu in dtb miss

	lda	p7, <trap__dtbm_single_hack1 - trap__pal_base>(r26)
	zapnot	p7, #^x3, p7
	beq	p7, trap__stc_hack

	lda	p7, <trap__dtbm_single_hack2 - trap__pal_base>(r26)
	zapnot	p7, #^x3, p7
	beq	p7, trap__stc_hack

	lda	p7, <trap__dtbm_single_hack3 - trap__pal_base>(r26)
	zapnot	p7, #^x3, p7
	beq	p7, trap__stc_hack

	.endc				; 1.44 force_path2
	.endc				; 1.49 pte_eco

	lda	p7, <trap__itb_miss_vpte - trap__pal_base>(r26)
	beq	p7, trap__ldvpte_dfault

	lda	p7, <trap__dtbm_single_vpte - trap__pal_base>(r26)
	beq	p7, trap__ldvpte_dfault

	lda	p7, <call_pal__urti_ldq - trap__pal_base>(r26)
	zapnot	p7, #^x3, p7		; 1.41 clean to be safe
	beq	p7, trap__dfault_in_pal_urti

					; 1.37 take out urti_stq

					; 1.41 test for cmov check
	lda	p7, <sys__mchk_istream_check_cmov - trap__pal_base>(r26)
	zapnot	p7, #^x3, p7
	beq	p7, trap__dfault_in_pal_istream_mchk

.if ne ev6_p1
	lda	p7, <trap__emul_ldf - trap__pal_base>(r26)
	beq	p7, trap__emul_ldst_dfault
	
	lda	p7, <trap__emul_ldg - trap__pal_base>(r26)
	beq	p7, trap__emul_ldst_dfault
	
	lda	p7, <trap__emul_lds - trap__pal_base>(r26)
	beq	p7, trap__emul_ldst_dfault
	
	lda	p7, <trap__emul_ldt - trap__pal_base>(r26)
	beq	p7, trap__emul_ldst_dfault
	
	lda	p7, <trap__emul_stf_s - trap__pal_base>(r26)
	beq	p7, trap__emul_ldst_dfault
	
	lda	p7, <trap__emul_stg_s - trap__pal_base>(r26)
	beq	p7, trap__emul_ldst_dfault
	
	lda	p7, <trap__emul_sts_s - trap__pal_base>(r26)
	beq	p7, trap__emul_ldst_dfault
	
	lda	p7, <trap__emul_stt_s - trap__pal_base>(r26)
	beq	p7, trap__emul_ldst_dfault
.endc

	br	r31, trap__ksp_invalid		; must be ksp invalid

.if ne force_path2				; 1.44
;+
; trap__stc_hack
;
; Current state:
;	p6		pal pc
;	p23		original pc
;	r25		needs to be restored
;	r26		needs to be restored
;
; The real stx_c will fault as well. Recover back to
; dtbm miss flow after the load.
;-
trap__stc_hack:
	hw_ldq/p r25, PT__R25(p_temp)		; restore r25
	hw_ldq/p r26, PT__R26(p_temp)		; restore r26

	addq	p6, #4, p6			; return past load
	hw_ret	(p6)				; return
.endc						; 1.44
;+
; trap__dfault_in_pal_urti
;
; Current state:
;	p5 		mm_stat
;	r25		needs to be restored
;	r26		needs to be restored
;
;	VA		faulting VA
;
;	PT__STACK_PC	pc+4 of urti
;
; Exit to trap__dfault_merge_from_urti:
;	p5		mm_stat
;	p6		exc_addr
;-
trap__dfault_in_pal_urti:
	hw_ldq/p r25, PT__R25(p_temp)			; restore r25
	hw_ldq/p r26, PT__R26(p_temp)			; restore r26

	hw_ldq/p p6, PT__STACK_PC(p_temp)		; get urti pc+4
	subq	p6, #4, p6				; back up the pc
	br	r31, trap__dfault_no_dismiss		; merge to post
;+
; trap__dfault_in_pal_istream_mchk
;
; Current state:
;	p5 		mm_stat
;	r25		needs to be restored
;	r26		needs to be restored
;
;	VA		faulting VA
;
;	PT__STACK_PC	exc_addr
;	PT__R1		save mchk frame addr
;-
trap__dfault_in_pal_istream_mchk:				; 1.41
	hw_ldq/p r25, PT__R25(p_temp)			; restore r25
	hw_ldq/p r26, PT__R26(p_temp)			; restore r26

	br	r31, sys__mchk_istream_cmov_fault	; error on cmov
;+
; trap__ldvpte_dfault
;
; Entry:
;	Branched to from dfault of ld_vpte during tb miss processing.
;	For itb miss, the originator is user code. For dtb miss, the
;	originator can be user code, PALcode (including stack processing).
;
;	This is a halt condition. This means that we had a valid level 2
;	PTE with KRE/KWE=0 in the TB, pulled in by the double miss
;	flow. The SRM states that protection is ignored on level 1
;	and level 2 valid PTE's, implying that valid level 1 and
;	level 2 PTE's must have KRE/WRE set so that TB accesses work.
;
; Function:
;	Restore r25 and r26.
;	Set PT__HALT_CODE to HALT__PTBR_INVAL.
;	Recover original pc.
;	Branch to update PCB and enter console.
;
; Current state:
;	r25		needs to be restored
;	r26		needs to be restored
;
;	p23		1.44 pc of instruction causing tb miss
;-
trap__ldvpte_dfault:
	hw_ldq/p r25, PT__R25(p_temp)			; restore r25
	hw_ldq/p r26, PT__R26(p_temp)			; restore r26

	lda	p20, HALT__PTBR_INVAL(r31)		; ptbr invalid
	hw_stq/p p20, PT__HALT_CODE(p_temp)		; store code (??)

	; 1.44 delete PT_VPTE_PC load into p23. p23 still valid.
	br	r31, trap__update_pcb_and_halt

;+
; dc_tag_perr
;
; Dcache tag parity error occured during the initial tag probe of a load
; or store instruction. This error created a synchronous fault to the dfault
; PALcode entry point, and is correctable. The virtual address associated
; with the error is available in the VA register.
;
; We evict using 2 hw_ld's with pa's that with the same index, and
; different tags from each other.
;
; WARNING!!!!! This code really only works if both sets were originally
; enabled. Otherwise we could load into the normally disabled cache, and
; could hit there even when it is re-disabled. So if we have systems that
; we allow to run with one cache, we have to be careful to chose pa's that
; the PALcode would normally never touch.
;
; If we are in PALmode, we do an error halt. Shadow registers have already
; been clobbered.
;
; Current state:
;	p5		mm_stat
;	p6		exc_addr
;	p7		exc_sum
;-
NO_DCTAG_PAR_EN = -
		<<dcache_set_en@EV6__DC_CTL__SET_EN__S> ! -
		 <0@EV6__DC_CTL__DCTAG_PAR_EN__S> ! -
		 <0@EV6__DC_CTL__DCDAT_ERR_EN__S>>

	ALIGN_FETCH_BLOCK

trap__dc_tag_perr:
	hw_mfpr	p4, EV6__VA			; (4-7,1L) get va
	bis	p6, p6, p23			; move exc_addr to p23
	bis	r31, #NO_DCTAG_PAR_EN, p20	; disable dctag_par_en
	hw_mtpr	p20, EV6__DC_CTL		; (6,0L)

	hw_mtpr	p20, EV6__DC_CTL		; (6,0L) force retire
	zap	p4, #^xF8, p4			; use va<23:0> as a pa

	mb

	ldah	p6, 1(r31)			; use <16> as a toggle bit
	xor	p4, p6, p6			; xor to get toggle
	hw_ldq/p p7, (p6)			; force evict from one set

	ldah	p6, 2(r31)			; use <17> as a toggle bit
	xor	p4, p6, p6			; xor to get toggle
	hw_ldq/p p7, (p6)			; force evict from other

	mb					; force completion

	ALIGN_FETCH_BLOCK <^x47FF041F>

	hw_ldq/p p20, PT__IMPURE(p_temp)	; impure base
	hw_ldq/p p20, CNS__DC_CTL(p20)		; get old DC_CTL value
	hw_mtpr	p20, EV6__DC_CTL		; (6,0L) restore DC_CTL
	bis	r31, r31, r31

	hw_mtpr	p20, EV6__DC_CTL		; (6,0L) force retire
	br	r31, sys__mchk_dc_tag_perr	; post retryable mchk

	END_HW_VECTOR

;+
; OPCDEC - offset 400
;
; Entry:
;	Vectored into via hardware trap on illegal opcode or function field
;	fault.
; Function:
;	Build stack frame.
;		r16 (a0)	opDec code
;		r17 (a1)	unpredictable
;		r18 (a2)	unpredictable
;	Vector via entIF.
;-

	START_HW_VECTOR <OPCDEC>

ASSUME OSF_P_MISC__PS__S eq 0

	hw_mfpr	p23, EV6__EXC_ADDR			; (0L) save exc_addr
	NOP						; no conditional br
	NOP						;	in 1st block
	NOP

.if ne ev6_p1
	hw_ldq/p p4, PT__IMPURE(p_temp)		; get base of impure area
	br	r31, trap__emul_merge		; let floating emulator handle
trap__opcdec_merge:
.endc

	blbs	p23, trap__pal_exc_bugcheck		; mchk if fen within pal
	addq	p23, #4, p23				; next pc

trap__opcdec_call_pal:
	and	p_misc, #<1@OSF_P_MISC__CM__S>, p4	; current mode
	and	p_misc, #OSF_P_MISC__PS__M, p20		; save original ps
	beq	p4, trap__opcdec_stack			; skip switch if kernel
;
; Switch to kernel mode. 
;
opcdec_cm_offset = <trap__opcdec_stack - trap__opcdec_cm>

	hw_stq/p r30, PT__USP(p_temp)			; save user SP
	hw_ldq/p r30, PT__KSP(p_temp)			; get kernel SP
	bic	p_misc, #OSF_P_MISC__PS__M, p_misc	; cm=0, ipl=0

	br	p6, trap__opcdec_cm			; change mode to kernel
trap__opcdec_cm:
	addq	p6, #<opcdec_cm_offset+1>, p6		; jump past in palmode
	hw_mtpr	r31, EV6__PS				; (4,0L) switch to kern
	bsr	r31, .					; push prediction stack
	PVC_JSR	opcdec_cm				; synch up
	hw_ret_stall (p6)				; pop prediction stack
	PVC_JSR	opcdec_cm, dest=1
;
; Take the trap. Be careful of tb miss!
; Current state:
;	p_misc__ps	if we were in kernel, original ps
;			if we were in user, new ps = 0 (cm=0, ipl=0)
;	p20		original ps
;	p23		nextpc
;
trap__opcdec_stack:
	hw_stq/p p23, PT__STACK_PC(p_temp)		; store away nextpc

	lda	r30, <0-OSF_FRM__SIZE>(r30)		; allocate stack space

	stq	p20, OSF_FRM__PS(r30)			; save original ps
	stq	r18, OSF_FRM__A2(r30)			; save a2

	hw_ldq/p p20, PT__STACK_PC(p_temp)		; 1.39 get pc back

	stq	r29, OSF_FRM__GP(r30)			; save gp
	stq	r16, OSF_FRM__A0(r30)			; save a0
	stq	r17, OSF_FRM__A1(r30)			; save a1
	stq	p20, OSF_FRM__PC(r30)			; 1.39 save next pc

	hw_ldq/p p23, PT__ENT_IF(p_temp)		; get entry point
	hw_ldq/p r29, PT__KGP(p_temp)			; get kgp
	bis	r31, #OSF_A0__OPDEC, r16		; set a0 

	hw_ret	(p23)					; to os

	END_HW_VECTOR

;+
; IACV - offset 480
;
; Entry:
;	Vectored into via hardware trap on a Istream access violation or
;	virtual address sign check/overflow/underflow error.
;
; Function:
;	For native mode, take an exception with the exception pc as follows:
;	IF (bad_iva, i.e, jsr)
;	THEN exception pc = va<63:0>
;	ELSE	IF (pc_ovfl & va_48)
;		THEN	IF (exc_addr<47>)
;			THEN exception pc = exc_addr<47:0> with <63:48>=0
;			ELSE exception pc = exc_addr<47:0> with <63;48>=1
;		ELSE	exception pc = exc_addr<63:0>
;
;	For PALmode, enter console with exception pc as follows:
;	Has to be bad_iva, i.e. jsr with exception pc= va<63:1>!1
;
; Register use:
;	p4	temporary
;	p5	temporary
;	p6	exc_addr
;	p7	exc_sum
;-
	START_HW_VECTOR <IACV>

	hw_mfpr	p6, EV6__EXC_ADDR		; (0L) save exception address
	hw_mfpr	p7, EV6__EXC_SUM		; (0L) check bad_iva and pc_ovfl
	NOP					; no conditional br
	NOP					;	in 1st block

	blbs	p6, trap__iacv_pal		; set=>bad iva in pal mode!
	srl	p7, #EV6__EXC_SUM__BAD_IVA__S, p4
	blbs	p4, trap__iacv_bad_iva		; set=>bad_iva
	srl	p7, #EV6__EXC_SUM__PC_OVFL__S, p4
	blbs	p4, trap__iacv_pc_ovfl		; set=>pc_ovfl
	br	r31, trap__iacv_post		; normal acv
;
; We had a pc overflow/underflow. If 43 bit mode, we can use exc_addr. If
; 48 bit mode, we need to look at bit 47 and do some work to produce the
; correct exception pc.
;
trap__iacv_pc_ovfl:
	hw_mfpr	p4, EV6__I_CTL			; (4,0L) look at va_48
	srl	p4, #EV6__I_CTL__VA_48__S, p4
	blbs	p4, trap__iacv_va_48		; set=>pc_ovfl & va_48
	br	r31, trap__iacv_post		; post as is from exc_addr

trap__iacv_va_48:				; pc_ovfl & va_48
	srl	p6, #47, p4			; check exc_addr<47>
	blbc	p4, trap__iacv_under		; branch for underflow
	zap	p6, #^xC0, p6			; overflow=> <63:48>=0
	br	r31, trap__iacv_post		; post

trap__iacv_under:				; underflow=> <63:48>=1
	subq	r31, #1, p4			; generate all 1s
	zapnot	p4, #^xC0, p4			; clean lower bytes
	or	p6, p4, p6			; or 1s into <63:48>
	br	r31, trap__iacv_post		; post
;
; JSR memory format instruction generated a bad va.
; Restrictions list says <63:1> are valid. Should we zap <0>?? Yes, for now.
;	
trap__iacv_bad_iva:
	hw_mfpr p7, EV6__VA			; (4-7,1L) get address from va
	bic	p7, #1, p6			; zap bit <0> and store in p6

	mb					; protect va (??)

	br	r31, trap__iacv_post		; post

	CONT_HW_VECTOR				; continue in free space
;
; Post the trap.
; Current state:
;	p6	exc_addr
;
trap__iacv_post:
	and	p_misc, #<1@OSF_P_MISC__CM__S>, p7	; current mode
	and	p_misc, #OSF_P_MISC__PS__M, p20		; save original ps
	beq	p7, trap__iacv_stack			; skip switch if kernel
;
; Switch to kernel mode.
; Current state:
;	p6		exc_addr
;	p20		original ps
;
iacv_cm_offset = <trap__iacv_stack - trap__iacv_cm>

	hw_stq/p r30, PT__USP(p_temp)				; save user SP
	hw_ldq/p r30, PT__KSP(p_temp)				; get kernel SP
	bic	p_misc, #OSF_P_MISC__PS__M, p_misc		; cm=0, ipl=0

	br	p7, trap__iacv_cm			; change mode to kernel
trap__iacv_cm:
	addq	p7, #<iacv_cm_offset+1>, p7		; jump past in palmode
	hw_mtpr	r31, EV6__PS				; (4,0L) switch to kern
	bsr	r31, .					; push prediction stack
	PVC_JSR	iacv_cm					; synch up
	hw_ret_stall (p7)				; pop prediction stack
	PVC_JSR	iacv_cm, dest=1
;
; Take the trap. Be careful of tb miss!
; Current state:
;	p6		exc_addr
;	p20		original ps
;	p_misc__ps	if we were in kernel, original ps
;			if we were in user, new ps = 0 (cm=0, ipl=0)
;
trap__iacv_stack:
	hw_stq/p p6, PT__STACK_PC(p_temp)		; store away pc

	lda	r30, <0-OSF_FRM__SIZE>(r30)		; allocate stack space

	stq	p20, OSF_FRM__PS(r30)			; save original ps
	stq	r18, OSF_FRM__A2(r30)			; save a2

	hw_ldq/p p20, PT__STACK_PC(p_temp)		; 1.39 get nextpc back

	stq	r29, OSF_FRM__GP(r30)			; save gp
	stq	r16, OSF_FRM__A0(r30)			; save a0
	stq	r17, OSF_FRM__A1(r30)			; save a1
	stq	p20, OSF_FRM__PC(r30)			; 1.39 save pc

	hw_ldq/p p23, PT__ENT_MM(p_temp)		; get entry point
	hw_ldq/p r29, PT__KGP(p_temp)			; get kgp
	bis	p20, r31, r16				; 1.39 a0 <- pc/va
	lda	r17, OSF_MMCSR__ACV(r31)		; a1 <- MMCSR
	subq	r31, #1, r18				; a2 <- istream (-1)

	hw_ret	(p23)					; to os
;
; JSR memory format instruction generated a bad va while in pal mode.
; We don't even bother checking the bad_iva bit, because that's the only
; way we can get here in pal mode.
;
; Set PT__HALT_CODE to HALT__SW_HALT. (Is there a better one ??)
; Branch to update PCB and enter console.
;
; For now, we stick a MB in to ensure that the mfpr VA is issued, though
; it's unlikely we will have any virtual loads after this.
;

trap__iacv_pal:
	hw_mfpr	p7, EV6__VA			; (4-7,1L) get address from va
	bis	p7, #1, p7			; or in PALmode bit
	lda	p20, HALT__SW_HALT(r31)		; is there a better code ??
	hw_stq/p p20, PT__HALT_CODE(p_temp)	; store code (??)
	bis	p7, r31, p23			; fault pc to p23

	mb					; make sure we issue mfpr VA

	br	r31, trap__update_pcb_and_halt

	END_HW_VECTOR
;+
; MCHK - offset 500
;
; Entry:
;	Vectored into via hardware trap on a machine check.
;-
	START_HW_VECTOR <MCHK>

	hw_mfpr	p23, EV6__EXC_ADDR		; (0L) exception addr
	bis	r31, r31, r31			; 1.41 cut down on double mchks
	bis	r31, r31, r31
	bis	r31, r31, r31

	beq	r31, trap__mchk_mb
trap__mchk_mb:
	mb
	mb					; 1.41 end of new part
	br	r31, sys__mchk			; finish in system part
;
; Bugcheck from call_pal
; Current state:
;	p23		pc+4 of call_pal
;
trap__pal_os_bugcheck:
	subq	p23, #4, p23			; decrement pc
	lda	p7, MCHK__OS_BUGCHECK(r31)	; mchk code
	br	r31, trap__pal_bugcheck
;
; Bugcheck from pal mode
; Current state:
;	p23		exc_addr
;
trap__pal_exc_bugcheck:
	lda	p7, MCHK__BUGCHECK(r31)		; mchk code

trap__pal_bugcheck:

	sll	p7, #OSF_P_MISC__MCHK_CODE__S, p7
	extbl	p_misc, #2, p5			; get mces
	zap	p_misc, #^x78, p_misc		; clear mchk_code & SCBv

	bis	p5, #<1@MCES__MCHK__S>, p6	; set MCES<MCHK>
	sll	p6, #OSF_P_MISC__MCES__MCHK__S, p6

	lda	p4, SCB__PROCMCHK(r31)		; SCB vector
	sll	p4, #OSF_P_MISC__SCBV__S, p4	; move SCBv into position

	bis	p6, p4, p6			; or mces and scbv
	bis	p6, p7, p_misc			; or mchk code

	blbs	p5, sys__double_machine_check	; halt on double
;
; Now compute where the frame is.
;
	CONT_HW_VECTOR<MCHK>				; 1.41 move to here

	hw_ldq/p p4, PT__WHAMI(p_temp)			; get whami

	lda	p5, PAL__LOGOUT_SPECIFIC_SIZE(r31)	; short&long size
	mulq	p4, p5, p5				; * whami

	GET_32CONS	p6, PAL__LOGOUT_BASE, r31	; logout base
	addq	p5, p6, p5				; (size*whami) + base
	lda	p5, MCHK__BASE(p5)			; start of mchk area

	hw_stq/p p23, MCHK__EXC_ADDR(p5)		; store exc_addr
	hw_stq/p p23, PT__STACK_PC(p_temp)		; save fault pc
							; 1.41 delete
							;   erroneous branch
	hw_mfpr	p4, EV6__ISUM				; 1.41 get isum here
	hw_stq/p p4, MCHK__ISUM(p5)			; 1.41 save isum
;
; To be neat, write 0 to the cbox logout quadwords and mm_stat logout
; quadword. Log ic_stat and dc_stat but don't clear them.
;
	hw_stq/p r31, MCHK_CRD__DC1_SYNDROME(p5)	; store 0
	hw_stq/p r31, MCHK_CRD__DC0_SYNDROME(p5)	; store 0
	hw_stq/p r31, MCHK_CRD__C_STAT(p5)		; store 0
	hw_stq/p r31, MCHK_CRD__C_STS(p5)		; store 0
	hw_stq/p r31, MCHK_CRD__C_ADDR(p5)		; store 0

	hw_stq/p r31, MCHK_CRD__MM_STAT(p5)		; store 0

	hw_mfpr p4, EV6__I_STAT				; (4,0L) get i_stat
	hw_mfpr	p6, EV6__DC_STAT			; (6,0L) get dc_stat
	hw_stq/p p4, MCHK_CRD__I_STAT(p5)		; store i_stat
	hw_stq/p p6, MCHK_CRD__DC_STAT(p5)		; store dc_stat

	bis	r31, r31, p20				; no retry
	br	r31, sys__mchk_header			; merge

	END_HW_VECTOR

;+
; ITB_MISS - offset 580
;
; Entry:
;	Vectored into via hardware trap on Istream translation buffer miss.
;
; Function:
;	Do a virtual fetch of the PTE, and fill the ITB if the PTE is both
;	valid and is not FOE. If invalid or FOE, exit to trap__invalid_ipte
;	or trap__foe. The virtual fetch can trap into double.
;
; Note:
;	The ITB_PTE register has the PFN starting at bit 13, so the
;	fetched pte must be manipulated before being written.
;	Also, we want to keep the main flow in a single cache line
;	(4 fetch blocks) to be as efficient as possible.
;
; NOTE: 
;	1-to-1 mapping scheme implemented with bit in p_misc shadow
; 	register. And we can probably stick it in bit 63 and just branch
;	on negative. Right now we shift and test. Critical path has the
;	cycle count.
;
; Register use:
;	p4	shadow register reserved for itb/dtb miss
;	p5	shadow register reserved for double miss
;	p6	shadow register reserved for itb/dtb miss
;	p7	shadow register reserved for dtb miss
;	p_misc	physical mode bit
;	p23	exc_addr of instruction causing ITB miss
;
; Exit state:
;	On exit to trap__invalid_ipte or trap__foe
;	p4	PTE
;	p23	pc of instruction causing ITB miss
;-
	START_HW_VECTOR <ITB_MISS>

ASSUME OSF_P_MISC__PHYS__S eq 63

	hw_mfpr	p4, EV6__IVA_FORM		; (0L) get vpte address
	hw_mfpr	p23, EV6__EXC_ADDR		; (0L) get exception address 
	lda	p6, ^x0FFF(r31)			; (xU) create mask for prot
	bis	r31, r31, r31			; (xU) fill out fetch block

trap__itb_miss_vpte:
	hw_ldq/v p4, (p4)			; (xL) get vpte
	and	p4, p6, p5			; (xL) get prot bits
	blt	p_misc, trap__i1to1		; (xU) 1-to-1 => branch
	srl	p4, #OSF_PTE__PFN__S, p6	; (xU) shift PFN to <0>

	sll	p6, #EV6__ITB_PTE__PFN__S, p6	; (xU) shift PFN into place
	and	p4, #<1@OSF_PTE__FOE__S>, p7	; (xL) get FOE bit
	blbc	p4, trap__invalid_ipte		; (xU) invalid => branch
	bne	p7, trap__foe			; (xU) FOE => branch

	srl	p4, #7, p7			; check for mb bit
	bis	p5, p6, p6			; (xL) PTE in ITB format

.if eq force_path2				; 1.44 force_path2 = 0

	ALIGN_FETCH_BLOCK <^x47FF041F>		; Edit 1.36

	PVC_VIOLATE <2>				; ignore scoreboard violation
	hw_mtpr	p23, EV6__ITB_TAG		; (6,0L) write tag
	hw_mtpr	p6, <EV6__ITB_PTE ! ^x40>	; (0&4&6,0L) write PTE

ASSUME <tb_mb_en + pte_eco> ne 2

  .if ne pte_eco
	blbc    p7, trap__itb_miss_mb		; branch for mb
	hw_ret_stall (p23)			; return

trap__itb_miss_mb:
	mb
	hw_ret_stall (p23)			; return
  .iff
	hw_ret_stall (p23)			; return
  .endc						; pte_eco

.iff						; 1.44 force_path2 = 1
;
; We need to avoid the situation where a ldx_l has acquired a lock,
; another processor has taken it, and a bad path before a stx_c has
; a load which pulls the data back in and makes it look like the
; lock has succeeded.
;
; Hold up pte write, which holds up loads. Since hw_ret_stall is in
; same fetch block (it would normally scoreboard stall off bit 0),
; it fires off right away and will mispredict before any loads
; can happen.
;
	mb					; allow hw_ret to fire
	mulq	p6, #1, p6			; hold up loads

	ALIGN_FETCH_BLOCK <^x47FF041F>		; Edit 1.36

	PVC_VIOLATE <2>				; ignore scoreboard violation
	mulq	p6, #1, p6			; hold up loads
	hw_mtpr	p23, EV6__ITB_TAG		; (6,0L) write tag
	hw_mtpr	p6, <EV6__ITB_PTE ! ^x40>	; (0&4&6,0L) write PTE
	hw_ret_stall (p23)			; mis-predict before any loads

.endc						; 1.44 force_path2

	CONT_HW_VECTOR				; 1.41 continue in free space

;+
; Do a 1-1 mapping
; Current state
;	p23	exc_addr
;-
trap__i1to1:
	srl	p23, #13, p6			; shift out the byte offset
	lda	p5, ^x0301(r31)			; all read enable
	sll	p6, #EV6__ITB_PTE__PFN__S, p6	; get pfn into position
	bis	p6, p5, p6			; produce the pte

	ALIGN_FETCH_BLOCK <^x47FF041F>		; Edit 1.36

	PVC_VIOLATE <2>				; ignore scoreboard violation
	hw_mtpr	p23, EV6__ITB_TAG		; (6,0L) write tag
	hw_mtpr	p6, <EV6__ITB_PTE ! ^x40>	; (0&4&6,0L) write PTE

    .if eq force_path				; 1.41
	hw_ret_stall (p23)			; return
    .iff
	ALIGN_FETCH_BLOCK <^x47FF041F>
	PVC_VIOLATE <1007>
	PVC_VIOLATE <1020>			; stop permutation
	hw_jmp_stall (p23)			; return with jmp
	br	r31, .-4			; stop predictor
    .endc					; 1.41


;+
; trap__foe - Fault On Execute Istream PTE fetched by ITB miss routine.
;
; Entry:
;	On branch from trap__itb_miss flow.
;
; Function:
;	At this point, we know FOE is set, and TNV isn't.
;	But we haven't checked for ACV. Assume FOE and merge
;	with invalid Istream PTE code.
;
; Current state:
;	p4	PTE
;	p23	faulting va/pc
;
; Exit state:
;	On exit to trap__invalid_ipte_merge
;	p4	PTE
;	p5	OSF_MMCSR__FOE
;	p23	pc/va of instruction causing ITB miss
;-
trap__foe:
	lda	p5, OSF_MMCSR__FOE(r31)		; assume FOE
	br	r31, trap__invalid_ipte_merge	; Merge with invalid flow

;+
; trap__invalid_ipte - Invalid Istream PTE fetched by ITB miss routine.
;
; Entry:
;	On branch from trap__itb_miss flow.
;
; Function:
;	At this point, we know we know we have TNV, but we haven't checked
;	for ACV, which takes precedence. We also have a merge from the
;	trap__foe flow, which was entered on FOE and no TNV. Take
;	a trap.
;
; Current state:
;	p4		PTE
;	p23		faulting va/pc
;
; When merging from trap_foe:
;	p5		OSF_MMCSR__FOE
;-

trap__invalid_ipte:
	lda	p5, OSF_MMCSR__TNV(r31)			; assume TNV

trap__invalid_ipte_merge:				; merge from FOE flow
	and	p_misc, #<1@OSF_P_MISC__CM__S>, p7	; current mode
	and	p_misc, #OSF_P_MISC__PS__M, p20		; save original ps
	beq	p7, trap__invalid_ipte_stack		; skip switch if kernel
;
; Switch to kernel mode.
; Current state:
;	p4		PTE
;	p5		marks TNV or FOE
;	p20		original ps
;
ipte_cm_offset = <trap__invalid_ipte_stack - trap__invalid_ipte_cm>

	hw_stq/p r30, PT__USP(p_temp)				; save user SP
	hw_ldq/p r30, PT__KSP(p_temp)				; get kernel SP
	srl	p4, #<OSF_PTE__URE__S - OSF_PTE__KRE__S>, p4	; shift u to k
	bic	p_misc, #OSF_P_MISC__PS__M, p_misc		; cm=0, ipl=0

	br	p7, trap__invalid_ipte_cm		; change mode to kernel
trap__invalid_ipte_cm:
	addq	p7, #<ipte_cm_offset+1>, p7		; jump past in palmode
	hw_mtpr	r31, EV6__PS				; (4,0L) switch to kern
	bsr	r31, .					; push prediction stack
	PVC_JSR	ipte_cm					; synch up
	hw_ret_stall (p7)				; pop prediction stack
	PVC_JSR	ipte_cm, dest=1
;
; Take the trap. Be careful of tb miss!
; Current state:
;	p4		PTE. If user, now shifted right to kernel position
;	p5		TNV or FOE
;	p20		original ps
;	p23		pc of instruction taking itb miss
;	p_misc__ps	if we were in kernel, original ps
;			if we were in user, new ps = 0 (cm=0, ipl=0)
;

trap__invalid_ipte_stack:
	hw_stq/p p23, PT__STACK_PC(p_temp)		; store away pc

	srl	p4, #OSF_PTE__KRE__S, p4		; get <re> to <0>
	cmovlbc p4, #OSF_MMCSR__ACV, p5			; acv over the others
	hw_stq/p p5, PT__NEW_A1(p_temp)			; save mmcsr

	lda	r30, <0-OSF_FRM__SIZE>(r30)		; allocate stack space

	stq	p20, OSF_FRM__PS(r30)			; save original ps
	stq	r18, OSF_FRM__A2(r30)			; save a2

	hw_ldq/p p20, PT__STACK_PC(p_temp)		; 1.39 get nextpc back

	stq	r29, OSF_FRM__GP(r30)			; save gp
	stq	r16, OSF_FRM__A0(r30)			; save a0
	stq	r17, OSF_FRM__A1(r30)			; save a1
	stq	p20, OSF_FRM__PC(r30)			; 1.39 save nextpc

	bis	p20, r31, r16				; 1.39 a0 <- pc/va
	subq	r31, #1, r18				; a2 <- istream (-1)
	hw_ldq/p p23, PT__ENT_MM(p_temp)		; get entry point
	hw_ldq/p r29, PT__KGP(p_temp)			; get kgp
	hw_ldq/p r17, PT__NEW_A1(p_temp)		; a1 <- MMCSR

	hw_ret	(p23)					; to os

	END_HW_VECTOR

;+
; ARITH - offset 600
;
; Entry:
;	Vectored into via hardware trap on an arithmetic exception
;	synchronous trap.
;
; Function:
;	For floating exceptions, we may just update the FPCR (see below)
;	and return to the user.
;
;	Otherwise build stack frame.
;		r16 (a0)	Exception summary
;		r17 (a1)	Register mask
;		r18 (a2)	unpredictable
;	Vector via entArith.
;
; Notes on FPCR register:
;	The FPCR is both a status and control register. The status reflect
;	problems encountered with the instruction. The 'disable' control bits
;	and instruction modifiers have an effect on trapping to the os.
;
;	If a trap disable bit is set and an instruction with the /S
;	qualifier set generates the associated trapping result, EV6
;	produces the IEEE non-trapping result and suppresses the trap.
;
; 	On EV6, PALcode assistance is required to assure correction operation
;	of the status bits. When the status *changes* from what is in the FPCR,
;	an arithmetic exception is taken, even if there is to be no trap. The
;	status changes are marked in EXC_SUM SET_condition bits. The PALcode
;	uses these bits to update the FPCR. The EXC_SUM condition bits are
;	examined to determine whether a trap should be delivered.
;
;		trap bit	set_x bit	actions
;		--------	---------	-------
;		0		1		no trap, set FPCR status bit
;		1		0		trap, don't set FPCR bit
;		1		1		trap, set FPCR status bit
;
;
; Notes on EXC_SUM register:
;		int - bit 7	iov - bit 6
;		-----------	-----------
;		0		1		fbox int overflow
;		1		1		ebox int overflow
;-

	START_HW_VECTOR <ARITH>

	hw_mfpr	p7, EV6__EXC_SUM			; (0L)
	hw_mfpr	p23, EV6__EXC_ADDR			; (0L) exception addr

.if ne ev6_p1
trap__arith_merge:					; merge for fp emulation
.endc

	and	p7, #<1@EV6__EXC_SUM__INT__S>, p4	; check for int ovr
	srl	p7, #EV6__EXC_SUM__REG__S, p5		; shift register field
	and	p5, #EV6__EXC_SUM__REG__M, p5		; isolate register

	blbs	p23, trap__pal_exc_bugcheck		; bugcheck on pal

	zap	p7, #^xC0, p7				; clean <63:48>
	addq	p23, #4, p23				; adjust to nextpc
	bne	p4, trap__arith_int			; branch on int case
	addq	p5, #32, p5				; set up for fx mask

trap__arith_int:
	srl	p7, #EV6__EXC_SUM__SET_INV__S, p4	; check for FPCR update
	and	p7, #^x7F, p7				; isolate <6:0>
	beq	p4, trap__arith_post			; just post

	CONT_HW_VECTOR <ARITH>

;
; Update FPCR. Then check to see if we post.
;
.if eq ev6_p1

	ftoit	f0, p6				; save f0 (thanks to ftoit!)
	mf_fpcr	f0				; get current FPCR
	sll	p4, #EV6__FPCR__INV__S, p4	; shift status into place
	ftoit	f0, p20				; convert FPCR to integer
	bis	p20, p4, p20			; or in new status bits
	itoft	p20, f0				; turn back into float

arith_fpcr_offset = <trap__arith_finish - trap__arith_fpcr>

	br	p4, trap__arith_fpcr
trap__arith_fpcr:
	addq	p4, #<arith_fpcr_offset+1>, p4	; set up to jump past in palmode
	mt_fpcr	f0				; write FPCR
	bsr	r31, .				; push prediction stack
	PVC_JSR arith_fpcr			; synch up
	hw_ret_stall (p4)			; pop prediction stack
	PVC_JSR arith_fpcr, dest=1

trap__arith_finish:
	itoft	p6, f0				; restore f0
	bic	p7, #1, p6			; check <6:1>
	bne	p6, trap__arith_post		; post if indicated

.endc

	hw_ret	(p23)				; back to user

;
; Current state:
;	p5	register number (adjusted for float)
;	p7	exc_sum<6:0>
;	p20	1
;	p23	next pc	
;
trap__arith_post:
	lda	p20, 1(r31)				; get a 1 for mask
	sll	p20, p5, p5				; register number to mask

	and	p_misc, #<1@OSF_P_MISC__CM__S>, p4	; current mode
	and	p_misc, #OSF_P_MISC__PS__M, p20		; save original ps
	beq	p4, trap__arith_stack			; skip switch if kernel
;
; Switch to kernel mode. 
;
arith_cm_offset = <trap__arith_stack - trap__arith_cm>

	hw_stq/p r30, PT__USP(p_temp)			; save user SP
	hw_ldq/p r30, PT__KSP(p_temp)			; get kernel SP
	bic	p_misc, #OSF_P_MISC__PS__M, p_misc	; cm=0, ipl=0

	br	p6, trap__arith_cm			; change mode to kernel
trap__arith_cm:
	addq	p6, #<arith_cm_offset+1>, p6		; jump past in palmode
	hw_mtpr	r31, EV6__PS				; (4,0L) switch to kern
	bsr	r31, .					; push prediction stack
	PVC_JSR	arith_cm				; synch up
	hw_ret_stall (p6)				; pop prediction stack
	PVC_JSR	arith_cm, dest=1
;
; Take the trap. Be careful of tb miss!
; Current state:
;	p5	register mask
;	p7	exc_sum<6:0>
;	p_misc__ps	if we were in kernel, original ps
;			if we were in user, new ps = 0 (cm=0, ipl=0)
;	p20		original ps
;	p23	next pc	
;
trap__arith_stack:
	hw_stq/p p23, PT__STACK_PC(p_temp)		; store away nextpc
	hw_stq/p p7, PT__NEW_A0(p_temp)			; save away exc sum
	hw_stq/p p5, PT__NEW_A1(p_temp)			; save away reg mask

	lda	r30, <0-OSF_FRM__SIZE>(r30)		; allocate stack space

	stq	p20, OSF_FRM__PS(r30)			; save original ps
	stq	r18, OSF_FRM__A2(r30)			; save a2

	hw_ldq/p p20, PT__STACK_PC(p_temp)		; 1.39 get nextpc back

	stq	r29, OSF_FRM__GP(r30)			; save gp
	stq	r16, OSF_FRM__A0(r30)			; save a0
	stq	r17, OSF_FRM__A1(r30)			; save a1
	stq	p20, OSF_FRM__PC(r30)			; 1.39 save pc

	hw_ldq/p p23, PT__ENT_ARITH(p_temp)		; get entry point
	hw_ldq/p r29, PT__KGP(p_temp)			; get kgp
	hw_ldq/p r16, PT__NEW_A0(p_temp)		; a0 <- exc sum
	hw_ldq/p r17, PT__NEW_A1(p_temp)		; a1 <- reg mask

	hw_ret	(p23)					; to os

	END_HW_VECTOR

;+
; INTERRUPT - offset 680
;
; Entry:
;	Vectored into via hardware trap on interrupt.
;
; Function:
;
; Note: If a new interrupt (hardware, serial line, crd or perf counter)
;	occurs simultaneously with an mfpr isum, the isum read will return 0's.
;	The interrupt would not be lost however. After a passive release in
;	the PALcode, we would see the interrupt again. One way to
;	minimize the number of passive releases is to read isum twice
;	and OR the results.
;-

ipl_offset = <IPL_TABLE - trap__pal_base>

	START_HW_VECTOR <INTERRUPT>

	hw_mfpr	p23, EV6__EXC_ADDR		; (0L) get exc_addr
	hw_mfpr	p4, EV6__ISUM			; (0L) get interrupt summary
	bis	r31, r31, r31
	bis	r31, r31, r31

	hw_mfpr	p5, EV6__ISUM			; (0L) read again
	bis	p4, p5, p4			; OR the results

	srl	p4, #EV6__ISUM__CR__S, p5	; check for cre (31)
	srl	p4, #EV6__ISUM__PC__S, p6	; check for perf ctr (29)
	and	p6, #EV6__ISUM__PC__M, p6	; get just perf ctr bits

	blbs	p5, sys__crd			; branch if cre
	bne	p6, sys__interrupt_pc		; branch if perf ctr

	srl	p4, #EV6__ISUM__EI__S, p7	; check for external int (20-23)
	and	p7, #EV6__ISUM__EI__M, p7	; get just ei bits

	beq	p7, trap__interrupt_sl		; check sl if not ei
	br	r31, sys__interrupt_ei		; handle ei

	CONT_HW_VECTOR
;+
; Check for serial line interrupt.
;-
trap__interrupt_sl:
	srl	p4, #EV6__ISUM__SL__S, p7	; check for sl
	beq	p7, trap__interrupt_dismiss	; dismiss if not
	br	r31, sys__interrupt_sl		; handle sl

;+
; Interrupts not being handled yet. ???
;-
trap__interrupt_passive:

;+
; Dismiss interrupt.
;-
trap__interrupt_dismiss:

.if eq force_path2				; 1.44 force_path2 = 0

	hw_ret_stall (p23)			; return to user

.iff						; 1.44 force_path2 = 1
;
; We need to avoid the situation where a ldx_l has acquired a lock,
; another processor has taken it, and a bad path before a stx_c has
; a load which pulls the data back in and makes it look like the
; lock has succeeded. Hold up loads by writing to MM_STAT with
; scoreboard bit 2 (and 6 -- 6 is required or we hang).
;
	mb					; make sure hw_ret goes

	ALIGN_FETCH_BLOCK <^x47FF041F>
	mulq	p6, #1, p6			; hold up loads
	mulq	p6, #1, p6			; hols up loads
	hw_mtpr p6, <EV6__MM_STAT ! ^x44>	; hold up loads
	hw_ret_stall (p23)			; return

.endc						; 1.44 force_path2

	END_HW_VECTOR

;+
; MT_FPCR - offset 700
;
; Entry:
;	Vectored into via hardware trap on issue of a MT_FPCR instruction.
;	This is a sychronous trap.
;
; Function:
;	The MT_FPCR instruction is issued from the floating point queue.
;	This instruction is implemented as an explicit IPR write: the value
;	is written into the "first" latch, and when the instruction retires,
;	the value is written into the "second" latch. There is no IPR
;	scoreboarding in the floating point queue. PALcode assistance is
;	required to ensure that subsequent readers of the FPCR get the
;	update value.
;
;	The PALcode can simply return using a HW_RET_STALL. This sequence
;	ensures that the MT_FPCR instruction will be correctly ordered with
;	respect to subsequent readers of the FPCR.
;
; Register use:
;	p23		exc_addr
;
;-
	START_HW_VECTOR <MT_FPCR>

	hw_mfpr	p23, EV6__EXC_ADDR		; (0L) save exc_addr
	NOP					; keep 1st fetch block free
	NOP					; 	of pvc violations
	NOP

	addq	p23, #4, p23			; adjust pc
	hw_ret_stall (p23)			; return to user

	END_HW_VECTOR

;+
; Wakeup
;
; Entry:
;	Vectored into via hardware trap on wakeup from sleep mode.
;
; Current state:
; 	Register file		unitialized
;	EV6__EXC_ADDR		undefined
;	EV6__PAL_BASE		last written
;	EV6__IER		last written
;	Iprs			reset state
;	Cbox write-once chain	last written
;	Cbox write-many chain	reset state
;	EV6__SIRR<28:14>	cpu number
;
;-

;+
; Initialize 80 retirator "done" status bits and mapper.
; The retirator and mapper must be initialized in the
; manner and order below. NO SOURCES other than
; x31 may be used until "mapper source enable" is
; turned on with a EV6_ITB_IA.
;-

EV6__WAKEUP_ENTRY = ^x780

	START_HW_VECTOR <WAKEUP>

.if eq reference_platform

	PVC_VIOLATE <1003>
.iff

    i = 0
    .repeat 15
	addq	r31, r31, i
	addq	r31, r31, <i+1>
	.if eq <ev6_p1>
	addt	f31, f31, i
	mult	f31, f31, <i+1>
	.iff
	addq	r31, r31, i
	addq	r31, r31, <i+1>
	.endc
	i = i+2
    .endr

	addq	r31, r31, r30
    .if eq <ev6_p1>
	addt	f31, f31, f30
    .iff
	addq	r31, r31, r30
    .endc
	addq	r31, r31, r0
	addq	r31, r31, r0

    .repeat 4
	addq	r31, r31, r0
	addq	r31, r31, r0
	addq	r31, r31, r0
	addq	r31, r31, r0
    .endr

;
; Now turn on mapper source enables.
;
	ASSUME_FETCH_BLOCK

	hw_mtpr	r31, EV6__ITB_IA	; (4,0L) Flush ITB, enable source map
	hw_mtpr	r31, EV6__DTB_IA	; (7,1L) Flush DTB
	addq	r31, r31, r0		; nop
	addq	r31, r31, r0		; nop
;
; Create a stall outside the IQ until the mtpr EV6__ITB_IA retires.
;
	hw_mtpr	r31, <EV6__MM_STAT ! ^x90>
	addq	r31, r31, r0		; nop
	addq	r31, r31, r0		; nop
	addq	r31, r31, r0		; nop
;
; We need 2 buffer fetch blocks to produce desired stalls..
;
	addq	r31, r31, r0		; 1st buffer fetch block. IMAP stall.
	addq	r31, r31, r0		; nop
	addq	r31, r31, r0		; nop
	addq	r31, r31, r0		; nop

	addq	r31, r31, r0		; 2nd buffer fetch block. FMAP stall.
	addq	r31, r31, r0		; nop
	addq	r31, r31, r0		; nop
	addq	r31, r31, r0		; nop

;+
; Now we must map the shadow registers.
;
; Current state:
;	retirator initialized
;	destinations mapped
;	source mapping enabled
;	itb, dtb flushed
;
; We need to ensure we are in the icache so we don't fetch junk that touches
; the shadow sources before we write the destinations.
;-

EV6__I_CTL__SHADOW_INIT = -
	<<3@EV6__I_CTL__IC_EN__S> ! -
	 <2@EV6__I_CTL__SDE__S> ! -
	 <1@EV6__I_CTL__CALL_PAL_R23__S>>

	GET_32CONS	r2, EV6__I_CTL__SHADOW_INIT, r31
	br	r31, trap__wakeup__touch0	; fetch in next block

	ALIGN_FETCH_BLOCK <^x43FF0400>		; pad with addq r31, r31, r0

trap__wakeup__next0:				;
	hw_mtpr	r2, EV6__I_CTL			; (4,0L) write I_CTL
	addq	r31, r31, r0			; nop
	br	r31, trap__wakeup__next1	; continue in next block
trap__wakeup__touch0:				;
	br	r31, trap__wakeup__touch1	; fetch next block

trap__wakeup__next1:				;
	hw_mtpr	r31, <EV6__MM_STAT ! ^x10>	; (4,0L) stall outside IQ
	addq	r31, r31, r0			; nop
	br	r31, trap__wakeup__next2	; continue in next block
trap__wakeup__touch1:				;
	br	r31, trap__wakeup__touch2	; fetch next block
;
; We need 3 buffer fetch blocks to get the correct SDE bit for the
; next fetch block. One for IMAP stall, one for FMAP stall, one more to allow
; for the retire to proprogate for the fetch.
;
trap__wakeup__next2:				;
	addq	r31, r31, r0			; 1st block. IMAP stall.
	addq	r31, r31, r0			; nop
	br	r31, trap__wakeup__next3	; continue in next block
trap__wakeup__touch2:				;
	br	r31, trap__wakeup__touch3	; fetch next block

trap__wakeup__next3:				;
	addq	r31, r31, r0			; 2nd block. FMAP stall.
	addq	r31, r31, r0			; nop
	br	r31, trap__wakeup__next4	; continue in next block
trap__wakeup__touch3:				;
	br	r31, trap__wakeup__touch4	; fetch next block

trap__wakeup__next4:				;
	addq	r31, r31, r0			; 3rd block. Propagate.
	addq	r31, r31, r0			; nop
	br	r31, trap__wakeup__next5	; continue in next block
trap__wakeup__touch4:				;
	br	r31, trap__wakeup__touch5	; fetch next block
;
; Now map the shadow registers
;
trap__wakeup__next5:				;
	addq	r31, r31, r4			; map r4
	addq	r31, r31, r5			; mapr r5
	br	r31, trap__wakeup__next6	; continue in next block
trap__wakeup__touch5:				;
	br	r31, trap__wakeup__touch6	; fetch next block

trap__wakeup__next6:				;
	addq	r31, r31, r6			; map r6
	addq	r31, r31, r7			; map r7
	br	r31, trap__wakeup__next7	; continue in next block
trap__wakeup__touch6:				;
	br	r31, trap__wakeup__touch7	; fetch next block

trap__wakeup__next7:				;
	addq	r31, r31, r20			; map r20
	addq	r31, r31, r21			; map r21
	br	r31, trap__wakeup__next8	; continue in next block
trap__wakeup__touch7:				;
	br	r31, trap__wakeup__touch8	; fetch next block

trap__wakeup__next8:				;
	addq	r31, r31, r22			; map r22
	addq	r31, r31, r23			; map r23
	br	r31, trap__wakeup__sde_done	; done
trap__wakeup__touch8:				;
	br	r31, trap__wakeup__next0	; go back and start executing

trap__wakeup__sde_done:
	br	r31, sys__wakeup		; continue in system code

.endc

	END_HW_VECTOR <^xC00 - TRAP__WAKEUP>

;+
; Other routines
;-
	GOTO_FREE_CODE

;+
; trap__update_pcb_and_halt
;
; Entry:
;	Branched to on halt conditions.
;
; Function:
;	Update PCB and branch to sys__enter_console.
;
; Current state:
;	p20		halt code
;	p23		offending pc
;
;	PT__HALT_CODE	halt code
;-
	ALIGN_CACHE_BLOCK
trap__update_pcb_and_halt:
	hw_ldq/p p4, PT__PCBB(p_temp)			; get pcbb
	and	p_misc, #<1@OSF_P_MISC__CM__S>, p7	; current mode
	bne	p7, trap__update_pcb_from_user		; branch if user mode
;
; Fall through for kernel mode.
;
	hw_stq/p r30, OSF_PCB__KSP(p4)			; update PCB location
	br	r31, trap__update_pcb_and_halt_cpc
;
; Updating from user mode.
;
trap__update_pcb_from_user:
	hw_stq/p r30, PT__USP(p_temp)		; update pt__usp
	hw_stq/p r30, OSF_PCB__USP(p4)		; update PCB location
;
; Merge to update cpc.
;
trap__update_pcb_and_halt_cpc:
	rpcc	p5				; get cycle counter
	srl	p5, #32, p6			; shift offset in cc<63:32>
	addl	p5, p6, p6			; cc<31:0> + cc<63:32>
	hw_stl/p p6, OSF_PCB__CPC(p4)		; store cc

	br	r31, sys__enter_console


;+
; pal__save_state
;
; The shadow registers, except of course p_misc and p_temp, are pretty much
; scratch, so we don't worry about stepping on them.
;
; Current state:
;	p7		return address
;	p20		halt code
;	p23		exc_addr
;
;	PT__HALT_CODE	halt code (do we need it there ??)
;-

EV6__I_CTL__SDE7__S = 7				; shadow mode bit

	ALIGN_FETCH_BLOCK

pal__save_state:
	hw_ldq/p p4, PT__IMPURE(p_temp)		; get base of impure area
	hw_stq/p r31, CNS__FLAG(p4)		; clear dump flag
	hw_stq/p p20, CNS__HALT(p4)		; halt code
	bis	r31, r31, r31

	STORE_REG 0, irn=p4			; save r0
	STORE_REG 1, irn=p4			; save r1
	STORE_REG 2, irn=p4			; save r2
	hw_mfpr	r0, EV6__I_CTL			; (4,0L) get i_ctl

	bis	p4, r31, r1			; base of impure area
	bic	r0, #<1@EV6__I_CTL__SDE7__S>, r0; zap sde
	STORE_REG 3, irn=p4			; save r3
	STORE_REG 8, irn=r1			; save gpr
	
	hw_mtpr	r0, EV6__I_CTL			; (4,0L) write i_ctl
	STORE_REG 9, irn=r1			; save gpr
	STORE_REG 10, irn=r1			; save gpr
	STORE_REG 11, irn=r1			; save gpr

	hw_mtpr	r0, EV6__I_CTL			; (4,0L) stall outside IQ
	STORE_REG 12, irn=r1			; save gpr
	STORE_REG 13, irn=r1			; save gpr
	STORE_REG 14, irn=r1			; save gpr

	STORE_REG 15, irn=r1			; buffer block 1 -- save gpr
	STORE_REG 16, irn=r1			; save gpr
	STORE_REG 17, irn=r1			; save gpr
	STORE_REG 18, irn=r1			; save gpr

	STORE_REG 19, irn=r1			; buffer block 2 --save gpr
	STORE_REG 24, irn=r1			; save gpr
	STORE_REG 25, irn=r1			; save gpr
	STORE_REG 26, irn=r1			; save gpr

	STORE_REG 27, irn=r1			; buffer block 3 --save gpr
	STORE_REG 28, irn=r1			; save gpr
	STORE_REG 29, irn=r1			; save gpr
	STORE_REG 30, irn=r1			; save gpr

	STORE_REG 4, irn=r1			; now store the un-shadowed gprs
	STORE_REG 5, irn=r1			; save gpr
	STORE_REG 6, irn=r1			; save gpr
	STORE_REG 7, irn=r1			; save gpr

	STORE_REG 20, irn=r1			; save gpr
	STORE_REG 21, irn=r1			; save gpr
	STORE_REG 22, irn=r1			; save gpr
	STORE_REG 23, irn=r1			; save gpr
;
; Now turn shadow registers back on.
;
	bis	r0, #<1@EV6__I_CTL__SDE7__S>, r0; or in sde
	hw_mtpr	r0, EV6__I_CTL			; (4,0L) write i_ctl
	bis	r31, r31, r31
	bis	r31, r31, r31

	hw_mtpr	r0, EV6__I_CTL			; (4,0L) stall outside IQ
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 1
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 2
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 3
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31
;
; Now save pal shadows. Registers p_temp and p_misc are really the
; only important ones.
;
; Current state:
;	p4	base of impure area
;
pal__save_shadow:
	hw_stq/p p4, CNS__P4(p4)
	hw_stq/p p5, CNS__P5(p4)
	hw_stq/p p6, CNS__P6(p4)
	hw_stq/p p7, CNS__P7(p4)
	hw_stq/p p20, CNS__P20(p4)
	hw_stq/p p_temp, CNS__P_TEMP(p4)
	hw_stq/p p_misc, CNS__P_MISC(p4)
	hw_stq/p p23, CNS__P23(p4)

.if eq ev6_p1				; no float in pass1

;
; Now save the floating point registers and FPCR.
; First make sure FEN is on.
;
	GET_16CONS	r0, <1@EV6__PROCESS_CONTEXT__FPE__S>, r31
	hw_mtpr	r0, EV6__FPE			; (4,0L) write new fpe

	ALIGN_FETCH_BLOCK <^x47FF041F>		; align with nops

	hw_mtpr	r0, EV6__FPE			; (4,0L) force retire
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 1
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 2
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 3
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	t = 0
	.repeat 32
	STORE_REG \t, srn=p5, irn=p4, fpu=1
	t=t+1
	.endr

	mf_fpcr	f0				; get current FPCR
	ftoit	f0, p5				; convert to integer
	hw_stq/p p5, CNS__FPCR(p4)		; save

.endc

;
; Now save the important PALtemps
;
	STORE_REG <IMPURE>, srn=p5, irn=p4, pal=1
	STORE_REG <WHAMI>, srn=p5, irn=p4, pal=1
	STORE_REG <SCC>, srn=p5, irn=p4, pal=1
	STORE_REG <PRBR>, srn=p5, irn=p4, pal=1
	STORE_REG <PTBR>, srn=p5, irn=p4, pal=1
	STORE_REG <TRAP>, srn=p5, irn=p4, pal=1
	STORE_REG <HALT_CODE>, srn=p5, irn=p4, pal=1	; Do I need this?
	STORE_REG <KSP>, srn=p5, irn=p4, pal=1
	STORE_REG <SCBB>, srn=p5, irn=p4, pal=1
	STORE_REG <PCBB>, srn=p5, irn=p4, pal=1
	STORE_REG <VPTB>, srn=p5, irn=p4, pal=1
	STORE_REG <M_CTL>, srn=p5, irn=p4, pal=1
;
; Now save the IPRs that are restorable with some informational sandwiched in.
;
	hw_ldq/p p5, PT__VA_CTL(p_temp)			; control part
	hw_ldq/p p6, PT__VPTB(p_temp)			; vtpb part
	bis	p5, p6, p5				; combine
	hw_stq/p p5, CNS__VA_CTL(p4)

	hw_stq/p p23, CNS__EXC_ADDR(p4)

	STORE_REG <IER_CM>, srn=p5, irn=p4, ipr=1		; (4,0L)
	STORE_REG <I_STAT>, srn=p5, irn=p4, ipr=1		; (0L) info only
	STORE_REG <SIRR>, srn=p5, irn=p4, ipr=1			; (4,0L)
	STORE_REG <MM_STAT>, srn=p5, irn=p4, ipr=1		; (0L) info only
	STORE_REG <PAL_BASE>, srn=p5, irn=p4, ipr=1		; (4,0L)
	STORE_REG <DTB_ALT_MODE>, srn=p5, irn=p4, pal=1		; (6,0L)
;	STORE_REG <I_CTL>, srn=p5, irn=p4, ipr=1		; (4,0L)
; isp bug workaround
;
	hw_mfpr	p5, EV6__I_CTL
	GET_32CONS p6, <^x804000>, r31
	bic	p5, p6, p5			; clear unpredicatables
	hw_stq/p p5, CNS__I_CTL(p4)
; End of hack
	NOP
	NOP
	NOP					; don't save PCTR_CTL
	NOP
	NOP
	STORE_REG <PROCESS_CONTEXT>, srn=p5, irn=p4, ipr=1	; (4,0L)
;
; Now save the rest of the informational IPRs. For now, we don't bother
; with the CBOX chain. ??
;
	STORE_REG <DC_STAT>, srn=p5, irn=p4, ipr=1	; (6,0L)
	STORE_REG <VA>, srn=p5, irn=p4, ipr=1		; (4-7,1L)
	STORE_REG <ISUM>, srn=p5, irn=p4, ipr=1		; (0L)
	hw_stq/p 31, CNS__EXC_SUM(p4)			; just zero exc_sum
							; since not useful
;
; For some reason, previous implementations have computed the size of the
; impure area taken up by a mchkflag quadword, the pal_temps, shadows, and
; iprs. That size is written to the mchkflag quadword. I will do that
; for now in case the console wants that information. Everything is quad,
; so I'm not bothering with rounding.
;
	GET_16CONS	r0, <CNS__SIZE>, r31
	GET_16CONS	r1, <CNS__MCHKFLAG>, r31
	subq	r0, r1, r0			; size - mchkflag location
	hw_stq/p r0, CNS__MCHKFLAG(p4)		; save the computation
;
; Now write the dump flag. 
;
	bis	r31, #1, r0
	hw_stq/p r0, CNS__FLAG(p4)		; set dump area flag
;
; Now return to caller
;
	bis	p7, #1, p7			; return in pal mode
	PVC_JSR save_state, bsr=1, dest=1
	hw_ret_stall (p7)			; stall for pvc

;+
; pal__restore_state
;
; The shadow registers, except of course p_misc and p_temp, are pretty much
; scratch, so we don't worry about stepping on them.
;
; Current state:
;	Shadow mode on
;
;	p7		return address
;
;	p_temp		valid
;	PT__IMPURE	valid
;	PT__WHAMI	valid
;
; The assumptions here are that shadow mode is on, that p_temp is valid,
; and that PT__IMPURE and PT__WHAMI have not been written over.
; If these assumptions are not valid (i.e, if we are switching between other
; than our VMS and UNIX PALcodes, the exit_console platform-dependent code
; will have to figure out how to set these up.
;
; Exit state:
;	p23		exc_addr
;-

pal__restore_state:
	hw_ldq/p	r1, PT__IMPURE(p_temp)	; get base of impure area

.if eq ev6_p1

;
; Restore floating point registers. First make sure FEN is on.
;
	GET_16CONS	r0, <1@EV6__PROCESS_CONTEXT__FPE__S>, r31
	hw_mtpr	r0, EV6__FPE			; (4,0L) write new fpe

	ALIGN_FETCH_BLOCK <^x47FF041F>		; align with nops

	hw_mtpr	r0, EV6__FPE			; (4,0L) force retire
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 1
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 2
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 3
	bis	r0, r0, r0			; for pvc #7
	bis	r0, r0, r0
	bis	r0, r0, r0

	hw_ldq/p r0, CNS__FPCR(r1)		; get FPCR
	itoft	r0, f0				; convert to float

restore_fpcr_offset = < pal__restore_fpcr_done - pal__restore_fpcr>

	br	r2, pal__restore_fpcr
pal__restore_fpcr:
	addq	r2, #<restore_fpcr_offset+1>, r2	; past stall in palmode
	mt_fpcr	f0					; restore FPCR
	bsr	r31, .					; push prediction stack
	PVC_JSR restore_fpcr
	hw_ret_stall (r2)				; pop prediction stack
	PVC_JSR restore_fpcr, dest=1
pal__restore_fpcr_done:

	t=0
	.repeat 31
	RESTORE_REG \t, srn=r0, irn=r1, fpu=1
	t=t+1
	.endr

.endc

;
; Now restore the important PALtemps.
; We assume the following did not change or were restored by exit_console
;	p_temp
;	PT__IMPURE
;	PT__WHAMI
;
	RESTORE_REG <SCC>, srn=r0, irn=r1, pal=1
	RESTORE_REG <PRBR>, srn=r0, irn=r1, pal=1
	RESTORE_REG <PTBR>, srn=r0, irn=r1, pal=1
	RESTORE_REG <KSP>, srn=r0, irn=r1, pal=1
	RESTORE_REG <SCBB>, srn=r0, irn=r1, pal=1
	RESTORE_REG <PCBB>, srn=r0, irn=r1, pal=1
	RESTORE_REG <VPTB>, srn=r0, irn=r1, pal=1

.if ne kseg_hack					; kseg hack
	hw_stq/p r31, PT__M_CTL(p_temp)
.iff
	RESTORE_REG <M_CTL>, srn=r0, irn=r1, pal=1
.endc

;
; Now restore the IPRs.
;
; Don't do I_CTL until we are ready to turn shadow registers off.
;
; Don't restore DC_CTL. The code has to assume this register has not
; changed. It wouldn't do to fool with the set enables.
;
; Restore the VPTB portion of VA_CTL from CNS__VPTB, the generic spot
; the firmware wants to use when it's modifying VPTB. Also restore the
; control part to PT__VA_CTL.
;
	hw_ldq/p r0, CNS__VA_CTL(r1)			; get va_ctl
	hw_ldq/p r2, CNS__VPTB(r1)			; get vptb
	sll	r0, #<64-EV6__VA_CTL__RSV1__S>, r0	; clean control part
	srl	r0, #<64-EV6__VA_CTL__RSV1__S>, r0
	hw_stq/p r0, PT__VA_CTL(p_temp)			; store control part
	srl	r2, #<EV6__VA_CTL__VPTB__S>, r2		; clean vptb part
	sll	r2, #<EV6__VA_CTL__VPTB__S>, r2
	bis	r0, r2, r0				; combine
	hw_mtpr	r0, EV6__VA_CTL				; (5,1L)

	RESTORE_REG <IER_CM>, srn=r0, irn=r1, ipr=1		; (4,0L)
	RESTORE_REG <DTB_ALT_MODE>, srn=r0, irn=r1, ipr=1	; (6,0L)
	RESTORE_REG <SIRR>, srn=r0, irn=r1, ipr=1		; (4,0L)

.if ne kseg_hack						; kseg hack
	hw_stq/p r31, CNS__M_CTL(r1)
	hw_stq/p r31, PT__M_CTL(p_temp)	
	hw_mtpr	r31, EV6__M_CTL
.iff
	RESTORE_REG <M_CTL>, srn=r0, irn=r1, ipr=1		; (6,0L)
.endc

	RESTORE_REG <PAL_BASE>, srn=r0, irn=r1, ipr=1		; (4,0L)
	NOP
	NOP
	NOP				; don't restore PCTR_CTL
	NOP
	NOP
	RESTORE_REG <PROCESS_CONTEXT>, srn=r0, irn=r1, ipr=1	; (4,0L)

.if ne ev6_p1
	srl	r0, #EV6__FPE__FPE__S, r2		; shift into position
	and	r2, #1, r2				; clean
	hw_stq/p r2, CNS__FPE_STATE(r1)			; restore 'fpe' state
.endc							; 	from pctx

;
; Current state:
;	r0		process context
;
; Grab ASN and write it to DTB_ASNx
;
	srl	r0, #EV6__ASN__ASN__S, r0	; shift down
	and	r0, #EV6__ASN__ASN__M, r0	; clean it
	sll	r0, #EV6__DTB_ASN0__ASN__S,r0 	; ASN into mbox spot
;
; There must be a scoreboard bit -> register dependency chain to prevent
; hw_mtpr DTB_ASx from issuing while ANY of scoreboard bits <7:4> are set.
;
	hw_mfpr	r2, <EV6__PAL_BASE ! ^xF0>	; (4-7,0L)
	xor	r2, r2, r2			; zap r2
	bis	r2, r0, r0			; force register dependency
	NOP					; force fetch block

	hw_mtpr	r0, EV6__DTB_ASN0		; (4,0L)
	hw_mtpr	r0, EV6__DTB_ASN1		; (7,1L)
;
; Now restore shadows as needed.
; Analysis:
;	p4		not necessary
;	p5		not necessary
;	p6		not necessary
;	p7		return address. Do not touch.
;	p20		not necessary
;	p_temp		valid. Do not touch.
;	p_misc		restore
;	p23		restore from CNS__EXC_ADDR (or CNS__P23)
;
	hw_ldq/p p_misc, CNS__P_MISC(r1)
	hw_ldq/p p23, CNS__EXC_ADDR(r1)
;
; Now restore integer registers. Restore I_CTL with
; sde clear. After restoring integer registers, rewrite I_CTL
; with sde set. Assumption: PALcode is running with sde set!!
;
; We need to write I_CTL twice in case we are toggling the VA_48 bit
; in order to stall the pipe during the change.
;
; Restore the VPTB portion of the I_CTL from CNS__VPTB, the generic spot
; the firmware wants to use when it's modifying VPTB.
;
	hw_ldq/p r0, CNS__I_CTL(r1)
	hw_ldq/p r2, CNS__VPTB(r1)			; get vptb
	sll	r0, #<64-EV6__I_CTL__CHIP_ID__S>, r0	; clean control part
	srl	r0, #<64-EV6__I_CTL__CHIP_ID__S>, r0
	srl	r2, #<EV6__I_CTL__VPTB__S>, r2		; clean vptb part
	sll	r2, #<EV6__I_CTL__VPTB__S>, r2

	ALIGN_FETCH_BLOCK <^x47FF041F>

	bis	r0, r2, r0			; combine
	bic	r0, #<1@EV6__I_CTL__SDE7__S>, r0; zap sde
	RESTORE_REG 8, irn=r1
	RESTORE_REG 9, irn=r1

	hw_mtpr	r0, EV6__I_CTL			; (4,0L) write i_ctl
	RESTORE_REG 10, irn=r1
	RESTORE_REG 11, irn=r1
	RESTORE_REG 12, irn=r1

	hw_mtpr	r0, EV6__I_CTL			; (4,0L) stall outside IQ
	RESTORE_REG 13, irn=r1
	RESTORE_REG 14, irn=r1
	RESTORE_REG 15, irn=r1

	RESTORE_REG 16, irn=r1			; buffer block 1
	RESTORE_REG 17, irn=r1
	RESTORE_REG 18, irn=r1
	RESTORE_REG 19, irn=r1

	RESTORE_REG 24, irn=r1			; buffer block 2
	RESTORE_REG 25, irn=r1
	RESTORE_REG 26, irn=r1
	RESTORE_REG 27, irn=r1

	RESTORE_REG 28, irn=r1			; buffer block 3
	RESTORE_REG 29, irn=r1
	RESTORE_REG 30, irn=r1
	bis	r31, r31, r31

	RESTORE_REG 4, irn=r1
	RESTORE_REG 5, irn=r1
	RESTORE_REG 6, irn=r1
	RESTORE_REG 7, irn=r1

	RESTORE_REG 20, irn=r1
	RESTORE_REG 21, irn=r1
	RESTORE_REG 22, irn=r1
	RESTORE_REG 23, irn=r1
;
; Now turn shadow registers back on.
;
	bis	r0, #<1@EV6__I_CTL__SDE7__S>, r0; or in sde
	hw_mtpr	r0, EV6__I_CTL			; (4,0L) write i_ctl
	bis 	r31, r31, r31
	bis 	r31, r31, r31

	hw_mtpr	r0, EV6__I_CTL			; (4,0L) stall outside IQ
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 1
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 2
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 3
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31
;
; Now restore r0-r3.
;
	bis	r1, r31, p4			; base of impure area

	RESTORE_REG 0, irn=p4			; restore r0
	RESTORE_REG 1, irn=p4			; restore r1
	RESTORE_REG 2, irn=p4			; restore r2
	RESTORE_REG 3, irn=p4			; restore r3
;
; Clear dump flag and return.
;
	hw_stq/p r31, CNS__FLAG(p4)		; clear dump flag

	bis	p7, #1, p7			; return in pal mode
	PVC_JSR restore_state, bsr=1, dest=1
	hw_ret_stall (p7)			; add stall for pvc

	END_FREE_CODE

	.=^xC00

INITIAL_PCBB:
	.repeat	8
	    .quad 0
	.endr

	ASSUME <.> le <^x0C88> 
;+
; Beginning of CALL_PAL instructions.
;-
;
; HACK ALERT!!!!!!!!!! ??? Bug in HAL!!!!!!! ???
; If this code contains a branch, HAL will later re-compute the branch and
; overwrite the code that was supposed to overwrite this code.
;
; This has been fixed, I think, but leave as is for now.
;
; We should probably also add check for pal mode. (??)
;-

opcdec_offset = <<trap__opcdec_call_pal - trap__pal_base>>

	. = ^x2000
CALL_PAL__START:
    .if eq add_extra_ret			; 1.47 add_extra_ret
	.repeat 128
	hw_mfpr	p6, EV6__PAL_BASE		; (4,0L) need pal base
	ldah	p6,<<opcdec_offset>+32768>@-16(p6)
	lda	p6,<<opcdec_offset> & ^xFFFF>(p6)
	bis	p6, #1, p6			; pal mode
	bsr	r31, .				; push prediction stack
	PVC_VIOLATE <1007>
	hw_ret	(p6)				; pop prediction stack
						; post
	.align 6
	.endr
    .iff					; 1.47 add_extra_ret
;
; Just delete the bsr to push the stack, and we're all set
;
	.repeat 128
	hw_mfpr	p6, EV6__PAL_BASE		; (4,0L) need pal base
	ldah	p6,<<opcdec_offset>+32768>@-16(p6)
	lda	p6,<<opcdec_offset> & ^xFFFF>(p6)
	bis	p6, #1, p6			; pal mode
	PVC_VIOLATE <1007>
	hw_ret	(p6)				; pop prediction stack
						; post
	.align 6
	.endr
    .endc					; 1.47 add_extra_ret

	GOTO_FREE_CODE

;+
; CALL_PAL__HALT -- PALcode for halt instruction
;
; Entry:
;	p23	pc of instruction following the call_pal instruction
;
; Function:
;	Set PT__HALT_CODE to HALT__SW_HALT. Decrement pc in p23,
;	and branch off to save PCB and enter console.
;
; Exit state:
;	PT__HALT_CODE	HALT__SW_HALT
;	p23		pc of halt instruction
;-
	START_CALL_PAL <HALT>

	subq	p23, #4, p23			; adjust pc
	lda	p20, HALT__SW_HALT(r31)		; halt code
	hw_stq/p p20, PT__HALT_CODE(p_temp)	; store (??)
	br	r31, trap__update_pcb_and_halt

	END_CALL_PAL


;+
; CALL_PAL__CFLUSH
;
; Entry:
;	r16(a0)	page frame number (PFN) of page to be flushed
;	p23	pc of instruction following call_pal instruction
;
; Function:
;	Flush all dstream caches of 1 entire page.
;
; Note on implementation:
;	Since the dcache is two-way set associative, we need to
;	do two passes with different tags.
;-
	START_CALL_PAL <CFLUSH>

	br	r31, sys__cflush		; handle in system module

	END_CALL_PAL


;+
; CALL_PAL__DRAINA
;
; Entry:
;	p23	pc of instruction following the call_pal instruction
;
; Function:
;	Stall instruction issuing until all prior instruction are
;	guaranteed to complete without incurring aborts.
;
; Note:
;	This call_pal is not all that useful, since it could incur
;	a machine_check_while_in_pal, which is not a very useful
;	state to get in to. However, as long as the machine check
;	flow does not write into p23, we could conceivably, in the
;	machine check flow, determine we were in a draina, and do a
;	recoverable machine check.
;
; Do an MB to drain all the queues, and a hw_ret_stall to synchronize
; retires.
;-
	START_CALL_PAL <DRAINA>

	mb
	NOP					; no hw_ret in 1st fetchblock
	NOP
	NOP

	hw_ret_stall (p23)			; return with stall

	END_CALL_PAL

;+
; CALL_PAL__CSERVE
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	Do an implementation specific console service.
;
; Exit state:
;	Exit to sys__cserve
;
;-
	START_CALL_PAL <CSERVE>

	br	r31, sys__cserve		; handle in system code

	END_CALL_PAL

;+
; CALL_PAL__SWPPAL
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	Swap control to another PALcode.
;
; Register use:
;	r16(a0)	new PALcode identifier
;			0 	=> new PALcode at address 0
;			2 	=> OSFPAL
;			>255	=> new PALcode at address in r16
;	r17(a1)	new PC
;	r18(a2)	new PCBB
;	r19	new VPTB
;
; Exit state:
;	r0(v0)	0	success (PALcode was switched)
;		1	unknown PALcode variant
;		2	know PALcode variant, but not loaded
;-
	START_CALL_PAL <SWPPAL>

	cmpule	r16, #255, r0			; see if a variant
	cmoveq	r16, r16, r0			; r16 = 0 is a valid address
	bis	r16, r31, p5			; will jump through p5
	blbc	r0, call_pal__swppal_addr	; try as an address

	cmpeq	r16, #2, r0			; see if our buddy OSFPAL
	blbc	r0, call_pal__swppal_fail	; nope, don't know this fellow

.iif eq <.&4>, NOP
	br	p4, call_pal__swppal_osf
	.quad	PAL__ENTER_OSF			; address must fit in a quadword
call_pal__swppal_osf:
	hw_ldq/p p4, 0(p4)			; fetch target address pointer
	hw_ldq/p p5, 0(p4)			; fetch target address

	ble	p5, call_pal__swppal_fail	; if not linked, say not loaded
	lda	p4, ^x7FFF(r31)			; checker for pal base
	and	p5, p4, p4			; get low 15 bits

	CONT_CALL_PAL <SWPPAL>

	cmpeq	p4, #0, r0			; check for non-zero bits
	blbc	r0, call_pal__swppal_fail	; if not clear, say unknown
;
; Current state:
;	p5	address
;
	bis	r31, #1, p4			; get a '1'
	sll	p4, #OSF_P_MISC__SWITCH__S,p4	; switch bit into position
	bis	p_misc, p4, p_misc		; mark 'switch'

	bis	r31, r31, r0			; status to success
	bis	p5, #1, p5			; set pal mode bit
	bsr	r31, .				; push prediction stack
	PVC_VIOLATE <1007>			; go to it
	hw_ret	(p5)				; pop prediction stack
;
; Looks like an address, either 0 or > 255. Check low bits.
; Current state:
;	p5	address
;
call_pal__swppal_addr:
	lda	p4, ^x7FFF(r31)			; checker for pal base
	and	p5, p4, p4			; get low 15 bits
	cmpeq	p4, #0, r0			; check for non-zero bits
	blbc	r0, call_pal__swppal_fail	; if not clear, say unknown
	br	r31, call_pal__swppal_cont	; address ok, so continue
;
; We have failed.
; Current state:
;	r0	0	unknown variant or bad address
;		1	osfpal, but not loaded
; Exit state:
;	r0	1	unknown variant or bad address
;		2	osfpal, but not loaded
;
call_pal__swppal_fail:
	
	addq	r0, #1, r0			; affect r0
	hw_ret	(p23)				; return

	END_CALL_PAL


;+
; CALL_PAL__WRIPIR
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	IPIR <- r16(a0)
;	Handled in system-specific code.
;-
	START_CALL_PAL <WRIPIR>

	br	r31, sys__wripir

	END_CALL_PAL


;+
; CALL_PAL__RDMCES
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	Return in r0(v0) the MCES IPR.
;	r0(v0) <- ZEXT(MCES)
;
; Exit state:
;	r0(v0)		MCES
;-
	START_CALL_PAL <RDMCES>

ASSUME OSF_P_MISC__MCES__MCHK__S eq 16

	extbl	p_misc, #2, r0			; get mces from p_misc
	NOP					; no ret in 1st fetch block
	NOP
	NOP

	hw_ret	(p23)				; return

	END_CALL_PAL

;+
; CALL_PAL__WRMCES
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;	r16(a0)		mces information
;
; Function:
;	if (r16<0> eq 1) then MCES<0> <- 0	mchk
;	if (r16<1> eq 1) then MCES<1> <- 0	system correctable -- sce
;	if (r16<2> eq 1) then MCES<2> <- 0	processor correctable -- pce
;	MCES<3> <- r16<3>			set <- disable pce -- dpc
;	MCES<4> <- r16<4>			set <- disable sce -- dsc
;
;-
	START_CALL_PAL <WRMCES>

ASSUME OSF_P_MISC__MCES__MCHK__S eq 16

mces_clear = <<1@MCES__MCHK__S> ! <1@MCES__SCE__S> ! <1@MCES__PCE__S>>
mces_dis = <<1@MCES__DPC__S> ! <1@MCES__DSC__S>>

ASSUME mces_clear eq <^x7>
ASSUME mces_dis le <^x18>

	extbl	p_misc, #2, p4		; get mces from p_misc

	and	r16, #mces_clear, p6	; get mchk, sce, pce
	ornot	r31, p6, p6		; flip mchk, sce, pce bits
	and	r16, #mces_dis, p7	; get disable bits

	and	p4, p6, p4		; update mchk, sce, pce
	bic	p4, #mces_dis, p4	; clear old dpc, dsc
	or	p4, p7, p4		; update dpc, dsc

	zap	p_misc, #4, p_misc			; clear out old mces
	sll	p4, #OSF_P_MISC__MCES__MCHK__S, p4	; move into position
	bis	p_misc, p4, p_misc			; or new mces in

	hw_ret	(p23)			; return to user

	END_CALL_PAL

;+
; CALL_PAL__WRFEN
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;	r16(a0)<0>	new fen bit
;
; Function:
;	FEN 	   	<- R16<0>
;	(PCB+40)<0>	<- FEN
;	r16(a0)		unpredictable
;-
	START_CALL_PAL <WRFEN>

	hw_ldq/p p4, PT__PCBB(p_temp)		; get PCBB
	and	r16, #1, r16			; clean new fen
	sll	r16, #EV6__FPE__FPE__S, p7	; shift into position
	hw_mtpr	p7, EV6__FPE			; (4,0L) write new fpe

	hw_mtpr p7, EV6__FPE			; (4,0L) force retire
	hw_stl/p r16, OSF_PCB__FEN(p4)		; store FEN in PCB
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 1
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 2
	bis	r31, r31, r31
	bis	r31, r31, r31
	CONT_CALL_PAL <WRFEN>

	bis	r0, r0, r0			; buffer block 3
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

.if ne ev6_p1
	hw_ldq/p p5, PT__IMPURE(p_temp)		; get impure pointer
	hw_stq/p r16, CNS__FPE_STATE(p5)	; save 'fpe' state
.endc

	hw_ret_stall (p23)			; stall for pvc

	END_CALL_PAL


;+
; CALL_PAL__WRVPTPTR
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;	r16(a0)		new VPTPTR
;
; Function:
;	VPTPTR <- r16
;	r16(a0)		unpredictable
;
; Note: We clean <29:0> before storing.
;-
	START_CALL_PAL <WRVPTPTR>

ASSUME EV6__I_CTL__VPTB__S eq EV6__VA_CTL__VPTB__S
;
; 1.48 Reorder code to force retire of va_ctl before hw_ret_stall.
;

	hw_mfpr	p6, EV6__I_CTL			; (4,0L) get i_ctl
	hw_ldq/p p7, PT__VA_CTL(p_temp)		; get control part of va_ctl

	bis	p7, r16, p7			; new va_ctl
	hw_stq/p r16, PT__VPTB(p_temp)		; new vptb
	hw_mtpr	p7, EV6__VA_CTL			; (5,0L)

	srl	r16, #EV6__I_CTL__VPTB__S, r16	; shift new vptb to clean
	sll	r16, #EV6__I_CTL__VPTB__S, r16	; shift back into position

	sll	p6, #<64 - EV6__I_CTL__VPTB__S>, p6	; clean
	srl	p6, #<64 - EV6__I_CTL__VPTB__S>, p6	; move back
	bis	p6, r16, p6				; or new vptb
	hw_mtpr	p6, <EV6__I_CTL ! ^x20>		; 1.48 (4&5,0L) write i_ctl

	ALIGN_FETCH_BLOCK <^x47FF041F>		; 1.48
	hw_ret_stall (p23)			; return with stall

	END_CALL_PAL

;+
; CALL_PAL__SWPCTX
;
; Entry:
;	p23	pc of instruction following the call_pal instruction
;	r16(a0)	address of new PCB
;
; Function:
;	The swap process context instruction saves the current process
;	data in the current PCB. Then swpctx switches to the PCB passed in
;	a0 and loads the new process context. The old PCBB is returned in v0.
;
;-
	START_CALL_PAL <SWPCTX>

	hw_ldq/p r0, PT__PCBB(p_temp)		; get old PCBB

	rpcc	p4				; get cycle counter
	srl	p4, #32, p6			; shift offset
	addl	p4, p6, p6			; merge to compute cpc

	hw_ldq/p p7, PT__USP(p_temp)		; get usp
	hw_stq/p r30, OSF_PCB__KSP(r0)		; store ksp
	hw_stq/p p7, OSF_PCB__USP(r0)		; store usp
	hw_stl/p p6, OSF_PCB__CPC(r0)		; store cpc

	hw_ldq/p p6, OSF_PCB__CPC(r16)		; get new cpc/asn
	hw_ldq/p p5, OSF_PCB__FEN(r16)		; get new fen/pme
	hw_stq/p r16, PT__PCBB(p_temp)		; set new PCBB
;
; Compute new cc offset
;
	subl	p6, p4, p4			; generate new cc offset
	sll	p4, #32, p4			; move into position
	hw_mtpr	p4, EV6__CC			; (5,1L) write it

	CONT_CALL_PAL <SWPCTX>

	hw_mtpr	p4, EV6__CC			; 1.48 (5,1L) force retire
;
; Now write ASN to DTB_ASNx.
;
; Current state:
;	p5	fen/pme quadword
;	p6	cpc/asn quadword
;
ASSUME OSF_PCB__ASN__M eq ^xFF

	srl	p6, #OSF_PCB__ASN__S, p6	; shift down ASN
	and	p6, #OSF_PCB__ASN__M, p6	; clean ASN
	sll	p6, #EV6__DTB_ASN0__ASN__S, p6	; ASN into mbox spot
;
; There must be a scoreboard bit -> register dependency chain to prevent
; hw_mtpr DTB_ASx from issuing while ANY of scoreboard bits <7:4> are set.
;
	hw_mfpr	p7, <EV6__PAL_BASE ! ^xF0>	; (4-7,0L)
	xor	p7, p7, p7			; zap p7
	bis	p7, p6, p6			; force register dependency
	NOP					; force fetch block

	hw_mtpr	p6, EV6__DTB_ASN0		; (4,0L)
	hw_mtpr	p6, EV6__DTB_ASN1		; (7,1L)
;
; Create Ibox Process Context IPR, filling in ASN, FPE, PPCE
;
; Current state:
;	p5	fen/pme quadword
;	p6	asn in DTB_ASNx position
;
ASSUME EV6__DTB_ASN0__ASN__S eq 56
ASSUME EV6__ASN__ASN__S eq 39
ASSUME EV6__FPE__FPE__S eq 2
ASSUME EV6__PPCE__PPCE__S eq 1
asn_shift = <EV6__DTB_ASN0__ASN__S - EV6__ASN__ASN__S>
fpe_shift = <EV6__FPE__FPE__S - OSF_PCB__FEN__S>
ppce_shift = <OSF_PCB__PME__S - EV6__PPCE__PPCE__S>

	srl	p6, #<asn_shift>, p6		; ASN back into ibox position
	sll	p5, #fpe_shift, p4		; get FEN into position
	and	p4, #<1@EV6__FPE__FPE__S>, p4	; clean it
	bis	p6, p4, p6			; or in FEN enable

.if ne ev6_p1
	hw_ldq/p p7, PT__IMPURE(p_temp)		; get impure pointer
	and	p5, #1, p4			; clean PCB FEN bit
	hw_stq/p p4, CNS__FPE_STATE(p7)		; write FPE_STATE
.endc

	ALIGN_FETCH_BLOCK <^x47FF041F>

	srl	p5, #ppce_shift, p4		; get pme into position
	and	p4, #<1@EV6__PPCE__PPCE__S>, p4	; clean it
	bis	p6, p4, p6			; or in ppce bit
	hw_mtpr	p6, EV6__PROCESS_CONTEXT	; (4,0L) write it
;
; Write new PTBR, get new USP and SP, save USP.
;
	hw_mtpr	p6, EV6__PROCESS_CONTEXT	; (4,0L) force retire
	hw_ldq/p p4, OSF_PCB__PTBR(r16)		; get new PTBR
	hw_ldq/p p5, OSF_PCB__USP(r16)		; get new USP
	hw_ldq/p r30, OSF_PCB__KSP(r16)		; get new KSP

	sll	p4, #page_offset_size_bits, p4	; convert PFN
	hw_stq/p p4, PT__PTBR(p_temp)		; store new PTBR
	hw_stq/p p5, PT__USP(p_temp)		; save USP
	bis	r31, r31, r31			; buffer block 1

	bis	r0, r0, r0			; buffer block 2
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 3
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	hw_ret_stall (p23)			; return

	END_CALL_PAL

;+
; CALL_PAL__WRVAL
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;	r16(a0)		new sysvalue
;
; Function:
;	sysvalue <- r16
;	r16(a0)		unpredictable
;
;-
	START_CALL_PAL <WRVAL>

	hw_stq/p r16, PT__SYSVAL(p_temp)	; write sysvalue
	NOP					; no hw_ret in 1st fetch block
	NOP
	NOP

	hw_ret	(p23)				; return to user

	END_CALL_PAL

;+
; CALL_PAL__RDVAL
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	r0(v0) <- sysvalue
;
;-
	START_CALL_PAL <RDVAL>

	hw_ldq/p r0, PT__SYSVAL(p_temp)		; read sysvalue
	NOP					; no hw_ret in 1st fetch block
	NOP
	NOP

	hw_ret	(p23)				; return to user

	END_CALL_PAL


;+
; CALL_PAL__TBI
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	case r16(a0) begin	
;		 1:	! tbisi
;			{invalidate ITB entry for va = r17(a1)}
;			break;
;		 2:	! tbisd
;			{invalidate DTB entry for va = r17(a1)}
;			break;
;		 3:	! tbis
;			{invalidate both TB entries for va = r17(a1)}
;			break;
;		-1:	! tbiap
;			{invalidate all TB entries with ASM=0}
;			break;
;		-2:	! tbia
;			{flush all TBs}
;			break;
;		other:
;			break;
;	endcase;
;
; Exit state:
;	r16(a0)		unpredictable
;	r17(a1)		unpredictable
;-
	START_CALL_PAL <TBI>

TBI_FUNCTION_POWER = 6					; 64 bytes (16 inst)

	addq	r16, #2, r16				; create positive range
	br	p4, call_pal__tbi_addr

call_pal__tbi_addr:
	cmpult	r16, #6, p5				; see if in range
	lda	p4, <tbi_tbl - call_pal__tbi_addr>(p4)	; base of table
	sll	r16, #TBI_FUNCTION_POWER, r16		; function power
	blbc	p5, call_pal__tbi_out			; branch if not in range

	addq	p4, r16, p4				; point to function
	bis	p4, #1, p4				; make pal mode
	bsr	r31, .					; push prediction stack
	PVC_JSR	tbi
	hw_ret	(p4)					; push prediction stack
							; go do it
call_pal__tbi_out:
	hw_ret	(p23)					; out of range

	CONT_CALL_PAL
;
; tbi_tbl
;
; Table to do tbi instructions.
;
; ?? Should we do this differently, as the slot requirements are so
; different??
;
	ALIGN_CACHE_BLOCK
tbi_tbl:
	; -2 tbia
	PVC_JSR tbi, dest=1
	hw_mtpr r31, EV6__ITB_IA		; (4,0L)
	hw_mtpr	r31, EV6__DTB_IA		; (7,0L)
	NOP
	NOP
	hw_mtpr	r31, EV6__IC_FLUSH		; (4,0L) flush icache
	bne	r31, .				; pvc #24
	hw_ret_stall (p23)			; return with stall

	; -1 tbiap
	.ALIGN TBI_FUNCTION_POWER
	PVC_JSR tbi, dest=1
	hw_mtpr r31, EV6__ITB_IAP		; (4,0L)
	hw_mtpr r31, EV6__DTB_IAP		; (7,0L)
	NOP
	NOP
	hw_mtpr	r31, EV6__IC_FLUSH_ASM		; (4,0L) flush icache
	bne	r31, .				; pvc #24
	hw_ret_stall (p23)			; return with stall

	; 0 unused
	.ALIGN TBI_FUNCTION_POWER
	PVC_JSR tbi, dest=1
	hw_ret (p23)				; return

	; 1 tbisi
	.ALIGN TBI_FUNCTION_POWER
	PVC_JSR tbi, dest=1
	hw_mtpr	r17, EV6__ITB_IS		; (4&6,0L)
	hw_ret_stall (p23)			; return with stall

	; 2 tbisd
	.ALIGN TBI_FUNCTION_POWER
	PVC_JSR tbi, dest=1
;
; There must be a scoreboard bit -> register dependency chain to prevent
; hw_mtpr DTB_ISx from issuing while ANY of scoreboard bits <7:4> are set.
;
	hw_mfpr	p6, <EV6__PAL_BASE ! ^xF0>	; (4-7,0L)
	xor	p6, p6, p6			; zap p6
	bis	p6, r17, r17			; force register dependency
	NOP					; force fetch block

	hw_mtpr	r17, EV6__DTB_IS0		; (6,0L)
	hw_mtpr	r17, EV6__DTB_IS1		; (7,1L)
	NOP					; force fetch block
	NOP					; are these necessary?

	hw_ret_stall (p23)			; return with stall

	; 3 tbis
	.ALIGN TBI_FUNCTION_POWER
	PVC_JSR tbi, dest=1
;
; There must be a scoreboard bit -> register dependency chain to prevent
; hw_mtpr DTB_ISx from issuing while ANY of scoreboard bits <7:4> are set.
;
	hw_mfpr	p6, <EV6__PAL_BASE ! ^xF0>	; (4-7,0L)
	xor	p6, p6, p6			; zap p6
	bis	p6, r17, r17			; force register dependency
	NOP					; force fetch block

	hw_mtpr	r17, EV6__DTB_IS0		; (6,0L)
	hw_mtpr	r17, EV6__DTB_IS1		; (7,1L)
	NOP					; force fetch block
	NOP

	hw_mtpr	r17, EV6__ITB_IS		; (4&6,0L)
	hw_ret_stall (p23)			; return with stall

	END_CALL_PAL

;+
; CALL_PAL__WRENT
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	case r17(a1) begin
;		0:	entInt	<- r16(a0); break;
;		1:	entArith<- r16(a0); break;
;		2:	entMM	<- r16(a0); break;
;		3:	entIF	<- r16(a0); break;
;		4:	entUna	<- r16(a0); break;
;		5:	entSys	<- r16(a0); break;
;	    other:	break;
;	endcase;
;
; Exit state:
;	r16(a0)		unpredictable
;	r17(a1)		unpredictable
;-
	START_CALL_PAL <WRENT>

	br	p4, call_pal__wrent_addr
call_pal__wrent_addr:
	cmpult	r17, #6, p5				; see if in range
	lda	p4, <wrent_tbl-call_pal__wrent_addr>(p4); base of table
	sll	r17, #3, r17				; * 8
	blbc	p5, call_pal__wrent_out			; branch if not in range

	addq	p4, r17, p4				; point to function
	bic	r16, #3, r16				; clean pc
	bis	p4, #1, p4				; make pal mode
	bsr	r31, .					; push prediction stack
	PVC_JSR	wrent				
	hw_ret	(p4)					; pop prediction stack
							; go do it
call_pal__wrent_out:
	hw_ret	(p23)					; out of range

	CONT_CALL_PAL
;
; wrent table
;
	ALIGN_CACHE_BLOCK
wrent_tbl:
	; 0 entInt
	PVC_JSR wrent, dest=1
	hw_stq/p r16, PT__ENT_INT(p_temp)		; write it
	hw_ret	(p23)					; return

	; 1 entArith
	PVC_JSR wrent, dest=1
	hw_stq/p r16, PT__ENT_ARITH(p_temp)		; write it
	hw_ret	(p23)					; return

	; 2 entMM
	PVC_JSR wrent, dest=1
	hw_stq/p r16, PT__ENT_MM(p_temp)		; write it
	hw_ret	(p23)					; return

	; 3 entIF
	PVC_JSR wrent, dest=1
	hw_stq/p r16, PT__ENT_IF(p_temp)		; write it
	hw_ret	(p23)					; return

	; 4 entUna
	PVC_JSR wrent, dest=1
	hw_stq/p r16, PT__ENT_UNA(p_temp)		; write it
	hw_ret	(p23)					; return

	; 5 entSys
	PVC_JSR wrent, dest=1
	hw_stq/p r16, PT__ENT_SYS(p_temp)		; write it
	hw_ret	(p23)					; return

	END_CALL_PAL

;+
; CALL_PAL__SWPIPL
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;	r16(a0)<2:0>	new ipl
;
; Function:
;	Return the current interrupt priority level in r0(v0), and set the
;	interrupt priority level to the value in r16. The new ier bits
;	are taken from an ipl-indexed table of quadwords.
;
; Exit state:
;	r0(v0)		PS<IPL>
;-
	START_CALL_PAL <SWPIPL>

ASSUME OSF_P_MISC__IPL__S eq 0

	hw_mfpr	p4, EV6__PAL_BASE		; (4,0L) get pal base

	and	r16, #7, r16			; clean ipl
	s8addq	r16, p4, p4			; pal base + index
	lda	p4, ipl_offset(p4)		; pal base + table base + index
	hw_ldq/p p4, (p4)			; get new ier

	and	p_misc, #<OSF_P_MISC__IPL__M>, r0	; old ipl to r0
	bic	p_misc, #<OSF_P_MISC__IPL__M>, p_misc	; clear out old ipl
	or	p_misc, r16, p_misc			; put new ipl in p_misc

	hw_mtpr	p4, EV6__IER			; (4,0L) write new ier

	hw_ret_stall (p23)			; return with stall

	END_CALL_PAL


;+
; CALL_PAL__RDPS
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	r0(v0) <- PS
;
; Exit state:
;	r0(v0)		PS
;-
ASSUME OSF_P_MISC__PS__S eq 0

	START_CALL_PAL <RDPS>

	and	p_misc, #OSF_P_MISC__PS__M, r0	; just get PS
	NOP					; no hw_ret in 1st fetch block
	NOP
	NOP

	hw_ret	(p23)				; return

	END_CALL_PAL

;+
; CALL_PAL__WRKGP
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	KGP <- r16(a0)
;-
	START_CALL_PAL <WRKGP>

	hw_stq/p r16, PT__KGP(p_temp)		; write kgp
	NOP					; no hw_ret in 1st fetch block
	NOP
	NOP

	hw_ret	(p23)				; return

	END_CALL_PAL

;+
; CALL_PAL__WRUSP
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	USP <- r16(a0)
;-
	START_CALL_PAL <WRUSP>

	hw_stq/p r16, PT__USP(p_temp)		; write usp
	NOP					; no hw_ret in 1st fetch block
	NOP
	NOP

	hw_ret	(p23)				; return

	END_CALL_PAL

;+
; CALL_PAL__MTPR_PERFMON
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;	r16(a0)		function select
;	r17(a1)		value
;
; Function:
;	Execute indicated performance monitoring function.
;
; Exit state:
;	r0(v0)		Value returned for read
;	r17(a1)		UNPREDICTABLE
;-


;-----------------------------------------------------------------------
;	Arguments and Actions
;-----------------------------------------------------------------------
;	r16(a0) = 0		Disable performance monitoring
;	r17(a1) = bitmask	r17(a1)<0>=1	disable CTR0
;				r17(a1)<1>=1	disable CTR1
;
;	Action: Clear PCTx_EN in I_CTL as directed. If CTRx is already
;		disabled, it stays disabled.
;-----------------------------------------------------------------------
;	r16(a0) = 1		Enable performance monitoring
;	r17(a1) = bitmask	r17(a1)<0>=1	enable CTR0
;				r17(a1)<1>=1	enable CTR1
;
;	Action: Set PCTx_EN in I_CTL as directed. If CTRx is already
;		enabled, it stays enabled.
;-----------------------------------------------------------------------
;	r16(a0) = 2		Mux select
;	r17(a1) = pctr_ctl	r17(a1)<3:0>	SL1 pass 2.3 (see table)
;				r17(a1)<3:2>	SL1 pass 3   (see table)
;				r17(a1)<4>	SL0 (see table below)
;	Action: Write PCTR_CTL SLx fields.
;-----------------------------------------------------------------------
;	r16(a0) = 3		Options
;	r17(a1) = options	r17(a1)<0>	logging option
;					0 => log all processes
;					1 => log only selected processes
;	Action: If r17(a1)<0> is clear, set SPCE bit in I_CTL
;-----------------------------------------------------------------------
;	r16(a0) = 5		Read counters
;
;	Action: Return in r0(v0) the counters of PCTR_CTL
;-----------------------------------------------------------------------
;	r16(a0) = 6		Write counters
;	r17(a1)			r17(a1)<0> Write counter 0
;				r17(a1)<1> Write counter 1
;				r17(a1)<25:6>  new pctr1 value
;				r17(a1)<47:28> new pctr0 value
;	Action: Selectively write counter fields in PCTR_CTL
;-----------------------------------------------------------------------
;	r16(a0) = 7		Enable and write selected counters
;	r17(a1) = bitmask	r17(a1)<0>=1	enable and clear PCTR0
;				r17(a1)<1>=1	enable and clear PCTR1
;
;	Action: Write specified counter(s) as directed
;		Set PCTx_EN in I_CTL as directed
;		User requests a clear by having 0 in the counter fields
;-----------------------------------------------------------------------
;	r16(a0) = 8		Read i_stat (ProfileMe)
;
;	Action: Return in r0(v0) the i_stat values
;-----------------------------------------------------------------------
;	r16(a0) = 9		Read pmpc (ProfileMe)
;
;	Action: Return in r0(v0) the PC of the last profiled instruction
;-----------------------------------------------------------------------
;-----------------------------------------------------------------------
; Pass 2.3 Aggregate mode
;
; SL0 mux values
;	0	PCTR0 <- cycles
;	1	PCTR0 <- retired instructions
; SL1 mux values
;	0000 	PCTR1 <- cycles
;	0001 	PCTR1 <- retired conditional branches
;	0010 	PCTR1 <- cycles (retired branch mispredicts not implemented)
;	0011 	PCTR1 <- retired dtb single misses * 2 (bats 943)
;	0100 	PCTR1 <- retired dtb double misses
;	0101 	PCTR1 <- retired itb misses
;	0110 	PCTR1 <- retired unaligned traps
;	0111 	PCTR1 <- replay traps
;-----------------------------------------------------------------------
; Pass 3 or higher Aggregate mode
;
; SL0 mux values
;	0	aggregate mode
; SL1 mux values
;	00	PCTR0 <- retired  PCTR1 <- cycles
;	01	PCTR0 <- cycles	  PCTR1 <- undefined
;	10	PCTR0 <- retired  PCTR1 <- bcache misses/long probe latency
;	11	PCTR0 <- cycles	  PCTR1 <- Mbox replay traps
;-----------------------------------------------------------------------
; Pass 3 or higher ProfileMe mode
;
; SL0 mux values
;	1	ProfileMe mode
; SL1 mux values
;	00	PCTR0 <- retired  PCTR1 <- cycles
;	01	PCTR0 <- cycles	  PCTR1 <- cycles of delayed retire ptr advance
;	10	PCTR0 <- retired  PCTR1 <- bcache misses/long probe latency
;	11	PCTR0 <- cycles	  PCTR1 <- Mbox replay traps
;-----------------------------------------------------------------------
;-----------------------------------------------------------------------
; Note that the PPCE (per process counter enable) bit in PCTX, is
; set/cleared on swap context, and does not get changed as a result of
; a perfmon call.
;-----------------------------------------------------------------------

	START_CALL_PAL <WRPERFMON>

ASSUME EV6__I_CTL__PCT0_EN__S eq ^x12
ASSUME EV6__I_CTL__PCT1_EN__S eq ^x13

	cmpeq	r16, #5, r0			; check for read counters
	bne	r0, call_pal__perfmon_rd	; branch if so
	cmpeq	r16, #6, r0			; check for write counters
	bne	r0, call_pal__perfmon_wr	; branch if so
	cmpeq	r16, #8, r0			; check for read i_stat
	bne	r0, call_pal__perfmon_rd_istat	; branch if so
	cmpeq	r16, #9, r0			; check for read pmpc
	bne	r0, call_pal__perfmon_rd_pmpc	; branch if so
	cmpeq	r16, #1, r0			; check for enable
	bne	r0, call_pal__perfmon_en	; branch if so
	cmpeq	r16, #2, r0			; check for mux select
	bne	r0, call_pal__perfmon_mux	; branch if so
	cmpeq	r16, #3, r0			; check for options
	bne	r0, call_pal__perfmon_opt	; branch if so

	CONT_CALL_PAL<WRPERFMON>

	cmpeq	r16, #7, r0			; check for enable and write
	bne	r0, call_pal__perfmon_en_wr	; branch if so
	bne	r16, call_pal__perfmon_unknown	; branch for unknown
	br	r31, call_pal__perfmon_dis	; branch for disable
;
; Disable
;	r17(a1) = bitmask	r17(a1)<0>=1	disable CTR0
;				r17(a1)<1>=1	disable CTR1
;
;	Action: Clear PCTx_EN in I_CTL as directed
;
	ALIGN_CACHE_BLOCK
call_pal__perfmon_dis:
	hw_mfpr	p4, EV6__I_CTL			; (4,0L) get current control
	and	r17, #3, p5			; clean input
	sll	p5, #EV6__I_CTL__PCT0_EN__S, p5	; shift into position
	bic	p4, p5, p4			; clear bits as requested
	hw_mtpr	p4, EV6__I_CTL			; (4,0L)
;
; Make sure that the counters are not near overflow so that we
; avoid interrupts being blocked in anticipation of an overflow interrupt.
; Rather than checking the whole count, just look at the most significant bit.
; Zap low nibble of count if most significant bit of count is set and
; we were asked to disable that counter.
;
	ALIGN_FETCH_BLOCK <^x47FF041F>

	hw_mfpr	p4, EV6__PCTR_CTL			; (4,0L) get PCTR_CTL

	bis	r31, #^xF, p5				; get an ^xF
	srl	p4, #<EV6__PCTR_CTL__PCTR0__S+19>, p6 	; pctr0 msbit to lsbit
	cmovlbc	p6, r31, p5				; zap mask if lsbit=0
	and	r17, #1, p6				; check for disable
	cmoveq	p6, r31, p5				; zap mask if dis=0

	bis	r31, #^xF, p7				; get an ^xF
	srl	p4, #<EV6__PCTR_CTL__PCTR1__S+19>, p6 	; pctr1 msbit to lsbit
	cmovlbc	p6, r31, p7				; zap mask if lsbit=0
	and	r17, #2, p6				; check for disable
	cmoveq	p6, r31, p7				; zap mask if dis=0

	sll	p5, #EV6__PCTR_CTL__PCTR0__S,  p5	; shift mask into place
	sll	p7, #EV6__PCTR_CTL__PCTR1__S,  p7	; shift mask into place
	bis	p5, p7, p7				; or together
	bne	p7, call_pal__perfmon_zap		; zap if non-zero

	hw_mtpr	r31, <EV6__MM_STAT ! ^x10>		; (4,0L) force retire
	hw_ret_stall (p23)				; return with stall

call_pal__perfmon_zap:
	bic	p4, p7, p4				; clear as needed
	hw_mtpr	p4, EV6__PCTR_CTL			; (4,0L) write PCTR_CTL

	ALIGN_FETCH_BLOCK <^x47FF041F>
	hw_mtpr	r31, <EV6__MM_STAT ! ^x10>		; (4,0L) force retire

	hw_ret_stall (p23)				; return with stall
;
; Enable
;	r17(a1) = bitmask	r17(a1)<0>=1	enable CTR0
;				r17(a1)<1>=1	enable CTR1
;
;	Action: Set PCTx_EN in I_CTL as directed. If CTRx is already enabled,
;		it stays enabled.
;
	ALIGN_CACHE_BLOCK
call_pal__perfmon_en:
 	hw_mfpr	p4, EV6__I_CTL			; (4,0L) get current control
	and	r17, #3, p5			; clean input
	sll	p5, #EV6__I_CTL__PCT0_EN__S, p5	; shift into position
	bis	p4, p5, p4			; enable bits as requested
	hw_mtpr	p4, EV6__I_CTL			; (4,0L)

	ALIGN_FETCH_BLOCK <^x47FF041F>
	hw_mtpr	r31, <EV6__MM_STAT ! ^x10>	; (4,0L) force retire

	hw_ret_stall (p23)			; return with stall
;
; Mux select
;	r17(a1) = pctr_ctl	r17(a1)<3:0>	SL1
;				r17(a1)<4>	SL0
;					0 => cycles
;					1 => retired instructions
;	Action: Write PCTR_CTL SLx fields.
;
ASSUME EV6__PCTR_CTL__SL1__S eq 2
ASSUME EV6__PCTR_CTL__SL0__S eq 4

	ALIGN_CACHE_BLOCK
call_pal__perfmon_mux:
	hw_mfpr	p4, EV6__PCTR_CTL		; (4,0L) get current control
	bis	r31, #^x1F, p6			; cleaning mask
	and	r17, #^x1F, p5			; clean input
	bic	p4, p6, p4			; clean mux
	bis	p4, p5, p4			; or in new mux values
	hw_mtpr	p4, EV6__PCTR_CTL		; (4,0L)

	ALIGN_FETCH_BLOCK <^x47FF041F>
	hw_mtpr	r31, <EV6__MM_STAT ! ^x10>	; (4,0L) force retire

	hw_ret_stall (p23)			; return with stall
;
; Options
;	r17(a1) = options	r17(a1)<0>	logging option
;					0 => log all processes
;					1 => log only selected processes
;	Action: If r17(a1)<0> is clear, set SPCE bit in I_CTL
;
	ALIGN_CACHE_BLOCK
call_pal__perfmon_opt:
	hw_mfpr	p4, EV6__I_CTL			; (4,0L) get I_CTL
	bis	r31, #1, p5			; get a 1
	sll	p5, #EV6__I_CTL__SPCE__S, p5	; get into spce position
	bic	p4, p5, p4			; clear spce
	cmovlbs	r17, r31, p5			; if lbs, just selected process
	bis	p4, p5, p4			; or new spce in
	hw_mtpr	p4, EV6__I_CTL			; (4,0L) write I_CTL

	ALIGN_FETCH_BLOCK <^x47FF041F>
	hw_mtpr	r31, <EV6__MM_STAT ! ^x10>	; (4,0L) force retire

	hw_ret_stall (p23)			; return with stall
;
; Read counters
;
	ALIGN_CACHE_BLOCK
call_pal__perfmon_rd:
	hw_mfpr	r0, EV6__PCTR_CTL		; (4,0L) get PCTR_CTL

	hw_ret	(p23)				; just return, no changes
;
; Write counters selectively
;	r17(a1)			r17(a1)<0> Write counter 0
;				r17(a1)<1> Write counter 1
;				r17(a1)<25:6>  new ctr1 value
;				r17(a1)<47:28> new ctr0 value
;	Action: Selectively write counter fields in PCTR_CTL
;
	ALIGN_CACHE_BLOCK
call_pal__perfmon_wr:
	hw_mfpr	p4, EV6__PCTR_CTL		; (4,0L) get PCTR_CTL
	GET_32CONS p5, <^xFFFFF>, r31		; get mask for count field

	blbc	r17, call_pal__perfmon_wr_1	; if clear, don't write 0
	sll	p5, #EV6__PCTR_CTL__PCTR0__S, p6; mask for pctr0
	and	r17, p6, p7			; new pctr0
	bic	p4, p6, p4			; clear old pctr0
	bis	p4, p7, p4			; or in new pctr0

call_pal__perfmon_wr_1:
	srl	r17, #1, p6			; now look at pctr1
	blbc	p6, call_pal__perfmon_wr_ipr	; if clear, go on to write
	sll	p5, #EV6__PCTR_CTL__PCTR1__S, p6; mask for pctr1
	and	r17, p6, p7			; new pctr1
	bic	p4, p6, p4			; clear old pctr1
	bis	p4, p7, p4			; or in new pctr1

call_pal__perfmon_wr_ipr:
;
; To make counter accurate, we no longer zap the lower nibble. But the user
; must be careful with the values! If the counter is near overflow and
; counting is disabled, interrupts can be blocked in anticipation of an
; overflow interrupt.
;
	hw_mtpr	p4, EV6__PCTR_CTL		; (4,0L) write PCTR_CTL

	ALIGN_FETCH_BLOCK <^x47FF041F>
	hw_mtpr	r31, <EV6__MM_STAT ! ^x10>	; (4,0L) force retire

	hw_ret_stall (p23)			; return with stall
;
; Enable and write specified counters
;	r17(a1) = bitmask	r17(a1)<0>=1	enable CTR0
;				r17(a1)<1>=1	enable CTR1
;
;	Action: Write specified counter(s)
;		Set PCTx_EN in I_CTL as directed
;
	ALIGN_CACHE_BLOCK
call_pal__perfmon_en_wr:
 	hw_mfpr	p4, EV6__I_CTL			; (4,0L) get current control
	and	r17, #3, p5			; clean input
	sll	p5, #EV6__I_CTL__PCT0_EN__S, p5	; shift into position
	bis	p4, p5, p4			; enable bits as requested
	hw_mtpr	p4, EV6__I_CTL			; (4,0L)
	br	r31, call_pal__perfmon_wr	; write selected counters
;
; Read I_STAT
;
EV6__I_STAT__PRFME__S = ^x1e	
EV6__I_STAT__PRFME__V = ^xb	
EV6__I_STAT__PRFME__M = ^x7FF

	ALIGN_CACHE_BLOCK

call_pal__perfmon_rd_istat:
	hw_mfpr	r0, EV6__I_STAT			; (4,0L) get I_STAT
	srl	r0, #EV6__I_STAT__PRFME__S, r0	; clean <29:0>
	lda	p4, EV6__I_STAT__PRFME__M(r31)	; mask for cleaning
	and	p4, r0, r0			; mask
	sll	r0, #EV6__I_STAT__PRFME__S,r0	; shift back
	hw_ret	(p23)				; just return, no changes
;
; Read pmpc
;
call_pal__perfmon_rd_pmpc:
	hw_mfpr	r0, EV6__PMPC			; get PMPC
	bic	r0, #2, r0			; clear RAZ bit to be neat
	hw_ret	(p23)				; just return, no changes
;
; Unknown function
;
call_pal__perfmon_unknown:

	hw_ret	(p23)				; just return

	END_CALL_PAL

;+
; CALL_PAL__RDUSP
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	r0(v0) <- USP
;-
	START_CALL_PAL <RDUSP>

	hw_ldq/p r0, PT__USP(p_temp)		; read usp
	NOP					; no hw_ret in 1st fetch block
	NOP
	NOP

	hw_ret	(p23)				; return

	END_CALL_PAL


;+
; CALL_PAL__003B
;
; Entry:
;	r16		0xBAC
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	Return in palmode. For FP emulation.
;-
.if ne ev6_p1

OSFPAL_FUNC__003B = ^x3B

	START_CALL_PAL <003B>

	lda	p4, ^xBAC(r31)			; test for ^xBAC
	cmpeq	p4, r16, p4
	bis	r31, r31, r31
	bis	r31, r31, r31

	beq	p4, call_pal__003B_fail		; fail if not ^xBAC
	sll	p23, #22, p23
	srl	p23, #22, p23			; clean off kseg
	bis	p23, #1, p23			; or in pal-mode bit
	hw_ret	(p23)				; return in pal-mode

call_pal__003B_fail:
	br	r31, trap__opcdec_call_pal	; take an opcdec

	END_CALL_PAL
.endc


;+
; CALL_PAL__WHAMI
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	r0(v0) <- WHAMI
;-
	START_CALL_PAL <WHAMI>

	hw_ldq/p r0, PT__WHAMI(p_temp)		; get whami
	NOP
	NOP
	NOP					; no hw_ret in 1st fetch block

	hw_ret	(p23)				; return

	END_CALL_PAL

;+
; CALL_PAL__RETSYS
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	The return from system call instruction pops the return address
;	and the user mode global pointer form the kernel stack. It then
;	saves the kernal stack pointer, sets the mode to user, sets the
;	IPL to 0, and enters the user mode code at the address popped
;	off the stack. In addition the interrupt and load lock flags
;	have been cleared.
;
; Note:
;	For now save pc+4. To be more accurate, we may want to save pc.
;	For ev5, just pc+4 was used. This pc is used for ksp not valid
;	halts.
; Note:
;	Any load below clears the lock flag.
;-
	START_CALL_PAL <RETSYS>

ASSUME OSF_P_MISC__PS__S eq 0
ASSUME EV6__PS__CM__S eq 3

	hw_stq/p p23, PT__STACK_PC(p_temp)		; in case of fault

	ldq	p20, OSF_FRM__PC(r30)			; get pc
	ldq	r29, OSF_FRM__GP(r30)			; get gp

	bic	p20, #3, p23				; clean pc, move to p23
	rc	r31					; clear interrupt flag

	lda	p20, OSF_FRM__SIZE(r30)			; pop kernel stack
	hw_ldq/p r30, PT__USP(p_temp)			; get usp
	hw_stq/p p20, PT__KSP(p_temp)			; save kernel stack
;
; Now set up for cm=1 and ipl=0.
;
	hw_mfpr	p4, EV6__PAL_BASE			; (4,0L) get pal base
	lda	p4, ipl_offset(p4)			; pal base + table base
	hw_ldq/p p4, (p4)				; get new ier

	CONT_CALL_PAL <RETSYS>				; finish in new space

	bic	p_misc, #OSF_P_MISC__PS__M, p_misc	; clean ps
	bis	p_misc, #<1@OSF_P_MISC__CM__S>, p_misc	; new ps with mode=user
	bis	p4, #<1@EV6__PS__CM__S>, p4		; new ier and new cm

	hw_mtpr	p4, EV6__IER_CM				; (4,0L) new ier_cm

	hw_ret_stall (p23)				; return

	END_CALL_PAL

;+
; CALL_PAL__WRINT
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;	r16(a0)		maximum nmber of interval clock ticks to skip
;
; Function:
;	The wait for interrupt instruction requests that, if possible, the
;	PALcode wait for the first of either of the following conditions
;	before returning:
;		any interrupt other than a clock tick
;		the first clock tick after the specified number
;
; Exit state:
;	r0(v0)		number of interval clock ticks actually skipped
;
;-

	START_CALL_PAL <WTINT>

.if eq reference_platform

	hw_ret	(p23)				; return

.iff

	br	r31, sys__wtint			; handle in system module

.endc

	END_CALL_PAL


;+
; CALL_PAL__RTI
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	The return from fault, trap, or interrupt instruction pops the
;	kernel stack, restoring a0..a2, PS, PC, GP. If the new mode is user,
;	the kernel stack is saved and the user stack is restored.
;	Any load below clears the lock flag.
;-
	START_CALL_PAL <RTI>

ASSUME OSF_P_MISC__PS__S eq 0

	hw_stq/p p23, PT__STACK_PC(p_temp)		; in case of fault

	ldq	p20, OSF_FRM__PS(r30)			; get ps
	ldq	r18, OSF_FRM__A2(r30)			; get a2
							; 1.39 last possible
							;   dfault or tnv

	ldq	r29, OSF_FRM__GP(r30)			; get gp
	ldq	r16, OSF_FRM__A0(r30)			; get a0
	ldq	r17, OSF_FRM__A1(r30)			; get a1

	ldq	p23, OSF_FRM__PC(r30)			; 1.39 get pc
	bic	p23, #3, p23				; clean return pc
	rc	r31					; clear interrupt flag

	and	p20, #<1@OSF_PS__CM__S>, p4		; get mode
	beq	p4, call_pal__rti_to_kern		; br if rti to kernel

	CONT_CALL_PAL <RTI>
;
; rti to user mode
;
; Some platforms, such as those with tsunami, have a problem with the
; latency between clearing an interrupt and the interrupt being deasserted.
; Those platforms are opting to, on isum<device_irq> set, write to
; a tsunami csr to deassert the interrupt. A real interrupt will
; re-assert on the next polling loop.
;

.if ne	check_interrupt_pending
	hw_mfpr	p6, EV6__ISUM				; (0L) get ISUM
.endc

	lda	p20, OSF_FRM__SIZE(r30)			; pop kernel stack
	hw_ldq/p r30, PT__USP(p_temp)			; get usp
	hw_stq/p p20, PT__KSP(p_temp)			; save kernel stack
;
; Now set up for cm=1 and ipl=0.
;
	hw_mfpr	p4, EV6__PAL_BASE			; (4,0L) get pal base
	lda	p4, ipl_offset(p4)			; pal base + table base
	hw_ldq/p p4, (p4)				; get new ier

	bic	p_misc, #OSF_P_MISC__PS__M, p_misc	; clean ps
	bis	p_misc, #<1@OSF_P_MISC__CM__S>, p_misc	; new ps with mode=user
	bis	p4, #<1@EV6__PS__CM__S>, p4		; new ier and new cm

.if ne	check_interrupt_pending
	xor	p6, p6, p5			; interlock isum and ier
	bis	p5, p4, p4			; interlock isum and ier
.endc

	hw_mtpr	p4, EV6__IER_CM			; (4,0L) new ier_cm

.if eq	check_interrupt_pending
	hw_ret_stall (p23)			; rei to non-kern
.iff
	lda	p4, IRQ_DEV__M(r31)		; mask of IRQ(s) (IRQ1)
	sll	p4, #EV6__ISUM__EI__S, p4	; shift into position
	and	p6, p4, p4			; check for IRQ

	bne	p4, sys__deassert_interrupt	; go off to deassert
	hw_ret_stall (p23)			; otherwise finish the rei
.endc

;
; rti to kernel mode
;
; Current state:
;	p20		new ps
;	p23		new pc, cleaned
;
; Note: probably don't need to save kernel stack to PT__KSP,
;	but it doesn't hurt.
;
; Some platforms, such as those with tsunami, have a problem with the
; latency between clearing an interrupt and the interrupt being deasserted.
; Those platforms are opting to, on isum<device_irq> set, write to
; a tsunami csr to deassert the interrupt. A real interrupt will
; re-assert on the next polling loop.
;
call_pal__rti_to_kern:

.if ne	check_interrupt_pending
	hw_mfpr	p6, EV6__ISUM				; (0L) get ISUM	
.endc

	lda	r30, OSF_FRM__SIZE(r30)			; pop kernel stack
	hw_stq/p r30, PT__KSP(p_temp)			; save kernel stack
;
; Now set up for new PS with cm=0, new ipl.
;
	hw_mfpr	p4, EV6__PAL_BASE			; (4,0L) get pal base

	and	p20, #OSF_P_MISC__IPL__M, p20	; clean ps
	s8addq	p20, p4, p4			; pal base + index
	lda	p4, ipl_offset(p4)		; pal base + table base + index
	hw_ldq/p p4, (p4)			; get new ier

	bic	p_misc, #<OSF_P_MISC__IPL__M>, p_misc	; clear out old ipl
	or	p_misc, p20, p_misc			; put new ipl in p_misc

.if ne	check_interrupt_pending
	xor	p6, p6, p5			; interlock isum and ier
	bis	p5, p4, p4			; interlock isum and ier
.endc

	hw_mtpr	p4, EV6__IER			; (4,0L) write new ier

.if eq	check_interrupt_pending
	hw_ret_stall (p23)			; rei to non-kern
.iff
	lda	p4, IRQ_DEV__M(r31)		; mask of IRQ(s) (IRQ1)
	sll	p4, #EV6__ISUM__EI__S, p4	; shift into position
	and	p6, p4, p4			; check for IRQ

	bne	p4, sys__deassert_interrupt	; go off to deassert
	hw_ret_stall (p23)			; otherwise finish the rei
.endc

	END_CALL_PAL

;+
; CALL_PAL__BPT
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	The breakpoint trap instruction switches mode to kernel, builds a
;	stack frame on the kernel stack, loads the GP with the KGP, loads
;	a value of 0 into a0, and dispatches to the breakpoint code pointed
;	to by the entIF register. The saved PC at (SP+08) is the address
;	of the instruction following the trap instruction that caused the
;	trap.
;-
	START_CALL_PAL <BPT>

	bis	r31, #OSF_A0__BPT, p4			; new a0
	hw_stq/p p4, PT__NEW_A0(p_temp)			; save it away
	br	r31, call_pal__bpt_post_if		; merge to complete

	CONT_CALL_PAL
;
; Merge bpt, bugchk, and gentrap to finish up.
;
call_pal__bpt_post_if:

	and	p_misc, #<1@OSF_P_MISC__CM__S>, p4	; current mode
	and	p_misc, #OSF_P_MISC__PS__M, p20		; save original ps
	beq	p4, trap__bpt_stack			; skip switch if kernel
;
; Switch to kernel mode. 
;
bpt_cm_offset = <trap__bpt_stack - trap__bpt_cm>

	hw_stq/p r30, PT__USP(p_temp)			; save user SP
	hw_ldq/p r30, PT__KSP(p_temp)			; get kernel SP
	bic	p_misc, #OSF_P_MISC__PS__M, p_misc	; cm=0, ipl=0

	br	p6, trap__bpt_cm			; change mode to kernel
trap__bpt_cm:
	addq	p6, #<bpt_cm_offset+1>, p6		; jump past in palmode
	hw_mtpr	r31, EV6__PS				; (4,0L) switch to kern
	bsr	r31, .					; push prediction stack
	PVC_JSR	bpt_cm					; synch up
	hw_ret_stall (p6)				; pop prediction stack
	PVC_JSR	bpt_cm, dest=1
;
; Take the trap. Be careful of tb miss!
; Current state:
;	p_misc__ps	if we were in kernel, original ps
;			if we were in user, new ps = 0 (cm=0, ipl=0)
;	p20		original ps
;	p23		nextpc
;
trap__bpt_stack:
	hw_stq/p p23, PT__STACK_PC(p_temp)		; store away nextpc

	lda	r30, <0-OSF_FRM__SIZE>(r30)		; allocate stack space

	stq	p20, OSF_FRM__PS(r30)			; save original ps
	stq	r18, OSF_FRM__A2(r30)			; save a2

	hw_ldq/p p20, PT__STACK_PC(p_temp)		; 1.39 get pc back

	stq	r29, OSF_FRM__GP(r30)			; save gp
	stq	r16, OSF_FRM__A0(r30)			; save a0
	stq	r17, OSF_FRM__A1(r30)			; save a1
	stq	p20, OSF_FRM__PC(r30)			; 1.39 save next pc

	hw_ldq/p p23, PT__ENT_IF(p_temp)		; get entry point
	hw_ldq/p r29, PT__KGP(p_temp)			; get kgp
	hw_ldq/p r16, PT__NEW_A0(p_temp)		; a0 <- type code

	hw_ret	(p23)					; to os

	END_CALL_PAL

;+
; CALL_PAL__BUGCHK
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	The bugchk instruction switches mode to kernel, builds a
;	stack frame on the kernel stack, loads the GP with the KGP, loads
;	a value of 1 into a0, and dispatches to the breakpoint code pointed
;	to by the entIF register. The saved PC at (SP+08) is the address
;	of the instruction following the trap instruction that caused the
;	trap.
;-
	START_CALL_PAL <BUGCHK>

	bis	r31, #OSF_A0__BUGCHK, p4		; new a0
	hw_stq/p p4, PT__NEW_A0(p_temp)			; save it away
	br	r31, call_pal__bpt_post_if		; merge to complete

	END_CALL_PAL

;+
; CALL_PAL__CALLSYS
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	The system call instruction is supported only from user mode.
;	Issuing a callsys from kernel mode causes a machine check exception.
;	The callsys instruction switches mode to kernel and builds a
;	callsys stack frame. The GP is loaded with the KGP. The exception
;	then dispatches to the system call code pointed to by the
;	entSys register.
;-
	START_CALL_PAL <CALLSYS>

	and	p_misc, #<1@OSF_P_MISC__CM__S>, p4	; current mode
	and	p_misc, #OSF_P_MISC__PS__M, p20		; save original ps
	beq	p4, trap__pal_os_bugcheck		; mchk if kernel mode
;
; Switch to kernel mode. 
;
callsys_cm_offset = <trap__callsys_cm_done - trap__callsys_cm>

	hw_stq/p r30, PT__USP(p_temp)			; save user SP
	hw_ldq/p r30, PT__KSP(p_temp)			; get kernel SP
	bic	p_misc, #OSF_P_MISC__PS__M, p_misc	; cm=0, ipl=0

	br	p6, trap__callsys_cm			; change mode to kernel
trap__callsys_cm:
	addq	p6, #<callsys_cm_offset+1>, p6		; jump past in palmode
	hw_mtpr	r31, EV6__PS				; (4,0L) switch to kern
	bsr	r31, .					; push prediciton stack
	PVC_JSR	callsys_cm				; synch up
	hw_ret_stall (p6)				; pop prediction stack
	PVC_JSR	callsys_cm, dest=1
trap__callsys_cm_done:

	CONT_CALL_PAL <CALLSYS>
;
; Take the trap. Be careful of tb miss!
; Current state:
;	p_misc__ps	new ps = 0 (cm=0, ipl=0)
;	p20		original ps (cm=1, ipl=0)
;	p23		nextpc
;
	hw_stq/p p23, PT__STACK_PC(p_temp)		; store away nextpc

	lda	r30, <0-OSF_FRM__SIZE>(r30)		; allocate stack space

	stq	p20, OSF_FRM__PS(r30)			; save original ps
	stq	r29, OSF_FRM__GP(r30)			; save gp

	hw_ldq/p p20, PT__STACK_PC(p_temp)		; 1.39 get nextpc back

	stq	p20, OSF_FRM__PC(r30)			; 1.39 save nextpc

	hw_ldq/p p23, PT__ENT_SYS(p_temp)		; get entry point
	hw_ldq/p r29, PT__KGP(p_temp)			; get kgp

	hw_ret	(p23)					; to os

	END_CALL_PAL

;+
; CALL_PAL__IMB
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	Make instruction stream coherent with data stream. Does not
;	guarantee other processors see a modification of the instruction
;	stream.
;-
	START_CALL_PAL <IMB>

	MB					; force memory operations
	NOP
	NOP
	NOP

	hw_mtpr	r31, EV6__IC_FLUSH		; (4,0L) flush the icache
	bne	r31, .				; pvc #24
	hw_ret_stall (p23)			; return with stall

	END_CALL_PAL


ASSUME <clrmap+force_ecc> ne 2
.if ne force_ecc
;+
; CALL_PAL__ECC_ERROR
;
; Entry:
;	r16		address
;	r17		data
;       p23             pc of instruction following the call_pal instruction
;
; Function:
;	Force ecc error.
;
; Current state:
;	p21	p_temp
;	p22	p_misc
;	p23	linkage register
;
; Exit state:
;       r0	clobbered
;
; Note: This is a debugging call_pal. It is not particularly robust.
; In particular, it does not expect any exceptions on the virtual accesses.
;-

OSFPAL_FUNC__ECCTEST        = ^x87

        START_CALL_PAL <ECCTEST>

        mb					; force completion
        hw_ldq/p p20, PT__IMPURE(p_temp)	; impure base
        hw_ldq/p r0, CNS__DC_CTL(p20)		; get old DC_CTL value
        ldq     p20, (r16)			; (xL,4) read the data

        mb					; force completion
        xor     p20, r17, p20
        bis     r0, #^x20, r0
        hw_mtpr r0, EV6__DC_CTL			; (6,0L) write DC_CTL

        bis     r31, r31, r31			
        bis     r31, r31, r31
        mb					; force completion
        hw_mtpr r0, EV6__DC_CTL			; (6,0L) force retire

	CONT_CALL_PAL <ECCTEST>

        ASSUME_FETCH_BLOCK

        stq     p20, (r16)			; (xL,4) write the data
        mb					; force completion
        bic     r0, #^x20, r0
        hw_mtpr r0, EV6__DC_CTL			; (6,0L) restore DC_CTL

        bis     r31, r31, r31
        bis     r31, r31, r31
        mb					; force completion
        hw_mtpr r0, EV6__DC_CTL			; (6,0L) force retire

        bis     r31, r31, r31
        hw_ret  (p23)				; return

        END_CALL_PAL
.endc


.if ne clrmap
;+
; CALL_PAL__CLRMAP
;
; Entry:
;       p23             pc of instruction following the call_pal instruction
;
; Function:
;	Clear the register map
;
; Current state:
;	p21	p_temp
;	p22	p_misc
;	p23	linkage register
;
; Exit state:
;       restored
;-

OSFPAL_FUNC__CLRMAP        = ^x87

        START_CALL_PAL <CLRMAP>

  .if eq ev6_p1
	hw_ret	(p23)				; just return if not p1
  .iff

        hw_mfpr p7, EV6__PAL_BASE		; (4,0L) pal base
	hw_ldq/p p6, PT__WHAMI(p_temp)		; get my cpu number
	s8addq	p6, p7, p7			; (cpu*8)+base
	hw_stq/p p_temp, ^x30(p7)		; p_temp at ^x30+(cpu*8)+base

        hw_ldq/p p4, PT__IMPURE(p_temp)		; get base of impure area
        hw_stq/p p_misc, CNS__P_MISC(p4)	; store p_misc
        hw_stq/p p23, CNS__P23(p4)		; store call_pal linkage
	hw_mfpr	p5, EV6__SIRR			; (4,0L) get sirr

	hw_stq/p p5, CNS__SIRR(p4)		; store sirr
	sll	p6, #EV6__SIRR__SIR__S, p6	; cpu number into place
	hw_mtpr	p6, EV6__SIRR			; (4,0L) save cpu number
	bis	r31, r31, r31			; pad fetch block

        hw_mfpr  p5, EV6__I_CTL                 ; (4,0L) get i_ctl

	CONT_CALL_PAL <CLRMASK>

        ASSUME_FETCH_BLOCK

	GET_32CONS p20, <^x804000>, r31		; hack for ev6isp
	bic	p5, p20, p5			; clear unpredictables
        hw_stq/p p5, CNS__I_CTL(p4)		; store i_ctl

        hw_stq/p r0, CNS__R0(p4)           	; save r0
        hw_stq/p r1, CNS__R1(p4)           	; save r1
        hw_stq/p r2, CNS__R2(p4)           	; save r2
        hw_mfpr r2, EV6__I_CTL                  ; (4,0L) get i_ctl

        bis     p4, r31, r1                     ; impure base into r1
        bic     r2, #<2@EV6__I_CTL__SDE__S>, r2 ; zap sde
        hw_stq/p r3, CNS__R3(p4)           	; save r3
        hw_stq/p r8, CNS__R8(r1)           	; save gpr

        hw_mtpr r2, EV6__I_CTL                  ; (4,0L) write i_ctl
        hw_stq/p r9, CNS__R9(r1)           	; save gpr
        hw_stq/p r10, CNS__R10(r1)         	; save gpr
        hw_stq/p r11, CNS__R11(r1)         	; save gpr

        hw_mtpr r2, EV6__I_CTL                  ; (4,0L) stall outside IQ
        hw_stq/p r12, CNS__R12(r1)         	; save gpr
        hw_stq/p r13, CNS__R13(r1)         	; save gpr
        hw_stq/p r14, CNS__R14(r1)         	; save gpr

        hw_stq/p r15, CNS__R15(r1)         	; buffer block 1 -- save gpr
        hw_stq/p r16, CNS__R16(r1)         	; save gpr
        hw_stq/p r17, CNS__R17(r1)         	; save gpr
        hw_stq/p r18, CNS__R18(r1)         	; save gpr

        hw_stq/p r19, CNS__R19(r1)         	; buffer block 2 -- save gpr
        hw_stq/p r24, CNS__R24(r1)         	; save gpr
        hw_stq/p r25, CNS__R25(r1)         	; save gpr
        hw_stq/p r26, CNS__R26(r1)         	; save gpr

        hw_stq/p r27, CNS__R27(r1)         	; buffer block 3 -- save gpr
        hw_stq/p r28, CNS__R28(r1)         	; save gpr
        hw_stq/p r29, CNS__R29(r1)         	; save gpr
        hw_stq/p r30, CNS__R30(r1)         	; save gpr

        hw_stq/p r4, CNS__R4(r1)           	; save gpr
        hw_stq/p r5, CNS__R5(r1)           	; save gpr
        hw_stq/p r6, CNS__R6(r1)           	; save gpr
        hw_stq/p r7, CNS__R7(r1)           	; save gpr

        hw_stq/p r20, CNS__R20(r1)         	; save gpr
        hw_stq/p r21, CNS__R21(r1)         	; save gpr
        hw_stq/p r22, CNS__R22(r1)         	; save gpr
        hw_stq/p r23, CNS__R23(r1)         	; save gpr

        hw_stq/p r31, CNS__R31(r1)         	; save gpr
	bis	r31, r31, r31
	bis	r31, r31, r31
	mb					; dll -- add mb

        hw_mfpr r0, EV6__I_CTL			; (4,0L) get i_ctl
        or	r31, #EV6__I_CTL__SBE__M, r3	; form SBE mask
        sll 	r3, #EV6__I_CTL__SBE__S, r3
        bic	r0, r3, r0			; zap SBE bits

clr_map_1_offset = <clr_map_1_done - clr_map_1>

        hw_mtpr r0, EV6__I_CTL			; (4,0L) turn off stream buffers
	br	r2, clr_map_1
clr_map_1:
	addq	r2, #<clr_map_1_offset+1>, r2	; jump past in palmode
	bsr	r31, .				; push prediction stack

        hw_mtpr r31, EV6__IC_FLUSH		; flush icache
	bne	r31, .
	PVC_JSR	clr_map_1			; synch up
	hw_ret_stall (r2)			; pop prediction stack
	PVC_JSR	clr_map_1, dest=1

        .ALIGN 6 ,<^X47FF041F>
clr_map_1_done:
        hw_mtpr r0, EV6__CLR_MAP      		; (4-7,0L) clear map
	NOP
	NOP
	NOP

	NOP					; dll - add quiet stuff
	NOP
	NOP
	NOP

clr_map_2_offset = <real_code_1 - clr_map_2>

	NOP
	NOP
	bsr	r31, .				; push prediction stack
	lda	r3, <clr_map_2_offset>(r31)

	br	r2, clr_map_2
clr_map_2:
	addq	r2, r3, r2
	addq	r2, #<1>, r2			; jump past in palmode
	PVC_JSR	clr_map_2			; synch up
	hw_ret_stall (r2)			; pop prediction stack
	PVC_JSR	clr_map_2, dest=1

	NOP					; dll - add more quiet stuff
	NOP
	NOP
	NOP

	.ALIGN 6,<^X47FF041F>
	NOP
	NOP
	NOP
	NOP

	.ALIGN 6,<^X47FF041F>
	NOP
	NOP
	NOP
	NOP

	.ALIGN 6,<^X47FF041F>
	NOP
	NOP
	NOP
	NOP

	.ALIGN 6,<^X47FF041F>
	NOP
	NOP
	NOP
	NOP

	.ALIGN 6,<^X47FF041F>
	NOP
	NOP
	NOP
	NOP
;
; Now do the mapper again
;
        .ALIGN 6 ,<^X47FF041F>
real_code_1:
        addq    r31,r31,r0
        addq    r31,r31,r1
        addq    r31,r31,r2
        addq    r31,r31,r3

        addq    r31,r31,r4
        addq    r31,r31,r5
        addq    r31,r31,r6
        addq    r31,r31,r7

        addq    r31,r31,r8
        addq    r31,r31,r9
        addq    r31,r31,r10
        addq    r31,r31,r11

        addq    r31,r31,r12
        addq    r31,r31,r13
        addq    r31,r31,r14
        addq    r31,r31,r15

        addq    r31,r31,r16
        addq    r31,r31,r17
        addq    r31,r31,r18
        addq    r31,r31,r19

        addq    r31,r31,r20
        addq    r31,r31,r21
        addq    r31,r31,r22
        addq    r31,r31,r23

        addq    r31,r31,r24
        addq    r31,r31,r25
        addq    r31,r31,r26
        addq    r31,r31,r27

        addq    r31,r31,r28
        addq    r31,r31,r29
        addq    r31,r31,r30
        addq    r31,r31,r0			; done 32

        addq    r31,r31,r0
        addq    r31,r31,r1
        addq    r31,r31,r2
        addq    r31,r31,r3

        addq    r31,r31,r4
        addq    r31,r31,r5
        addq    r31,r31,r6
        addq    r31,r31,r7

        addq    r31,r31,r8
        addq    r31,r31,r9
        addq    r31,r31,r10
        addq    r31,r31,r11

        addq    r31,r31,r12
        addq    r31,r31,r13
        addq    r31,r31,r14
        addq    r31,r31,r15

        addq    r31,r31,r16
        addq    r31,r31,r17
        addq    r31,r31,r18
        addq    r31,r31,r19

        addq    r31,r31,r20
        addq    r31,r31,r21
        addq    r31,r31,r22
        addq    r31,r31,r23

        addq    r31,r31,r24
        addq    r31,r31,r25
        addq    r31,r31,r26
        addq    r31,r31,r27

        addq    r31,r31,r28
        addq    r31,r31,r29
        addq    r31,r31,r30
	addq	r31,r31,r0			; done 64

        addq    r31,r31,r0
        addq    r31,r31,r1
        addq    r31,r31,r2
        addq    r31,r31,r3

        addq    r31,r31,r4
        addq    r31,r31,r5
        addq    r31,r31,r6
        addq    r31,r31,r7

        addq    r31,r31,r8
        addq    r31,r31,r9
        addq    r31,r31,r10
        addq    r31,r31,r11

        addq    r31,r31,r12
        addq    r31,r31,r13
        addq    r31,r31,r14
        addq    r31,r31,r15			; done 80
;
; Now turn on mapper source enables
;
        hw_mtpr r31,EV6__ITB_IA         ; (4,0L) flush ITB, enable source map
        hw_mtpr r31,EV6__DTB_IA         ; (7,1L) flush DTB
        addq    r31,r31,r0              ; nop
        addq    r31,r31,r0              ; nop
;
; Create a stall outside the IQ until the mtpr EV6__ITB_IA retires.
;
	PVC_VIOLATE <21>
        hw_mtpr r31,<EV6__MM_STAT ! ^x90>    ; (4&7,0L) IQ stall.
        addq    r31,r31,r0              ; nop
        addq    r31,r31,r0              ; nop
        addq    r31,r31,r0              ; nop

        addq    r31,r31,r0              ; 1st buffer fetch block. IMAP stall
        addq    r31,r31,r0              ; nop
        addq    r31,r31,r0              ; nop
        addq    r31,r31,r0              ; nop

        addq    r31,r31,r0              ; 2nd buffer fetch block. FMAP stall
        addq    r31,r31,r0              ; nop
        addq    r31,r31,r0              ; nop
        addq    r31,r31,r0              ; nop
;
; Now map the shadow registers
;
        lda     r0,^x0086(r31)          ; load I_CTL.....
	bis	r0, r0, r0
	bis	r0, r0, r0
	bis	r0, r0, r0

        hw_mtpr r0,EV6__I_CTL           ; .....SDE=2, IC_EN=3 (SCRBRD=4)
	bis	r0, r0, r0
	bis	r0, r0, r0
	bis	r0, r0, r0

        hw_mtpr r0,EV6__I_CTL           ; .....SDE=2, IC_EN=3 (SCRBRD=4)
        addq    r31,r31,r0              ; nop
        addq    r31,r31,r0              ; nop
        addq    r31,r31,r0              ; nop

        addq    r31,r31,r0              ; 1st buffer fetch block for above map-stall
        addq    r31,r31,r0              ; nop
        addq    r31,r31,r0              ; nop
        addq    r31,r31,r0              ; nop

        addq    r31,r31,r0              ; 2nd buffer fetch block for above map-stall
        addq    r31,r31,r0              ; nop
        addq    r31,r31,r0              ; nop
        addq    r31,r31,r0              ; nop

        addq    r31,r31,r0              ; need 3rd buffer fetch block to get sde bit
        addq    r31,r31,r0              ; nop
        addq    r31,r31,r0              ; nop
        addq    r31,r31,r0              ; nop
;
; map shadow registers
;
        addq    r31,r31,r4
        addq    r31,r31,r5
        addq    r31,r31,r6
        addq    r31,r31,r7

        addq    r31,r31,r20
        addq    r31,r31,r21
        addq    r31,r31,r22
        addq    r31,r31,r23
;
; Now restore stuff
;	p21 = p_temp
;	p22 = p_misc
;	p23 = linkage
;
;	(pal_base+^x30+(8*cpu)) = p_temp
;	CNS__P_MISC =  p_misc
;	CNS__P23    = linkage
;	CNS__I_CTL  = i_ctl
;	CNS__SIRR   = sirr value
;
	ASSUME_FETCH_BLOCK

	hw_mfpr	p6, EV6__SIRR			; (4,0L) get cpu number
	srl	p6, #EV6__SIRR__SIR__S, p6	; shift into place
	bis	r31, r31, r31
	bis	r31, r31, r31

	hw_mfpr	r2, EV6__PAL_BASE		; (4,0L) pal base
	s8addq	p6, r2, r2			; (cpu*8)+base
	hw_ldq/p p_temp, ^x30(r2)		; restore p_temp
	hw_ldq/p r1, PT__IMPURE(p_temp)		; get base of impure area

        hw_ldq/p p_misc, CNS__P_MISC(r1)	; restore p_misc
        hw_ldq/p p23, CNS__P23(r1)		; restore call_pal linkage
	hw_ldq/p p5, CNS__SIRR(r1)		; get SIRR back
	hw_mtpr	p5, EV6__SIRR			; (4,0L) restore it

        hw_ldq/p r0, CNS__I_CTL(r1)		; get saved i_ctl
        bic     r0, #<2@EV6__I_CTL__SDE__S>, r2 ; zap sde
        hw_ldq/p r8, CNS__R8(r1)           	; restore gpr
        hw_ldq/p r9, CNS__R9(r1)           	; restore gpr

        hw_mtpr r2, EV6__I_CTL                  ; (4,0L) write i_ctl
        hw_ldq/p r10, CNS__R10(r1)         	; restore gpr
        hw_ldq/p r11, CNS__R11(r1)         	; restore gpr
        hw_ldq/p r12, CNS__R12(r1)         	; restore gpr

        hw_mtpr r2, EV6__I_CTL                  ; stall outside IQ
        hw_ldq/p r13, CNS__R13(r1)         	; restore gpr
        hw_ldq/p r14, CNS__R14(r1)         	; restore gpr
        hw_ldq/p r15, CNS__R15(r1)         	; restore gpr

        hw_ldq/p r16, CNS__R16(r1)         	; buffer block 1 -- restore gpr
        hw_ldq/p r17, CNS__R17(r1)         	; restore gpr
        hw_ldq/p r18, CNS__R18(r1)         	; restore gpr
        hw_ldq/p r19, CNS__R19(r1)         	; restore gpr

        hw_ldq/p r24, CNS__R24(r1)         	; buffer block 2 -- restore gpr
        hw_ldq/p r25, CNS__R25(r1)         	; restore gpr
        hw_ldq/p r26, CNS__R26(r1)         	; restore gpr
        hw_ldq/p r27, CNS__R27(r1)         	; restore gpr

        hw_ldq/p r28, CNS__R28(r1)         	; buffer block 3 -- restore gpr
        hw_ldq/p r29, CNS__R29(r1)         	; restore gpr
        hw_ldq/p r30, CNS__R30(r1)         	; restore gpr
        bis     r31, r31, r31

        hw_ldq/p r4, CNS__R4(r1)           	; restore gpr
        hw_ldq/p r5, CNS__R5(r1)           	; restore gpr
        hw_ldq/p r6, CNS__R6(r1)           	; restore gpr
        hw_ldq/p r7, CNS__R7(r1)           	; restore gpr

        hw_ldq/p r20, CNS__R20(r1)         	; restore gpr
        hw_ldq/p r21, CNS__R21(r1)         	; restore gpr
        hw_ldq/p r22, CNS__R22(r1)         	; restore gpr
        hw_ldq/p r23, CNS__R23(r1)         	; restore gpr
;
; Now turn shadow mode back on.
;
        bis     r2, #<2@EV6__I_CTL__SDE__S>, r2 ; enable sde
        hw_mtpr r2, EV6__I_CTL                  ; (4,0L) write i_ctl
        bis     r31, r31, r31
        bis     r31, r31, r31

        hw_mtpr r2, EV6__I_CTL                  ; (4,0L) stall outside IQ
        bis     r31, r31, r31
        bis     r31, r31, r31
        bis     r31, r31, r31

        bis     r0, r0, r0                      ; buffer block 1
        bis     r31, r31, r31
        bis     r31, r31, r31
        bis     r31, r31, r31

        bis     r0, r0, r0                      ; buffer block 2
        bis     r31, r31, r31
        bis     r31, r31, r31
        bis     r31, r31, r31

        bis     r0, r0, r0                      ; buffer block 3
        bis     r31, r31, r31
        bis     r31, r31, r31
        bis     r31, r31, r31

        bis     r1, #0, p4                      ; impure pointer
        hw_ldq/p r0, CNS__R0(p4)           	; restore r0
        hw_ldq/p r1, CNS__R1(p4)           	; restore r1
        hw_ldq/p r2, CNS__R2(p4)           	; restore r2

        hw_ldq/p r3, CNS__R3(p4)           	; restore r3
        hw_ret_stall  (p23)                     ; return

  .endc						; if/iff eq ev6_p1

        END_CALL_PAL
.endc						; if ne clrmap

;+
; CALL_PAL__URTI
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	The return from user mode trap insruction pops the user registers
;	(a0..a2, and GP), the new user AT, SP, PC and PS from the user stack.
;
; Stack Frame:
;	---------------------------------------------------------
;	|			at				|:00
;	---------------------------------------------------------
;	|			SP				|:08
;	---------------------------------------------------------
;	|			PS				|:16
;	---------------------------------------------------------
;	|			PC				|:24
;	---------------------------------------------------------
;	|			GP				|:32
;	---------------------------------------------------------
;	|			a0				|:40
;	---------------------------------------------------------
;	|			a1				|:48
;	---------------------------------------------------------
;	|			a2				|:56
;	---------------------------------------------------------
;
; Algorithm:
;	if (PS<mode> EQ 0) then
;		machineCheck
;	 endif
;	if {SP<5:0> NE 0 }
;		{ initiate illegal operand exception }
;	tempps  <- (SP+16)              ! Check access to stack frame
;	if {{ tempps<mode> EQ 0 } OR {tempps<IPL> NE 0 }} then
;		{ initiate illegal operand exception }
;	endif
;
;	at      <- (SP+0)
;	tempsp  <- (SP+8)
;	temppc  <- (SP+24)
;	GP      <- (SP+32)
;	a0      <- (SP+40)
;	a1      <- (SP+48)
;	a2      <- (SP+56)
;
;	intr_flag = 0
;	lock_flag = 0
;
;	SP      <- tempsp
;	PC      <- temppc
;
; Note: On a multiprocessor system, the other CPU can mark the in-memory
; PTE with fault bits to indicate it is a LRU candidate for the VM
; system. The PALcode must ensure it sees the same PTE throughout the
; operation. By doing a scoreboard stall, we ensure that we either
; get what's already in the TB for the entire operation, or take the
; miss and get a new PTE that we use for the entire operation.
;-
	START_CALL_PAL <URTI>

	hw_mtpr	r31, <EV6__MM_STAT ! ^xF0>	; 1.40 wait for pte write

	hw_stq/p p23, PT__STACK_PC(p_temp)		; in case of fault

	and	p_misc, #<1@OSF_P_MISC__CM__S>, p4	; current mode
	and	r30, #^x3F, p5				; check alignment
	rc	r31					; clear interrrupt flag

	beq	p4, trap__pal_os_bugcheck		; mchk if kernel mode
	bne	p5, call_pal__urti_illop		; error if unaligned
;
; The stack is naturally aligned, so we don't need to probe the ends.
; TNV and DFaults can occur on the first read. We also must
; avoid misses anywhere after the first read because another cpu
; may have marked the in memory PTE with fault bits to indicate it is a LRU
; candidate in the VM handler. We must ensure we see the same PTE the entire
; flow. So, at the beginning of this flow, we did a scoreboard stall to
; avoid having a mid-stream PTE/TAG  write retire and kick our entry out.
;
call_pal__urti_ldq:
	ldq	p20, 16(r30)				; fetch new ps
	and	p20, #^xF, p4				; check <3:0>
							; 1.37 take out urti_stq
	xor	p4, #<1@OSF_PS__CM__S>, p4		; test cm=1, ipl=0
	bne	p4, call_pal__urti_illop

	CONT_CALL_PAL <URTI>

	ldq	p20, 24(r30)				; 1.39 new pc
	bic	p20, #3, p20				; 1.39 clean pc

	ldq	r16, 40(r30)				; a0
	ldq	r17, 48(r30)				; a1
	ldq	r18, 56(r30)				; a2
	ldq	r29, 32(r30)				; gp
	ldq	r28, 0(r30)				; at
	ldq	r30, 8(r30)				; sp

	hw_ret	(p20)					; 1.39 return
;
; Illop. Either stack not aligned or new ps wrong.
;
call_pal__urti_illop:
	hw_ldq/p p23, PT__STACK_PC(p_temp)		; 1.37 get pc back
	bis	r31, #OSF_A0__ILLOP, p4			; new a0
	hw_stq/p p4, PT__NEW_A0(p_temp)			; save it away
	subq	p23, #4, p23				; want pc of urti
	br	r31, call_pal__bpt_post_if		; merge to complete

	END_CALL_PAL

;+
; CALL_PAL__RDUNIQUE
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	Write into r0(v0) the hardware process (thread) unique context value.
;
; Exit state:
;	r0(v0)		process unique context
;
;-
	START_CALL_PAL <RDUNIQUE>

	hw_ldq/p r0, PT__PCBB(p_temp)	; get PCBB
	hw_ldq/p r0, OSF_PCB__UNQ(r0)	; get UNQ
	NOP				; no hw_ret in 1st fetch block
	NOP

	hw_ret	(p23)			; return

	END_CALL_PAL

;+
; CALL_PAL__WRUNIQUE
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	Store the value of r16(a0) in the hardware process (thread) unique
;	context value.
;-
	START_CALL_PAL <WRUNIQUE>

	hw_ldq/p p4, PT__PCBB(p_temp)	; get PCBB
	hw_stq/p r16, OSF_PCB__UNQ(p4)	; write UNQ
	NOP				; no hw_ret in 1st fetch block
	NOP

	hw_ret	(p23)			; return

	END_CALL_PAL


;+
; CALL_PAL__GENTRAP
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	The gentrap instruction switches mode to kernel, builds a
;	stack frame on the kernel stack, loads the GP with the KGP, loads
;	a value of 2 into a0, and dispatches to the breakpoint code pointed
;	to by the entIF register. The saved PC at (SP+08) is the address
;	of the instruction following the trap instruction that caused the
;	trap.
;-
	START_CALL_PAL <GENTRAP>

	bis	r31, #OSF_A0__GENTRAP, p4		; new a0
	hw_stq/p p4, PT__NEW_A0(p_temp)			; save it away
	br	r31, call_pal__bpt_post_if		; merge to complete

	END_CALL_PAL



.if ne ev6_p1
    .if ne fp_count

;+
; CALL_PAL__FP_COUNT
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	r0(v0) <- fp counter
;
;-

OSFPAL_FUNC__00AD = ^xAD

	START_CALL_PAL <00AD>

	hw_ldq/p r0, PT__RSV_FOR_PAL(p_temp)	; read fp counter
	NOP					; no hw_ret in 1st fetch block
	NOP
	NOP

	hw_ret	(p23)				; return to user

	END_CALL_PAL

    .endc
.endc


;+
; CALL_PAL__CLRFEN
;
; Entry:
;	p23		pc of instruction following the call_pal instruction
;
; Function:
;	The clear floating-point enable (CLRFEN) instruction writes a
;	zero to the floating-point enable register and to the PCB
;	at offset (PCBB+FEN)<0>.
;
;	FEN 	   <- 0
;	(PCB+FEN) <- FEN
;-
	START_CALL_PAL <CLRFEN>

	hw_ldq/p p4, PT__PCBB(p_temp)		; get PCBB
	hw_mtpr	r31, EV6__FPE			; (4,0L) write new fpe
	hw_stl/p r31, OSF_PCB__FEN(p4)		; store cleared FEN in PCB
	bis	r31, r31, r31

	hw_mtpr	r31, EV6__FPE			; (4,0L) force retire
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 1
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

	bis	r0, r0, r0			; buffer block 2
	bis	r31, r31, r31
	bis	r31, r31, r31
	CONT_CALL_PAL <CLRFEN>

	bis	r0, r0, r0			; buffer block 3
	bis	r31, r31, r31
	bis	r31, r31, r31
	bis	r31, r31, r31

.if ne ev6_p1
	hw_ldq/p p5, PT__IMPURE(p_temp)		; get impure pointer
	hw_stq/p r31, CNS__FPE_STATE(p5)	; clear FPE state	
.endc

	hw_ret_stall (p23)			; return with stall

	END_CALL_PAL

	END_FREE_CODE
